% Generated by Sphinx.
\documentclass[openany,a4paper,10pt,english]{manual}
\usepackage[utf8]{inputenc}
\usepackage[T1]{fontenc}
\usepackage{babel}
\title{NECI Documentation}
\date{August 20, 2008}
\release{0.1}
\author{Alavi Group}
\newcommand{\sphinxlogo}{}
\renewcommand{\releasename}{Release}
\usepackage{amsmath}

\newcommand{\bra}{\ensuremath{\langle}}
\newcommand{\ket}{\ensuremath{\rangle}}

\newcommand{\veci}{\ensuremath{\mathbf{i}}}
\newcommand{\vecj}{\ensuremath{\mathbf{j}}}
\newcommand{\vecz}{\ensuremath{\mathbf{0}}}

\makeindex
\newcommand\at{@}
\newcommand\lb{[}
\newcommand\rb{]}
\newcommand\PYGaz[1]{\textcolor[rgb]{0.00,0.63,0.00}{#1}}
\newcommand\PYGax[1]{\textcolor[rgb]{0.84,0.33,0.22}{\textbf{#1}}}
\newcommand\PYGay[1]{\textcolor[rgb]{0.00,0.44,0.13}{\textbf{#1}}}
\newcommand\PYGar[1]{\textcolor[rgb]{0.73,0.38,0.84}{#1}}
\newcommand\PYGas[1]{\textcolor[rgb]{0.25,0.44,0.63}{\textit{#1}}}
\newcommand\PYGap[1]{\textcolor[rgb]{0.78,0.36,0.04}{#1}}
\newcommand\PYGaq[1]{\textcolor[rgb]{0.38,0.68,0.84}{#1}}
\newcommand\PYGav[1]{\textcolor[rgb]{0.00,0.44,0.13}{\textbf{#1}}}
\newcommand\PYGaw[1]{\textcolor[rgb]{0.13,0.50,0.31}{#1}}
\newcommand\PYGat[1]{\textcolor[rgb]{0.32,0.47,0.09}{#1}}
\newcommand\PYGau[1]{\textcolor[rgb]{0.13,0.50,0.31}{#1}}
\newcommand\PYGaj[1]{\textcolor[rgb]{0.00,0.44,0.13}{#1}}
\newcommand\PYGak[1]{\textcolor[rgb]{0.14,0.33,0.53}{#1}}
\newcommand\PYGah[1]{\textcolor[rgb]{0.00,0.13,0.44}{\textbf{#1}}}
\newcommand\PYGai[1]{\textcolor[rgb]{0.73,0.38,0.84}{#1}}
\newcommand\PYGan[1]{\textcolor[rgb]{0.00,0.44,0.13}{\textbf{#1}}}
\newcommand\PYGao[1]{\textcolor[rgb]{0.25,0.44,0.63}{\textbf{#1}}}
\newcommand\PYGal[1]{\colorbox[rgb]{1.00,0.94,0.94}{\textcolor[rgb]{0.25,0.50,0.56}{#1}}}
\newcommand\PYGam[1]{\textbf{#1}}
\newcommand\PYGab[1]{\textit{#1}}
\newcommand\PYGac[1]{\textcolor[rgb]{0.25,0.44,0.63}{#1}}
\newcommand\PYGaa[1]{\textcolor[rgb]{0.19,0.19,0.19}{#1}}
\newcommand\PYGaf[1]{\textcolor[rgb]{0.25,0.50,0.56}{\textit{#1}}}
\newcommand\PYGag[1]{\textcolor[rgb]{0.13,0.50,0.31}{#1}}
\newcommand\PYGad[1]{\textcolor[rgb]{0.25,0.44,0.63}{#1}}
\newcommand\PYGae[1]{\textcolor[rgb]{0.13,0.50,0.31}{#1}}
\newcommand\PYGaZ[1]{\textcolor[rgb]{0.02,0.16,0.45}{\textbf{#1}}}
\newcommand\PYGbf[1]{\textcolor[rgb]{0.44,0.63,0.82}{\textit{#1}}}
\newcommand\PYGaX[1]{\textcolor[rgb]{0.00,0.44,0.13}{#1}}
\newcommand\PYGaY[1]{\textcolor[rgb]{0.25,0.44,0.63}{#1}}
\newcommand\PYGbc[1]{\textcolor[rgb]{0.00,0.44,0.13}{\textbf{#1}}}
\newcommand\PYGbb[1]{\textcolor[rgb]{0.13,0.50,0.31}{#1}}
\newcommand\PYGba[1]{\textcolor[rgb]{0.00,0.00,0.50}{\textbf{#1}}}
\newcommand\PYGaR[1]{\textcolor[rgb]{0.73,0.38,0.84}{#1}}
\newcommand\PYGaS[1]{\textcolor[rgb]{0.25,0.50,0.56}{\textit{#1}}}
\newcommand\PYGaP[1]{\textcolor[rgb]{0.25,0.44,0.63}{#1}}
\newcommand\PYGaQ[1]{\textcolor[rgb]{0.13,0.50,0.31}{#1}}
\newcommand\PYGaV[1]{\textcolor[rgb]{0.05,0.52,0.71}{\textbf{#1}}}
\newcommand\PYGaW[1]{\textcolor[rgb]{0.25,0.44,0.63}{#1}}
\newcommand\PYGaT[1]{\textcolor[rgb]{0.50,0.00,0.50}{\textbf{#1}}}
\newcommand\PYGaU[1]{\textcolor[rgb]{0.00,0.44,0.13}{#1}}
\newcommand\PYGaJ[1]{\textcolor[rgb]{0.25,0.44,0.63}{#1}}
\newcommand\PYGaK[1]{\textcolor[rgb]{0.02,0.16,0.49}{#1}}
\newcommand\PYGaH[1]{\fcolorbox[rgb]{1.00,0.00,0.00}{1,1,1}{#1}}
\newcommand\PYGaI[1]{\textcolor[rgb]{0.56,0.13,0.00}{#1}}
\newcommand\PYGaN[1]{\textcolor[rgb]{0.05,0.52,0.71}{\textbf{#1}}}
\newcommand\PYGaO[1]{\textcolor[rgb]{0.78,0.36,0.04}{\textbf{#1}}}
\newcommand\PYGaL[1]{\textcolor[rgb]{0.73,0.73,0.73}{#1}}
\newcommand\PYGaM[1]{\textcolor[rgb]{0.00,0.44,0.13}{#1}}
\newcommand\PYGaB[1]{\textcolor[rgb]{0.00,0.25,0.82}{#1}}
\newcommand\PYGaC[1]{\textcolor[rgb]{0.33,0.33,0.33}{\textbf{#1}}}
\newcommand\PYGaA[1]{\textcolor[rgb]{0.00,0.44,0.13}{#1}}
\newcommand\PYGaF[1]{\textcolor[rgb]{1.00,0.00,0.00}{#1}}
\newcommand\PYGaG[1]{\textcolor[rgb]{0.73,0.38,0.84}{#1}}
\newcommand\PYGaD[1]{\textcolor[rgb]{0.25,0.50,0.56}{\textit{#1}}}
\newcommand\PYGaE[1]{\textcolor[rgb]{0.63,0.00,0.00}{#1}}
\newcommand\PYGbg[1]{\textcolor[rgb]{0.00,0.44,0.13}{\textbf{#1}}}
\newcommand\PYGbe[1]{\textcolor[rgb]{0.40,0.40,0.40}{#1}}
\newcommand\PYGbd[1]{\textcolor[rgb]{0.25,0.44,0.63}{#1}}


\begin{document}
%(shorthandoff)s
\maketitle
\tableofcontents



\resetcurrentobjects


\hypertarget{introduction}{}\chapter{Introduction}

NECI is a rapidly developing code based on a post Hartree--Fock electronic structure method.

It calculates electron correlation via path-resummations in Slater Determinant space \cite{SumPaper}, \cite{StarPaper}, \cite{ThomPhDThesis}.

As a standalone package, NECI can perform calculations on electrons confined to a box, the uniform electron gas and the hubbard model.

NECI can also read in wavefunctions, or a set of integrals based on them, of molecular systems produced by another program (e.g. \cite{DALTON} or \cite{MolPro}) and run calculations using them as the basis for forming the necessary Slater Determinants.

Finally, NECI can also be compiled as a library for integration into existing codes.  Currently this has been performed for the \cite{CPMD} or \cite{VASP} plane-wave packages, allowing calculations to be performed on periodic systems.

\resetcurrentobjects


\hypertarget{installation}{}\chapter{Installation}

Requirements:
\begin{itemize}
\item {} \begin{description}
\item[subversion]
The Alavi group currently only distributes code via the wwmm
subversion repository (account required), which is kindly hosted
by the Unilever Centre.

\end{description}

\item {} 
LAPACK

\item {} 
BLAS.

\item {} 
FFTW 3.x.

\end{itemize}


\section{Download}


\subsection{NECI}

The NECI source code can be downloaded from:

\begin{Verbatim}[commandchars=@\[\]]
svn checkout https://wwmm.ch.cam.ac.uk/svn2/groups/alavi/NECI/trunk NECI
\end{Verbatim}

Various branches also exist, but they may or may not be stable or under
active development.


\subsection{CPMD}

Our modified version of CPMD (containing the necessary routines for
integration with NECI) can be downloaded from:

\begin{Verbatim}[commandchars=@\[\]]
svn checkout https://wwmm.ch.cam.ac.uk/svn2/groups/alavi/CPMD/branches/QMC/trunk CPMD
\end{Verbatim}

Again, there exist various branches and they may or may not be stable
or under active development.
The default setting used in the compilation scripts are to have the NECI source as
a subdirectory of the CPMD source.  This can be changed by using the
command line options or setting them in the .compileconf file (see below).
It is suggested that you place NECI as a subdirectory of the CPMD source,
as that is how the CPMD configuration files in the repository are set up.


\subsection{VASP}

Only users on the access list can download VASP from our repository:

\begin{Verbatim}[commandchars=@\[\]]
svn checkout https://wwmm.ch.cam.ac.uk/svn2/groups/alavi/VASP/vasp.4.lib vasp.4.lib
svn checkout https://wwmm.ch.cam.ac.uk/svn2/groups/alavi/VASP/vasp.5 vasp.5
\end{Verbatim}


\section{Compilation}


\subsection{CPMD and NECI}

For historical reasons, CPMD and NECI are much more closely interwoven
than NECI is with VASP.  In addition, the tools used to compile CPMD and
NECI are very similar (but then they were written by the same people!).

The repository version of CPMD depends upon NECI, thus NECI must be
compiled first.  Various scripts take care of this for us.

CPMD and NECI have CONFIGURE subdirectories.  Each file in a CONFIGURE
subdirector contains the necessary information to compile the code using
a certain compiler.  Most of the time only the paths and flags for the
FFTW, LAPACK and BLAS libraries, given in the LFLAGS variable, will need
to be adjusted (if that).  It is necessary to use the same compiler to
compile CPMD and NECI.  We have compiled and tested the codebases with the
gfortran (4.2 and later), Portland and Intel compilers in 32 and 64 bit.
The mkconfig.sh scripts in the CPMD and NECI directories produce the
relevant makefiles, but this is better done via helper scripts.

Please note that not all the CPMD configure scripts will work: many of
them were supplied with CPMD and have never been used with our modified
version.  Please only use platforms that exist in the NECI/CONFIGURE
directory as well as the CPMD/CONFIGURE directory.  It is easy to make
your own configuration files using existing ones as a template.

Note that some of the platforms obtain LAPACK and BLAS as part of atlas,
ACML or MKL, so this will need to bechanged if different source libraries
are used.

Two versions of CPMD (and the corresponding NECI library) exist, gcpmd.x
(for gamma-point calculations) and kcpmd.x (for k-point calculations),
to take advantage of some substantial memory savings when running
gamma-point calculations, as all wavefunctions are then real.  This is
controlled via a C pre-processing statement.  As neci.x is only for
molecular calculations, it only exists in one form.

runmake.sh and compile are scripts for CPMD and NECI respectively which
control the process of creating makefiles and compiling the codebases.
They both refer to a ``platform'', which is just a name of one of the
configuration files in the CONFIGURE subdirectories.

compile by default generates new makefile (for both gamma-point and
k-point compilations) and does a clean build of neci.x:

\begin{Verbatim}[commandchars=@\[\]]
@lb[]NECI@rb[]@$ ./compile -h
usage: ./compile @lb[]options@rb[] @lb[]platform@rb[]
Generate new makefiles and do a clean build of neci.x using platform as the configuration.

If platform is not specified, then the platform given in .compileconf is used.
If .compileconf also doesn't exist, then the default (PC-PGI64) is used.

Options:
-d Compile with the compiler debug options on.
-f Fast: don't do a make clean before compiling.
-m Only make new makefiles.
\end{Verbatim}

In contrast, runmake.sh has a different default behaviour, in that it
doesn't produce new makefiles by default, and does not do clean builds:

\begin{Verbatim}[commandchars=@\[\]]
@lb[]CPMD@rb[]@$ ./runmake.sh -h
Usage: ./runmake.sh @lb[]-c@rb[] @lb[]-h@rb[]
Compile NECI (neci.x) and CPMD/NECI code for Gamma point (gcpmd.x)
code and for k-point sampling (kcpmd.x).
Warning: the option to set the NECI source directory are
*only* used when a new makefile is produced (i.e. requires the -m or -p
flag).

Options:
    -c  Recompile only CPMD routines.
    -d  Generate new makefiles for debugging.  Recompile (at least)
        CPMD/qmc routines and all of NECI.
    -g  Compile only Gamma point code.
    -k  Compile only K-point code.
    -m  Generate new Makefiles by running mkconfig scripts in NECI and
        CPMD directories, using the default platform (PC-PGI64 unless
        otherwise specified in .compileconf), and compiles.
    -n  Recompile only NECI routines.
    -p @lb[]32,64,platform@rb[]
        Produce makefile for @lb[]pgi-32bit,pgi-64bit,platform@rb[]
        compilation, where platform is an alternative configuration (eg
        for gfortran).
    -s @lb[]NECI source directory@rb[]
        Set the location of the directory containing the NECI source code.
        Warning: must be used only when new makefiles are produced (i.e.
        when -m or -p are specified).
    -h  Print this message.
\end{Verbatim}

Note that runmake.sh produces new makefiles for CPMD \textbf{and} for NECI,
and compiles neci.x, the NECI libraries needed for CPMD, and gcpmd.x
and kcpmd.x.  To aid compilation, the dest subdirectories in the CPMD and
NECI source directories contains the compiled objects for the gamma-point
code and the kdest subdirectories contain the compiled objects for the
k-point code.

Both the NECI and CPMD scripts default to compiling the codebases using
the Portland 64-bit compiler, if a platform is not specified either via
the command line or given in .compileconf, which is a text file which
contains the name of the desired platform.  Note that runmake.sh will
use the same platform for both the CPMD and NECI makefiles.

The CPMD and NECI source directories also contain controlling Makefiles
to further help the make process (and generally just act as wrappers
for the runmake.sh and compile scripts).  Run:

\begin{Verbatim}[commandchars=@\[\]]
make help
\end{Verbatim}

in each directory to see the various targets available.

To quickest way to compile both CPMD and NECI is to run:

\begin{Verbatim}[commandchars=@\[\]]
@lb[]CPMD@rb[]@$ make all
\end{Verbatim}

from within the CPMD source directory.


\subsubsection{.compileconf}

The .compileconf files are not under source code management and allow local defaults
to be set.  It is used both in the CPMD and NECI compilation scripts.

When running runmake.sh, please note that it uses the CPMD .compileconf information
for compiling NECI, rather than the NECI .compileconf file.  This is to ensure that
the same platform is used for both.

The settings in .compileconf are overridden by command line options, but override
any defaults in the compilation scripts.

.compileconf in its simplest form simply contains the name of the desired
platform, e.g.:

\begin{Verbatim}[commandchars=@\[\]]
PC@PYGbe[-]ifort64
\end{Verbatim}

will use the PC-ifort64 platform as the default.

The CPMD .compileconf can be used to set local defaults for more
variables---see the comments in runmake.sh for more details.  For example, to set
different defaults for the platform and the location of the NECI source:

\begin{Verbatim}[commandchars=@\[\]]
platform=PC-ifort64
NECIsrc=@textasciitilde[]/NECI/source
\end{Verbatim}


\subsubsection{File structure}

NECI files:
\begin{description}
\item[\textbf{NECI/neci.x}]
Standalone neci-executable (links to NECI/dest/neci.x).

\item[\textbf{NECI/dest/neci-cpmd.a}]
NECI library for CPMD gamma-point code.

\item[\textbf{NECI/kdest/neci-vasp.a}]
NECI library for CPMD k-point code.

\item[\textbf{NECI/dest/neci-cpmd.a}]
NECI library for VASP gamma-point code.

\item[\textbf{NECI/kdest/neci-vasp.a}]
NECI library for VASP k-point code.

\end{description}

CPMD files:
\begin{description}
\item[\textbf{CPMD/gcpmd.x}]
Gamma-point executable of the CPMD-NECI code (links to CPMD/dest/cpmd.x).
Must not be used for k-point calculations!

\item[\textbf{CPMD/kcpmd.x}]
k-point executable of the CPMD-NECI code (links to CPMD/dest/cpmd.x).
Must not be used for gamma-point calculations!

\end{description}


\subsection{VASP}

James has managed it.  It's not completely pleasant.  More to follow
once it's been made easier!


\section{testcode}

testcode is a set of scripts written by James Spencer that is used to
check that our programs produce the same results as they did before.
It is useful both for development work, to ensure that regression issues
are avoided, and testing successful compilations.

Every night the latest version of the codebase is checked out of the
subversion repository and tested against a variety of compilers, giving
confidence in the continued stability of the codebase.

testcode and the set of test jobs (both for NECI and CPMD-NECI), can be
checked out of the subversion repository:

\begin{Verbatim}[commandchars=@\[\]]
svn checkout https://wwmm.ch.cam.ac.uk/svn2/groups/alavi/testcode testcode
\end{Verbatim}

Please see the testcode documentation for more details.

\resetcurrentobjects


\hypertarget{run}{}\chapter{Run}


\section{NECI}

\begin{notice}{note}{Note:}
How to obtain FCIDUMP/density-fitting input files?
\end{notice}

\begin{Verbatim}[commandchars=@\[\]]
neci.x input@_file
\end{Verbatim}

If no file is given, then it takes input options from STDIN.  This is rarely useful, however.

NECI prints output to SDTOUT, so output needs to be captured in some way:

\begin{Verbatim}[commandchars=@\[\]]
neci.x input@_file @textgreater[] output@_file
neci.x nput@_file | tee output@_file
\end{Verbatim}


\section{CPMD-NECI}

The converged Kohn--Sham orbitals obtained from a \textbf{OPTIMIZE
WAVEFUNCTION} CPMD calculation can be used as input for a NECI
calculation.

In contrast to the molecular case, NECI calculations based upon
CPMD-generated wavefunctions are called from within CPMD itself.
This allows us to take advantage of many routines that CPMD already
possesses (FFT routines, initialisation, reading in the wavefunctions
etc.).

To run, specify \textbf{QMC} in the \textbf{\&CPMD} section of the CPMD input file.
\textbf{RESTART WAVEFUNCTIONS OCCUPATION DENSITY COORDINATES LATEST} must
also be specified.  Running CPMD (assuming it has been correctly compiled
with the appropriate NECI library) then calls NECI to read the NECI
input file and perform the desired calculation.

For gamma-point calculations:

\begin{Verbatim}[commandchars=@\[\]]
gcpmd.x input@_file @textgreater[] output@_file
\end{Verbatim}

For k-point calculations:

\begin{Verbatim}[commandchars=@\[\]]
kcpmd.x input@_file @textgreater[] output@_file
\end{Verbatim}

There are many other appropriate options that can be specified in the
CPMD input file rather than the NECI input file.  Please see the CPMD
manula and the local CPMD documentation detailing addtions the Alavi
group has made.

\resetcurrentobjects


\hypertarget{theory-index}{}\chapter{Theoretical review}

\resetcurrentobjects


\hypertarget{theory-introduction}{}\section{Introduction}

The energy of a system can be evaluated using a standard statisical mechanics result:
\begin{align}\begin{split}E = \frac{\operatorname{Tr}[H e^{-\beta H}]}{\operatorname{Tr}[e^{-\beta H}]}\end{split}\end{align}
We choose to work in a Slater Determinant space, which is, by construction, anti-symmetric.  In this space the energy expression becomes:
\begin{align}\begin{split}E &= \frac{\sum_{\veci} \bra D_{\veci} | H e^{-\beta H} | D_{\veci} \ket}{\sum_{veci} \bra D_{\veci} | e^{-\beta H} | D_{\veci} \ket} \\    &= \frac{\sum_{\veci} w_{\veci} \tilde{E}_{\veci}}{\sum_{\veci} w_{\veci}}\end{split}\end{align}
A given term in the numerator is simply the differential of the
corresponding term in the denominator.  There is a cleaner and more
efficient way of evaluating the numerator than differentiation, but we
will first turn our attention to the denominator.

We can expand each term into a closed path of $P$ steps through the discrete Slater Determinant space:
\begin{align}\begin{split}w^P_{\veci,\veci_1,\veci_2,\cdots,\veci_p,\veci} &= \sum_{\veci_1} \sum_{\veci_1} \cdots \sum_{\veci_P} \bra D_{\veci_1} | e^{\beta H/P} | D_{\veci_2}  \ket \bra D_{\veci_2} | e^{\beta H/P} | D_{\veci_3}  \ket \cdots \bra D_{\veci_P} | e^{\beta H/P} | D_{\veci_1}  \ket \\  & =  \sum_{\veci_1} \sum_{\veci_1} \cdots \sum_{\veci_P} \rho_{\veci_1\veci_2} \rho_{\veci_2\veci_3} \cdots \rho_{\veci_P\veci_1},\end{split}\end{align}
where the $\rho$ matrix consists of elements
$\rho_{\veci\vecj}=\bra D_{\veci} | e^{\beta H/P} | D_{\vecj} \ket$.

Each path does not necessarily visit $P-1$ determinants: ``hopping''
terms are allowed.  Due to the $\rho$ matrix being diagonally
dominant, paths containing small numbers of unique determinants will
tend to have a much greater contribution to the overall energy.

The size of the Slater determinant space grows factorially with the number
of electrons and virtual orbitals, making it impossible to sum together
all the paths.  Furthermore, the sign of a path is an incredibly poorly
behaved quantity.  It is possible to perform an analytical resummation
of the paths into objects we term graphs, where each graph contains
paths which only visit the vertices contained within the graph.

\begin{notice}{note}{Note:}
To come: pictures of paths --\textgreater{} graph.
\end{notice}

The expression for the energy now becomes a sum over Slater determinants and a sum graphs which originate from each Slater determinant:
\begin{align}\begin{split}E = \frac{\sum_{\veci} \sum_G w_{\veci}[G] \tilde{E}_{\veci}[G]}{\sum_{\veci} \sum_G w_{\veci}[G]}\end{split}\end{align}
Furthermore, the graphs have a much better sign behaviour: there are many graphs with a definite-positive weight, at least for graphs with less than 5 vertices, which makes a Monte Carlo approach feasible.

The resummation of paths into graphs still leaves a sum that is far too
large to be completely evaluated.  There are various approximations we
can apply.
\begin{enumerate}
\item {} 
Use a single reference reference approach, i.e. approximate the
energy with:
\begin{align}\begin{split}E = \frac{\sum_G w_{\vecz}[G] \tilde{E}_{\vecz}[G]}{\sum_G w_{\vecz}[G]}\end{split}\end{align}
where $\vecz$ refers to the reference (i.e. Hartree--Fock)
determinant.

This sum, in general, still contains too many terms (and grows
too rapidly with system size) to be of much use.

\item {} 
Truncate the sum at a certain graph size (e.g. restrict it to
two or three vertices).  This approach is referred to as a
\textbf{VERTEX SUM} approach.

\item {} 
Find a large graph that is a good approximation to the ground state
and is easy to evaluate.  Our current model is the single and
double excitation star, which contains all single and double
excitations connected to the reference determinant but ignores any
connections between the excited determinants.  In other words,
it couples all the single and double excitations together,
but only through the reference determinant.  This method is
referred to as a \textbf{VERTEX STAR} approach.  The star contains all
graphs in the sum truncated at the two vertex level and much more but
at no additional costly integrals to evaluate.  This makes it a very
attractive approach.

\end{enumerate}

\begin{notice}{note}{Note:}
To come: the propogation operator.
\end{notice}

\resetcurrentobjects


\hypertarget{theory-graph-evaluation}{}\section{Graph evaluation}

There are two approaches to evaluating the weight and energy contribution
of a given graph: either by diagonalising the $\rho$  matrix of
the graph or by diagonalising the Hamiltonian matrix of the graph.


\subsection{\textbf{RHODIAG}}

Diagonalisation of the $\rho$  matrix is referred to as \textbf{RHODIAG}
in the input documentation.

The $\rho$  matrix of the graph is the evaluation of the
high-temperature thermal density operator on the space of Slater
determinants spanned by the graph, or more formally:
\begin{align}\begin{split}\rho[G] = \sum_{\veci\vecj \in G} |D_\veci \ket \rho_{\veci\vecj}\bra D_\vecj|\end{split}\end{align}
We can obtain the eigenvectors and -values, $\{v_k\}$ and
$\{\lambda_k\}$ of $\rho[G]$ via matrix diagonalisation, and can
then use them to evaluate the weight of the graph:
\begin{align}\begin{split}w_{\veci}[G] & = \bra D_{\veci} | e^{-\beta H[G]} | D_{\veci} \ket \\               & = \sum_{kl} \bra D_{\veci} | v_k \ket \bra v_k |  e^{-\beta H[G]} | v_l \ket \bra v_l | D_{\veci} \ket \\               & = \sum_{kl} \bra D_{\veci} | v_k \ket \bra v_k | \lambda_l^P | v_l \ket \bra v_l | D_{\veci} \ket \\              & = \sum_{kl} \bra D_{\veci} | v_k \ket \lambda_l^P \delta_{kl} \bra v_l | D_{\veci} \ket \\               & = \sum_k \lambda_k^P \bra D_{\veci} | v_k \ket \bra v_k | D_{\veci} \ket\end{split}\end{align}
where we have applied the identity operator, $\sum_k |v_k \ket \bra v_k |$ twice and used:
\begin{align}\begin{split}e^{-\beta H[G]} | v_l \ket = \rho[G]^{P-1} \lambda_l | v_l \ket.\end{split}\end{align}
In a similar fashion, the energy contribution, $w_{\veci}\tilde{E}_{\veci}$ can be evaluated:
\begin{align}\begin{split}w_{\veci}\tilde{E}_{\veci} & = \bra D_{\veci} | H e^{-\beta H[G]} | D_{\veci} \ket \\                            & = \sum_{\vecj \in G} \bra D_{\veci} | H | D_{\vecj} \ket \bra D_{\vecj} | e^{-\beta H[G]} | D_{\veci} \ket \\                            & = \sum_{\vecj \in G} \sum_{kl} \bra D_{\veci} | H | D_{\vecj} \ket \bra D_{\vecj} | v_k \ket \bra v_k |  e^{-\beta H[G]} | v_l \ket \bra v_l | D_{\veci} \ket \\                            & = \sum_{\vecj \in G} \sum_{kl}  \bra D_{\veci} | H | D_{\vecj} \ket \bra D_{\vecj} | v_k \ket \lambda_l^P \delta_{kl} \bra v_l | D_{\veci} \ket \\                            & = \sum_{\vecj \in G} \sum_{k} \bra D_{\veci} | H | D_{\vecj} \ket \lambda_k^P \bra D_{\vecj} | v_k \ket \bra v_k | D_{\veci} \ket\end{split}\end{align}
The $\rho$ matrix elements can be evaluated using a Taylor expansion
with or without a Trotter approximation to improve the accuracy of the expansion.


\subsection{\textbf{HDIAG}}

Alternatively, we can use a slightly simpler approach which avoids having to evaluate
$\rho$ matrix by dealing with the Hamiltonian matrix directly.  This method is referred
to as \textbf{HDIAG} in the input documentation.  The two approaches give
essentially the same result.  In an analogous fashion to the application
of the $rho$ matrix in the space of the graph, we consider the Hamiltonian to be a propogator
acting in the space of the graph:
\begin{align}\begin{split}H[G] = \sum_{\veci\vecj \in G} |D_\veci \ket H_{\veci\vecj}\bra D_\vecj|\end{split}\end{align}
We can evaluate use this to evaluate the weight of the graph:
\begin{align}\begin{split}w_{\veci}[G] & = \bra D_{\veci} | e^{-\beta H[G]} | D_{\veci} \ket \\              & = \sum_{kl} \bra D_{\veci} | v_k \ket \bra v_k | 1 - \beta H[G] + \frac{\beta^2 H[G]^2}{2!} - \frac{\beta^3 H[G]^3}{3!} + \cdots | v_l \ket \bra v_l | D_{\veci} \ket \\              & = \sum_{kl} \bra D_{\veci} | v_k \ket (1 - \beta\lambda_l + \frac{\beta^2\lambda_l^2}{2!} - \frac{\beta^3\lambda_l^3}{3!} + \cdots) \delta_{kl} \bra v_l | D_{\veci} \ket \\              & =  \sum_k e^{-\beta\lambda_k} \bra D_{\veci} | v_k \ket \bra v_k | D_{\veci} \ket\end{split}\end{align}
where now $\{v_k\}$ and $\{\lambda_k\}$ are eigenvectors and
-values of the Hamiltonian matrix in the space of the graph.

Similarly, we can obtain the energy contribution of the graph:
\begin{align}\begin{split}w_{\veci}\tilde{E}_{\veci} & = \bra D_{\veci} | H e^{-\beta H[G]} | D_{\veci} \ket \\                            & = \sum_{\vecj \in G} \bra D_{\veci} | H | D_{\vecj} \ket \bra D_{\vecj} | e^{-\beta H[G]} | D_{\veci} \ket \\                            & = \sum_{\vecj \in G} \sum_{kl} \bra D_{\veci} | H | D_{\vecj} \ket \bra D_{\vecj} | v_k \ket \bra v_k | 1 - \beta H[G] + \frac{\beta^2 H[G]^2}{2!} - \frac{\beta^3 H[G]^3}{3!} + \cdots  | v_l \ket \bra v_l | D_{\veci} \ket \\                            & = \sum_{\vecj \in G} \sum_{kl}  \bra D_{\veci} | H | D_{\vecj} \ket \bra D_{\vecj} | v_k \ket e^{-\beta\lambda_l} \delta_{kl} \bra v_l | D_{\veci} \ket \\                            & = \sum_{\vecj \in G} \sum_{k} \bra D_{\veci} | H | D_{\vecj} \ket e^{-\beta\lambda_k} \bra D_{\vecj} | v_k \ket \bra v_k | D_{\veci} \ket\end{split}\end{align}
\resetcurrentobjects


\hypertarget{input-index}{}\chapter{Input options}

\resetcurrentobjects


\hypertarget{input-overview}{}\section{Overview}

The NECI input file is keyword driven and requires a minimal amount of information.

The NECI input file is divided into various sections, or input blocks: system, precalc, calc, integral and logging.  Of these, only the system and calc blocks are compulsory: all others are optional.  Inside each input block, it is possible to set a variety of options.  There are also three types of keywords that exist outside of an input block.

The order of the input blocks is not important (but certain orders are more logical than others), and nor is the order within a block, unless an option is only valid when a logical statement is true, in which case the relevant keyword for the logical statement must precede its related keywords.

General points to note:
\begin{itemize}
\item {} 
The input file is not case sensitive.  In the input documentation, the keywords are given in capitals and \textbf{emphasised} for clarity and options or data required are in square brackets.

\item {} 
Parameters which follow a keyword ought to be on the same line as the keyword,but this isn't a strict requirement.

\item {} 
A new line is required for each keyword, unless the keyword is an option of another keyword, in which case it ought to be on the same line.

\item {} 
Blank lines are ignored.

\item {} 
Comments are enclosed in parentheses.

\item {} 
Data items are terminated by space or comma.

\item {} 
Only the variables relevant to the desired run are required.

\item {} 
Unknown keywords return an error message and stop the run.

\item {} 
Sensible defaults are set, reducing the amount of information required from the input file.  There exist different sets of default options, allowing a large set of variables to be set with one command.

\end{itemize}

The overall structure, with a reasonably logical layout, is:

\textbf{TITLE}

\textbf{DEFAULTS}

\textbf{SYSTEM} {[}system type{]}

{[}System options{]}

\textbf{ENDSYS}

\textbf{PRECALC}

{[}PreCalc options{]}

\textbf{ENDPRECALC}

\textbf{CALC}

{[}Calc options{]}

\textbf{ENDCALC}

\textbf{INTEGRAL}

{[}Integral options{]}

\textbf{ENDINT}

\textbf{LOGGING}

{[}Integral options{]}

\textbf{ENDLOG}

\textbf{END}

\begin{notice}{warning}{Warning:}
This is a work in progress.  Many places (especially, but not
exclusively, where noted) need to be expanded and/or improved.

In addition, the following keywords are valid options, but are
\emph{not} documented:
\begin{quote}
\begin{itemize}
\item {} 
CALCREALPROD

\item {} 
CALCRHOPROD

\item {} 
DELTAH

\item {} 
DERIV

\item {} 
DETPOPS

\item {} 
DIAGSHIFT

\item {} 
EQUILSTEPS

\item {} 
EXCHANGE-ATTENUATE

\item {} 
HAPP

\item {} 
LINROOTCHANGE

\item {} 
MAXVERTICES

\item {} 
MODMPTHEORY

\item {} 
RESUMFCIMC

\item {} 
RHOAPP

\item {} 
RHOELEMS

\item {} 
SAVEPREVARLOGGING

\item {} 
SHIFTDAMP

\item {} 
STARPROD

\item {} 
STEPSSHIFT

\item {} 
SUMPRODII

\end{itemize}

In contrast, the following options are documented, but are \emph{not} valid
input options:
\begin{itemize}
\item {} 
BANDGAP

\item {} 
EXCHANGE-DAMPING

\item {} 
STOCHASTICTIME

\item {} 
MPMODTHEORY

\item {} 
SAVEPRECALCLOGGING

\end{itemize}
\end{quote}
\end{notice}

\resetcurrentobjects


\hypertarget{input-non-block}{}\section{Non-block level options}

The following options exist outside of any input block:
\begin{description}
\item[\textbf{TITLE}]
Takes the rest of the line as the title and prints it to output.  Useful for labelling the output.

\item[\textbf{DEFAULTS} {[} \textbf{DEFAULT} \textbf{FEB08} {]}]
Default: \textbf{DEFAULT}.
NECI has a default set of defaults (the \textbf{DEFAULT} set), which are sensible, safe defaults.
The \textbf{FEB08} set of defaults reflect furthr work, and change the defaults as follows:
\begin{itemize}
\item {} 
Fock-Partition-Lowdiag is set in the integral block.

\item {} 
RhoEpsilon= $10^{-8}$ in the calc block.

\item {} 
MCPATHS is set to be on in the logging block.

\end{itemize}

\end{description}

This can be specified anywhere in the input file outside of an input block.  All other options in the input file override the defaults.
\begin{description}
\item[\textbf{END}]
End of input file.  Not required, unless there is text after the input (e.g. comments or notes) which is not commented out or if the input file is given via STDIN.

\end{description}

\resetcurrentobjects


\hypertarget{input-system}{}\section{System}
\begin{description}
\item[\textbf{SYSTEM} {[}system type{]}]
Starts system block.  The system type must be provided and specifies
the basis upon which NECI performs a calculation.  ORDER is only valid
for some system types---see below.

\end{description}

{[}System options---see below.{]}
\begin{description}
\item[\textbf{ENDSYS}]
End the system input block.

\end{description}

The available system types fall into three categories:
\begin{itemize}
\item {} 
Read in data produced by a molecular computational chemistry package:
\begin{quote}
\begin{description}
\item[\textbf{READ} {[}\textbf{ORDER}{]}]
Perform a calculation on a (molecular) system based upon reading in the integrals produced
by a third-party program from disk.

\item[\textbf{GENERIC} {[}\textbf{ORDER}{]}]
Synonym for \textbf{READ}.

\end{description}
\end{quote}

\item {} 
Use a model system:
\begin{quote}
\begin{description}
\item[\textbf{BOX}]
Run a calculation on electrons confined to a box.  See \cite{TwoElBox}
for more details.

\item[\textbf{HUBBARD}]
Run a Hubbard model calculation.

\item[\textbf{UEG}]
Run a uniform electron gas calculation.

\end{description}
\end{quote}

\item {} 
Periodic systems:
\begin{quote}
\begin{description}
\item[\textbf{CPMD} {[}\textbf{ORDER}{]}]
Perform a calculation based upon the Kohn--Sham wavefunctions
produced by CPMD.  Only available in a combined CPMD-NECI
executable.

\item[\textbf{VASP}]
Perform a calculation based upon the Hartree--Fock wavefunctions
produced by VASP.  Only available in a combined VASP-NECI
executable.

\end{description}
\end{quote}

\end{itemize}
\begin{description}
\item[\textbf{ORDER}]
If \textbf{ORDER} is specified directly after \textbf{READ}, \textbf{GENERIC},
then a quick HF calculation in the space of the orbitals is performed.
The orbitals are then reordered according to the HF energies,
rather than using the orbital energies read in.

If \textbf{CPMD} is followed by \textbf{ORDER}, then the CPMD orbitals are
ordered, not according to their Kohn--Sham eigenvalues, but instead
according to their one-electron energies (i.e. with no exchange or
correlation).  \textbf{ORDER} is not valid for any other system type.

\end{description}


\subsection{General options}
\begin{description}
\item[\textbf{BANDGAP}]
Perform calculations for systems containing NEL, NEL+1, and NEL-1
electrons and extract the band gap energy.

\item[\textbf{COULOMB} {[}FCOUL{]}]
Multiply the strength of the coulomb interaction by FCOUL.

\item[\textbf{COULOMB-DAMPING ENERGY} {[}$\mu\ \beta${]}]
Damp the two-electron coulomb integrals, $\bra ab ||
c d\ket$ with factor $f(E_a)f(E_b)f(E_c)f(E_d)$ where
$f(E_a)=\operatorname{erfc}(\beta*(E_a-\mu))$.  A $\beta$
of 1 gives a damping range of 2; a $\beta$ of 40 gives a damping
range of 0.05.

\item[\textbf{COULOMB-DAMPING ORBITAL} {[}ORB $\beta${]}]
Damp the coulomb integrals as above, with MU set to be halfway between
the energies of ORB and ORB+1.

\end{description}

\begin{notice}{note}{Note:}
\textbf{COULOMB-DAMPING} is now disabled {[}26/7/06{]}.
\end{notice}
\begin{description}
\item[\textbf{CSF} {[}STOT{]}]
Default off.  Default STOT=0.

If specified, work in CSFs rather than determinants.  CSFs might not
function properly for some Monte Carlo, but should work for vertex
sums and diagonalization.   STOT is twice the magnitude of spin to
restrict the resultant space.

\item[\textbf{ELECTRONS} {[}NEL{]}]
Specify the number of electrons.  Required for all system types
apart from CPMD- or VASP-based  calculations.

\item[\textbf{ENERGY-CUTOFF} EMax]
Default off.

Reject basis functions with an (unscaled) energy larger than EMax.

\item[\textbf{EXCHANGE} {[}\textbf{ON} | \textbf{OFF}{]}]
Default \textbf{ON}.

Specify whether to include Exchange in the Slater-Condon rules.
If off, we are effectively reduced to a using Hartree multi-electron
wavefunctions rather than Slater determinants.

\item[\textbf{NEL} {[}NEL{]}]
Synonym for \textbf{ELECTRONS}.

\item[\textbf{NOSYMMETRY}]
Ignore all spatial symmetry information. This does not apply to
periodic calculations.

\item[\textbf{SPIN-RESTRICT} {[}LMS{]}]
Default off.  Default LMS=0.  Turns spin restriction on, limiting
the working space to the z-component of spin being LMS*2.

\item[\textbf{SYM} {[}$l_x,l_y,l_z$ iSym{]}]
Default off.

If specified, limit the working Slater determinant space to the set
of determinants with the specified symmetry quantum numbers. The symmetry
of a given orbital is specified in one of two ways:
\begin{quote}
\begin{description}
\item[model system calculations:]
3 quantum numbers, $l_x,l_y,l_z$.

\item[molecular or periodic calculations:]
Symmetry label, iSym, which corresponds to an irreducible
representation of the symmetry group.

\end{description}
\end{quote}

The symmetry label(s) of each orbital is included in the output,
from which the symmetry of the desired set of Slater determinants
can be evaluated (albeit in a somewhat laborious manner). All four
numbers are required, but only the relevant one(s) are used.

For Abelian symmetry groups, each symmetry is printed out in terms of
a propogating vector.  Internally an integer label is still used, according to
the formula:
\begin{align}\begin{split}i_{\textrm{SYM}} = \sum_{i=1}^3 p_i * 2^{15^{i-1}}\end{split}\end{align}
where $p_i$ are the components of the propogating vector.

\item[\textbf{USEBRILLOUINTHEOREM}]
Apply Brillouin's theorem: the net effect of single-excitations of
the Hartree--Fock determinant coupled to the Hartree--Fock determinant
is zero, so explicitly exclude such single excitations.

\end{description}


\subsection{Read options}
\begin{quote}
\begin{description}
\item[\textbf{BINARY}]
Read in an unformatted FCIDUMP file containing the molecular
integrals.

\item[\textbf{DensityFitted}]
Read in a set of density fitted coefficients and coulomb integrals
from files SAV\_DFaSOL and SAV\_Ta\_INT (generated by \cite{CamCasp}).
One-electron integrals are read in from HONEEL, which also contains
$\bra ij | ij \ket$ and $\bra ij | ji \ket$ integrals
(generated by readintOCC.x---a local package).

\item[\textbf{STARSTORE} {[}\textbf{BINARY}{]}]
Only the integrals required for a double-excitation star
calculation are read in from an FCIDUMP.  The one-electron
integrals, which we call TMAT elements, are stored as integrals
involving spatial orbitals, meaning that UHF is no longer
available.  In addition, only non-zero one-electron integrals i
are stored. The memory required to store the coulomb integrals
is massively reduced, from  $\frac{M^4}{8}$ to just
$\frac{N^{2} M^{2}}{2}$, where $M$ and $N$ are
the total number of orbitals and the number of occupied orbitals
respecitvely.  We only store the $\bra ij | ab \ket$
integrals in the UMAT array, where i and j are occupied, as well
as the $\bra ii | jj \ket$ and $\bra ij | ij \ket$
integrals over all states in the UMAT2D array.  Can only
be used for the 2-vertex sum and the 2-vertex star calculations.
If \textbf{BINARY} is also specfied, then an unformatted FCIDUMP file
is used.

\item[\textbf{STORE-AS-EXCITATIONS}]
Store determinants as a 4-integer list of orbitals excited from, and
orbitals excited to, in comparison to the reference determinant,
rather than as an n-electron list of the occupied orbitals
in the determinant. This means that the scaling is reduced to
$N^2M^2$ rather than $N^3M^2$, as we run through the
list for each excitation.  Currently only working for the 2-vertex
star Fock-Partition-Lowdiag calculations.

\end{description}
\end{quote}


\subsection{Model system options}

The following apply to electron in a box, Hubbard model and uniform
electron gas calculations, unless otherwise noted.
\begin{description}
\item[\textbf{BOXSIZE} {[}A {[}BOA COA{]} {]}]
Required for \textbf{UEG} and \textbf{BOX} calculations.  BOA and COA optional. Default
BOA=COA=1.

Set lattice constants a, b and c respectively, where b and c are defined
as a ratio of a.

\item[\textbf{CELL} {[}NMAXX NMAXY NMAXZ{]}]
Maximum basis functions for each dimension.  For \textbf{HUBBARD} and \textbf{UEG},
functions range from -NMAXi to NMAXi, but for \textbf{BOX}, they range from 1
to NMAXi, where i=X,Y,Z.

\end{description}


\subsection{Box options}
\begin{description}
\item[\textbf{ALPHA} {[}$\alpha${]}]
Sets TALPHA=.true. and defines $\alpha$.

Integrate out the Coulomb singularity by performing part in real
space and part in Fourier space, with the division according to the
screening parameter $\alpha$.  See \cite{TwoElBox}.

\item[\textbf{MESH} {[}NMSH{]}]
Default NMSH=32.

Number of mesh points used for calculating integrals.

\end{description}


\subsection{Hubbard options}
\begin{description}
\item[\textbf{B} {[}BHUB{]}]
Default=0.

Sets B (hopping or kinetic energy) parameter for the Hubbard model.

\item[\textbf{U} {[}UHUB{]}]
Default=0.

Sets U (on-site repulsion) parameter for the Hubbard model.

\item[\textbf{REAL}]
Set Hubbard model to be in real space.

\item[\textbf{APERIODIC}]
Hubbard model is set to be not periodic.

\item[\textbf{TILT} {[}ITILTX ITILTY{]}]
Default off.

The Hubbard model is tilted and the unit vectors are
(x,y)=(ITILTX,ITILTY) and (-y,x).  Require x $\ge$ y.

\end{description}


\subsection{UEG options}
\begin{description}
\item[\textbf{EXCHANGE-CUTOFF} {[}$R_c${]}]
Use the method detailed in \cite{AttenEx} for calculating the exchange
integrals.

Sets cutoff distance $R_c$ for the exchange electron-electron
potential.  If $R_c$ is not explicitly set, it will
be set to be equivalent to a sphere of the same volume as the cell,
$R_c=(\frac{\Omega}{4\pi/3})^{1/3}$.

\item[\textbf{EXCHANGE-DAMPING} {[}$R_c${]}]
Sets cutoff parameter $R_c$ for attenuated potential
$V(r)=\frac{\operatorname{erfc}(r/R_c)}{r}$.  If $R_c$ is not explicitly set,
it will be set to be equivalent to a sphere of the same volume as the cell,
$R_c=(\frac{\Omega}{4\pi/3})^{1/3}$.

\end{description}

\resetcurrentobjects


\hypertarget{input-precalc}{}\section{PreCalc}

\begin{notice}{note}{Note:}
George, please improve!  My interpretations also need to be checked...
\end{notice}
\begin{description}
\item[\textbf{PRECALC}]
Start pre-calculation block.  This chooses which weighting parameters
to use in Monte Carlo calculation , in order to give minimum variance.
This is an optional input block, and is not required if the default
parameters are to be used, or are specified explicitly in the \textbf{CALC}
input block.  Currently, only the \textbf{IMPORTANCE} parameter, the C
\textbf{EXCITWEIGHTING} parameter, and the a \& b optimal parameters are
searched for simultaneously, optimised and parsed through the the
main program.

\end{description}

{[}PreCalc options---see below.{]}
\begin{description}
\item[\textbf{ENDPRECALC}]
End the precalc input block.

\end{description}


\subsection{PreCalc Options}

\begin{notice}{note}{Note:}
George:
What on earth are the following:
\begin{itemize}
\item {} 
A,B etc parameters.

\item {} 
XXX.

\item {} 
U matrix.

\end{itemize}
\end{notice}
\begin{description}
\item[\textbf{VERTEX} {[}\textbf{HDIAG} \textbf{RHODIAG}{]} {[}\textbf{SUM} \textbf{MC}{]}]
Similar to the methods section in the \textbf{CALC} block, specify the
method to use at the next vertex level (the first entry is for the
second vertex level) in searching for the best parameters, using the
hamiltonian matrix diagonaltisation (\textbf{HDIAG}) or the $\rho$
matrix diagonalisation (\textbf{RHODIAG}). After this is specified, on
the same line, specify whether to calculate the expected variance
using the full sum at this vertex level (\textbf{SUM}), or using a Monte
Carlo sum (\textbf{MC}).

Currently, only the \textbf{HDIAG} routine works when performing a MC
expected variance, though the diagonalisation of the \emph{rho} matrix
now works with the full sum.

For example:

\begin{Verbatim}[commandchars=@\[\]]
**VERTEX** **HDIAG** **SUM**
XXX
XXX
**VERTEX** **HDIAG** **MC**
XXX
\end{Verbatim}

where XXX are the vertex options (see below).

If no further options are specified for a given vertex level, the
optimum values of all the \textbf{EXCITWEIGHTING} variables will be found,
but not used in the main program.

\item[\textbf{GRIDVAR} {[}A\_ExcitFromStart{]} {[}A\_ExcitFromEnd{]} {[}A\_ExcitFromStep{]} {[}B\_ExcitToStart{]} {[}B\_ExcitToEnd{]} {[}B\_ExcitToStep{]}]
Produce a 3D map of the variance landscape, but do not explicitly
calculate the minimum. Vaules for the A parameter start, end, and
step must be specified, followed by the same for the B parameter.

\item[\textbf{LINEVAR} {[}G\_VMC\_PIStart{]} {[}G\_VMC\_PIEnd{]} {[}G\_VMC\_PIStep{]}]
Same as GRIDVAR, but produce a 1D line for one variable - currently
only working for the \textbf{IMPORTANCE} parameter and the U-matrix element
parameter. Shows change in expected variance over the designated range
of values.

\item[\textbf{MEMORISE}]
All the graphs, their excitation generators, weights, energies, and
unbiased probabilities should be stored in the memory.  Speeds up
the calculation of the variance by around twofold. However, this
cannot be used for large systems.  \textasciitilde{}31000 4v graphs, or 22000 3v
graphs was the maximum for the nitrogen dimer with the VQZ basis .

Currently only available for \textbf{MC} precalculations. If this is not
set, then only the first node excitation generators are stored---there
should be enough memory for this

\item[\textbf{PREGRAPHEPSILON} {[}PREWEIGHTEPS{]}]
Default $10^{-8}$.

Gives the threshold above which the weight of a graph must be if
they are to be included in the full precalc variance calculation.

If the graph weight is below the threshold, then the probability
of obtaining the graph does not need to be calculated, and so
the optimisation routine is faster (but less accurate) at higher
thresholds.

\item[\textbf{TOTALERROR} {[}Desired Error from main calculation{]}]
Calculates the required number of cycles so that the final
error from the Monte Carlo calculation is equal to the error specified.

Only valid if the highest vertex level in the precalculation stage is the
same as the highest vertex level in the main MC calculation, i.e. the
highest MC vertex level is independently optimised.

The optimum vertex splitting is also calculated.

\item[\textbf{TOLERANCE} {[}TOLERANCE{]}]
Default 0.1.

The fractional precision to which the optimum parameter is obtained,
using the minimisation method.

\item[\textbf{TRUECYCLES} {[}No.of cycles{]}]
Specify the total number of MC cycles that we want to use in the
main calculation.  The precalculation stage will then automatically
split the cycles between the vertex levels in the main calculation,
according to how the use statements indicate the parameters are to
be split.

The cycles are then split so to best minimise the overall variance
of the run, assuming that the variance of the whole run is simply
the sum of the variances of the individual vertex results.

\end{description}


\subsection{Vertex options}

For the specified vertex level:
\begin{description}
\item[\textbf{CYCLES} {[}nCYCLES{]}]
Specify the number of graphs generated in the MC algorithm
for each expected variance calculation in the parameter minimisation algorithm.

Applies only to a vertex level which evaluates the expected variance
using the MC algorithm in the \textbf{VERTEX} line.

\item[\textbf{NONE}]
Perform no calculations or optimisations.

\item[\textbf{UEPSILON} {[}UEPSILON{]}]
Default UEPSILON is 0.

Find the  optimum C \textbf{EXCITWEIGHTING} coefficient and pass through to the main program,
unless setting C to be zero changes the expected variance by an amount less
than UEPSILON, in which case C is set to be zero.

The calculation of the U matrix elements can be time-consuming
in a real MC simulation, yet can have a negligible effect on the
final result.  In these cases, setting the C coefficient to be zero
makes the full MC simulation much faster.

With the the default value of UEPSILON, the optimum value of C will
always be used in the main program.

\item[\textbf{FINDC}]
Find the optimum C \textbf{EXCITWEIGHTING} parameter.

The C \textbf{EXCITWEIGHTING} parameter. will only be found if this
flag is set, or if a \textbf{UEPSILON} is set.

By default, the optimisation algoritm will only seek to find the
values which give the minimum expected variance by varying the A and
B \textbf{EXCITWEIGHTING} parameters, (or the parameters in the weighting
scheme specified).

\item[\textbf{FINDD}]
Find the optimum D \textbf{EXCITWEIGHTING} parameter for this vertex
level (g\_VMC\_ExcitToWeight2).

\item[\textbf{USED}]
Pass the optimum D \textbf{EXCITWEIGHTING} parameter found at this vertex level
through to the main calculation.

\item[\textbf{FINDIMPORT}]
Run the optimisation algorithm for the IMPORTANCE
parameter.

The optimised value will be found printed out,
but will not be passed through to the main calculation.

Can only be set for vertex levels of three or higher for
obvious reasons.

\end{description}

\begin{notice}{note}{Note:}
And the obvious reasons are?
\end{notice}
\begin{description}
\item[\textbf{USEIMPORT}]
Find the optimal \textbf{IMPORTANCE} parameter, and use in the main
calculation.

As for \textbf{FINDIMPORT}, can only be set for vertex levels of three
or higher.

\item[\textbf{USE} {[}MC\_VERTEX\_LEVEL\_1{]} {[}MC\_VERTEX\_LEVEL\_2{]} ...]
Use in the main calculation the parameters calculated from the specified precalc
vertex levels, rather than any other, are to be passed through and
used in the main program when performing a MC at one of the vertex
levels specified. Any given vertex level can only be specified once
in all the \textbf{USE} statements.

Vertex levels in the main calculation which are not specified in one of
the precalc \textbf{USE} statements, will
use the parameters which are given in the \textbf{CALC} block of
the input file.

A \textbf{USE} statement on its own will only calculate the
A and B \textbf{EXCITWEIGHTING} parameters (or any of the other weighting
scheme parameters specified) and use them for all MC vertex levels
in the main calculation, unless \textbf{FINDC} or \textbf{UEPSILON} is specified,
in which case for the C parameter to also be used.

\end{description}

\begin{notice}{note}{Note:}
This is somewhat confusing.  How does it fit in with USEIMPORT etc.?
\end{notice}

\resetcurrentobjects


\hypertarget{input-calc}{}\section{Calc}
\begin{description}
\item[\textbf{CALC}]
Start calculation block.  This chooses what calculation to do.

\end{description}

{[}Calculation options---see below.{]}
\begin{description}
\item[\textbf{ENDCALC}]
End the calculation input block.

\end{description}


\subsection{General options}
\begin{description}
\item[\textbf{ALLPATHS}]
Choose all determinants (i.e. set NPATHS = -1).

\item[\textbf{BETA} {[}BETA{]}]
Set $\beta$.

\item[\textbf{BETAOVERP} {[}BETAP{]}]
Default= 1.d-4.

Set $\beta/P$.

\item[\textbf{DELTABETA} {[}DBETA{]}]
Set $\delta\beta$.  If given a negative value, calculate it exactly.

\begin{notice}{note}{Note:}
What is this used for?
\end{notice}

\item[\textbf{DETINV} {[}DETINV{]}]
Specify the root determinant for which the complete vertex series is
worked out, using the determinant index obtained from a previous
calculation.  If \textbf{DETINV} is negative, the NPATHS calculations
are started at this determinant.

\item[\textbf{EXCITE} {[}ICILEVEL{]}]
Default 0.

Excitiation level at which to truncate determinant list.  If ICILEVEL=0
then all determinants are enumerated.

\item[\textbf{EXCITATIONS} {[}\textbf{OLD} \textbf{NEW}{]}]
For generation of up to double excitations use the old (completely
reliable), or new (faster, but does not work for more than 2-vertex
level SUMS) routine

\begin{notice}{note}{Note:}
You can now use the \textbf{NEW} routines for all methods, right?
What is the difference between \textbf{NEW} and \textbf{OLD}?  (If it doesn't say, how else
can a user make an informed decision as to which to use?)
\end{notice}

\item[\textbf{EXCITATIONS} {[}\textbf{SINGLES} \textbf{DOUBLES}{]}]
Default is to use all excitations.

Restricts determinants which are allowed to be connected to the
reference determinant to be either single or double excitations of
the reference determinant.

Applies only to the \textbf{VERTEX} {[}\textbf{SUM} \textbf{STAR}{]} \textbf{NEW} methods.

\item[\textbf{HAMILTONIAN} {[}\textbf{STAR}{]}]
Store the Hamiltonian.  This is defaulted to ON if \textbf{ENERGY} is set,
but can be used without \textbf{ENERGY}.
\begin{description}
\item[\textbf{STAR}]
Only the connections between the root determinant and its
excitations should be included in the Hamiltonian and not
off-diagonal elements between excited determinants.

\end{description}

\item[\textbf{MAXVERTICES} {[}MAXVERTICES{]}]
Give the vertex level of the calculation.  Cannot be used in
conjunction with a \textbf{METHODS} block.

\item[\textbf{METHOD} {[}Method option(s){]}]
Specify the method for a graph theory calculation.  See Method
options for the available methods.

Can only be specified once if used outside of the methods block,
in which case the given method is applied to all vertex levels.

\item[\textbf{METHODS}]
Begin a methods block.  This allows a different method for each vertex
level.  Each vertex level can contain \textbf{EXCITATIONS}, \textbf{VERTICES},
\textbf{CYCLES} and \textbf{CALCVAR} keywords.
Each \textbf{METHOD} line and the options that follow it detail the calculation
type for the next vertex level, with the first \textbf{METHOD} line used for the
the second-vertex level, unless over-ridden with the \textbf{VERTICES} option.

The block terminates with \textbf{ENDMETHODS}.

For example:

\begin{Verbatim}[commandchars=@\[\]]
METHODS
   METHOD VERTEX SUM NEW
   EXCITATIONS DOUBLES
   METHOD VERTEX STAR POLY
   EXCITATIONS SINGLES
   VERTICES 2
ENDMETHODS
\end{Verbatim}

sets the first method, at the two-vertex level, to be a complete 2-vertex
sum of only doubles, and the second method, overriden to be also at
the two-vertex level, to be a vertex star of singles.

Similarly:

\begin{Verbatim}[commandchars=@\[\]]
METHODS
   METHOD VERTEX SUM NEW
   METHOD VERTEX SUM MC
   @lb[]Monte Carlo options@rb[]
ENDMETHODS
\end{Verbatim}

performs a full sum at the two-vertex level and a Monte Carlo
calculation at the three-vertex level.

\item[\textbf{ENDMETHODS}]
Terminate a methods block.

\item[\textbf{PATHS} {[}option{]}]
Select the number of determinants taken to be the root of the graph.
Usually set to 1.  Valid options:
\begin{quote}
\begin{description}
\item[NPATHS]
Choose the first NPATHS determinants and calculate RHOPII etc.

\item[\textbf{ALL}]
Choos all determinants (same as ALLPATHS).

\item[\textbf{ACTIVE}]
Choose only the active space of determinants: the degenerate
set containing the highest energy electron.

\item[\textbf{ACTIVE} \textbf{ORBITALS} nDown nUp]
Set the active space to be nDown and nUp orbitals respectively
from the Fermi level

\item[\textbf{ACTIVE} \textbf{SETS} nDown nUp]
Set the active space to be nDown and nUp degenerate sets
respectively from the Fermi level

\end{description}
\end{quote}

\item[\textbf{RHOEPSILON} {[}RHOEPSILON{]}]
Set the minimum significant value of an element in the $rho$
matrix as a fraction of the maximum value in the $rho$ matrix.
Matrix elements below this threshold are set to be 0.

\item[\textbf{STARCONVERGE} {[}STARCONV{]}]
Default 1.d-3.

Set the convergence criteria for whether a roots to the star graph
is significant.

\item[\textbf{TROTTER}]
Default.

Perform a Trotter decomposition to evaluate the $rho$ matrix elements.

\item[\textbf{TIMESTEPS} {[}I\_P{]}]
Set P, the timesteps into which $e^{-\beta H}$ is split.  Automatically
sets $\beta/P=0$ (as required) but returns an error message if \textbf{BETAOVERP}
is also used.

\item[\textbf{WORKOUT} {[}NDETWORK{]}]
Sets the number of determinants which are worked out exactly.

\begin{notice}{note}{Note:}
What is this used for?
\end{notice}

\item[\textbf{VERTICES}]
Only available inside a methods block.

By default, each method takes a
number of vertices corresponding to its index within the methods
block, the first methods corresponding to the 2-vertex level, the
second to the 3-vertex level, and so on.  \textbf{VERTICES} overrides this,
and allows the vertex level of each method to be explicitly specified,
enabling, for example, the 2-vertex level to be split up and the
contributions from single and double excitations of the reference
determinant to be handled separately.

\end{description}


\subsection{Method options}
\begin{description}
\item[\textbf{VERTEX SUM} {[}\textbf{OLD} \textbf{NEW} \textbf{HDIAG}{]} {[}\textbf{SUB2VSTAR}{]} {[}\textbf{LOGWEIGHT}{]}]
Calculate the vertex sum approximation.
\begin{description}
\item[\textbf{OLD}]
Diagonalise the $\rho$ matrix using the original method.

\item[\textbf{NEW}]
Diagonalise the $\rho$ matrix using a more modern, more
efficient method.  Recommended.

\item[\textbf{HDIAG}]
Diagonalise the Hamiltonian matrix instead of the $rho$ matrix
in order to calculate the weight and energy contribution of each graph.

\item[\textbf{SUB2VSTAR}]
Remove paths which were present in the 2-vertex
star for each graph.  If this is specified for ANY vertex level,
it applies to all \textbf{SUM} and MC vertex levels.

\item[\textbf{LOGWEIGHT}]
Form Q as a multiplication of factors from graphs.  This results
in the quantity $\operatorname{log} w$ being used instead
of $w$, which also translates to the energy expression
only involving $\tilde{E}$ not weights.  Hopefully this
is size-consistent.

\end{description}

\begin{notice}{warning}{Warning:}
\textbf{SUB2VSTAR} and \textbf{LOGWEIGHT} are experimental options.
\end{notice}

\item[\textbf{VERTEX} {[}\textbf{MC} \textbf{MCMETROPOLIS} \textbf{MCDIRECT} \textbf{MCMP}{]} {[}\textbf{HDIAG}{]}]
Perform a Monte Carlo calculation.
\begin{description}
\item[\textbf{MCDIRECT}]
Perform direct stochastic sampling for the graph theory vertex sum
method, dividing each freshly generated graph by its normalized
generation probability.

If \textbf{MULTIMCWEIGHT} is specified then
the sampling generates graphs from all weighted levels using
the weighting - a single MC calculation is performed.

If \textbf{MULTIMCWEIGHT} is not specified (default), a separate
MC calculation is performed at each vertex level.  Combined
statistics are printed.

\begin{notice}{warning}{Warning:}
\textbf{MULTIMCWEIGHT} is not documented.  Use with great caution.
\end{notice}

\item[\textbf{MCMP}]
Perform direct stochastic sampling, as in \textbf{MCDIRECT},
but for the Moller--Plesset method.

\item[\textbf{MC} or \textbf{MCMETROPOLIS}]
Perform Metropolis Monte Carlo.

This may be performed in a number of ways. The way is
chosen by the location of the \textbf{VERTEX} \textbf{MC} command.

\begin{notice}{warning}{Warning:}
The following options appear in INPUT\_DOC but, however, are incredibly
poorly documented.  In particular:
\begin{itemize}
\item {} 
No detail on the arguments the options take (e.g. \textbf{BIAS}).

\item {} 
Some options documented don't exist (e.g. \textbf{SINGLE}, \textbf{BIAS}, \textbf{MULTI}, \textbf{STOCHASTICTIME}).

\item {} 
Sufficient tests are not present in the test suite.

\end{itemize}

Do not use.

The ``options'' are:

\begin{Verbatim}[commandchars=@\[\]]
 **STOCHASTICTIME**
     may also be specified to perform stochastic
     time simulations with a given **BIAS**

**SINGLE**
    MC is performed at a single vertex level using a composite
    1-vertex graph containing a full sum previously performed.

**BIAS**
    is used to choose whether a step selects a composite
    (all lower levels) or a normal (this level) graph.  Stochastic
    time MC is performed. This can only be specified in the
    **METHODS** section, and only at the last vertex level.
    Uses **EXCITWEIGHTING** for excitation generation weighting
    and **IMPORTANCE** for graph generation weighting

**MULTI**
    MC is performed at a multiple vertex levels, but still
    using a composite 1-vertex graph containing a full sum
    previously performed. MULTI should be specified in all the
    (contiguous) vertex levels to be included (not composited)
    in the MC.  **BIAS** is used to choose whether a step
    selects a composite (all lower levels) or a normal (the
    **MULTI** levels) graph.  **MULTIMCWEIGHT** is specified
    for each **MULTI** level, and gives a relative weighting
    of selecting the vertex level graphs once a non-composite
    graph is chosen.  Stochastic time MC is performed.
    This can only be specified in the **METHODS** section.
    Once **MULTI** has been specified, it must be specified
    on all subsequent vertex levels in a **METHODS** section.
    Uses **EXCITWEIGHTING** for excitation generation weighting
    and **IMPORTANCE** for graph generation weighting

**FULL**
    Does  MC at all levels using BIAS to bias the levels,
    **EXCITWEIGHTING** for excitation generation, and
    **IMPORTANCE** to for graph generation weighting.  This is
    only available *WITHOUT* a **METHODS** section. If **HDIAG**
    is specified, the H-diagonalizing routine is used, otherwise,
    the rho-diagonalizer is used.  **HDIAG** is automatically
    specified for **MCMP**.
\end{Verbatim}
\end{notice}

\end{description}

\item[\textbf{VERTEX} \textbf{SUM} \textbf{READ}]
Read in from pre-existing MCPATHS file for that vertex level.
Only really useful in a \textbf{METHODS} section.

\item[\textbf{VERTEX} \textbf{STAR} {[}\textbf{ADDSINGLES} \textbf{COUNTEXCITS}{]} {[}star method{]} {[}\textbf{OLD} \textbf{NEW} {[}\textbf{H0}{]} {]}]
Construct a single and double excitation star from all determinants
connected to the root (ignoring connections between those dets).
See \cite{StarPaper} for more details.
\begin{description}
\item[\textbf{ADDSINGLES}]
Extend the star graph approach.

Add the single exctitaions which are en-route to each double
excitation to that double excitation as spokes, and prediagonalize
the mini-star centred on each double excitation.  For example,
if the double excitation is (ij-\textgreater{}ab), then singles
(i-\textgreater{}a),(i-\textgreater{}b),(j-\textgreater{}a) and (j-\textgreater{}b) are created in a star with
(ij-\textgreater{}ab), the result diagonalized, and the eigenvalues and
vectors used to create a new spoke of the main star graph.

Only works with \textbf{NEW}.

\item[\textbf{COUNTEXCITS}]
Run through all the symmetry allowed excitations
first and count the connected determinants on the star.  Enables the
memory requirements to be reduced as only connected determinants need
to be stored. However, the time taken is increased, as it is necessary
to run through all determinants in the star twice. Especially useful
for large systems with memory restraints, when density fitting has
necessarily turned off symmetry. Also useful if a \textbf{RHOEPSILON}
has been set to a large value so that many of the symmetry allowed
excitations  will be counted as disconnected.

\begin{notice}{note}{Note:}
Useful for periodic calculations?  Does it need just the
symmetry info or the transition matrix elements as well?
\end{notice}

\item[\textbf{OLD}]
Use a pre-generated list of determinants using the excitation
routine version specified in \textbf{EXCITATIONS} \textbf{OLD} or
\textbf{EXCITATIONS} \textbf{NEW}.

\item[\textbf{NEW}]
Generate determinants on the fly without storing them, using
the \textbf{NEW} excitation routine.  Much more memory efficient.

\item[\textbf{NEW H0}]
Use the zeroth order N-particle Hamiltonian (shifted such that
$H^0_{ii} = H_{ii}$) rather than the fully interacting
Hamiltonian to generate the roots of the polynomial.

\begin{notice}{note}{Note:}
And you'd want to use \textbf{NEW H0} why exactly?
\end{notice}

\end{description}

The available star methods are:
\begin{quote}
\begin{description}
\item[\textbf{DIAG}]
Perform a complete diagonalization on the resultant matrix.  This can
be very slow. However, by specifying \textbf{LANCZOS} in the \textbf{CALC}
block, you can do a Lanczos diagonalisation, which scales much
better. \textbf{EIGENVALUES} can also be specify to only evaluate the
first few eigenvalues.

\item[\textbf{POLY}]
Use the special properties of the matrix to find the roots of
the polynomial and uses them to calculate the relevant values.
This is order $\text{Ngraph}^2$.

\begin{notice}{note}{Note:}
Ngraph==nDets?
\end{notice}

\item[\textbf{POLYMAX}]
Similar to \textbf{POLY} but only finds the highest root of the polynomial, so
is order Ngraph.  It can be used when P is very large (i.e. $\beta$
is very large, e.g. 40).

\item[\textbf{POLYCONVERGE}]
Similar to \textbf{POLY} but adds i out of N $\lambda_i$
roots, such that $(N-i) \lambda_i^P < 10^{-3}$, i.e. we
evaluate enough roots such that a very conservative error
estimate of the contribution of the remaining roots is
negligible.

\item[\textbf{POLYCONVERGE2}]
Similar to \textbf{POLYCONVERGE} but requires
$w(1..i) (N-i) \lambda_i^P < 10^{-3}$, where
$w(1..i)$ is the cumulative sum of $\lambda_i^P$,
which should be a better estimate of the convergence.

\end{description}
\end{quote}

The following are experimental star methods:
\begin{quote}
\begin{description}
\item[\textbf{MCSTAR}]
Use a basic implementation of the spawning algorithm in
order to sample the star graph stochastically. The sampling uses
elements of the Hamiltonian matrix rather than the $rho$ matrix,
so there will be some differences in the converged energy
compared to a \textbf{VERTEX STAR NEW} calculation.

Many of the \textbf{FCIMC} options are also available with MCStar,
and there are also some extra one.

\item[\textbf{NODAL}]
Prediagonalise a completely connected set of virtuals for each
set of occupied (i,j) spin-orbitals. The diagonalised
excitations are then solved as a star graph. Must be used
with \textbf{NEW}.

\item[\textbf{STARSTARS}]
Use an approximation that the change of eigenvalues and the
first element of the eigenvectors of the star graph is linear with
respect to multiplying the diagonal elements by a constant. Once
this scaling is found, all stars of stars are prediagonalised,
and reattached to the original graph. This results in N\textasciicircum{}2 scaling,
where N is the number of excitations.

\item[\textbf{TRIPLES}]
Prediagonalise an excited star of triple excitations from each
double excitation, reattach the eigenvectors, and solves
the complete star. Currently only available with `\textbf{NEW}`,
`\textbf{COUNTEXCITS}` and `\textbf{DIAG}`.

\end{description}
\end{quote}

\end{description}


\subsubsection{Experimental methods}
\begin{description}
\item[\textbf{VERTEX} \textbf{FCIMC} {[}\textbf{MCDIFFUSION}{]}]
Perform Monte Carlo calculations over pure determinant space, which
is sampled using a series of `particles' (or `walkers').

The walkers are not necessarily unique and must be sorted at every
iteration.  Each walker has its own excitation generator.

\textbf{MCDIFFUSION} is a completely particle-conserving diffusion
algorithm and is much more experimental.

\textbf{FCIMC} and \textbf{MCDETS} calculations share many of the same options
(see Walker Monte Carlo options, below).

\item[\textbf{VERTEX} \textbf{GRAPHMORPH} {[}\textbf{HDIAG}{]}]
Set up an initial graph and systematically improve it, by applying the
$rho$ matrix of the graph and its excitations as a propagator
on the largest eigenvector of the graph. From this, an improved graph
is stochastically selected, and the process is repeated, lowering
the energy. If \textbf{HDIAG} is specified, it is the hamiltonian matrix
elements which determine the coupling between determinants, and it
is the hamiltonian matrix which is diagonalised in each iteration
in order to calculate the energy.

\begin{notice}{note}{Note:}
\textbf{GRAPHMORPH} has not been tested with complex wavefunctions.  It will
almost certainly not work for them.
\end{notice}

\item[\textbf{VERTEX} \textbf{MCDETS}]
Perform Monte Carlo calculations over pure determinant space, which
is sampled using a series of `particles' (or `walkers').

\textbf{MCDETS} is similar to \textbf{FCIMC} but maintains at most one
`particle' at each determinant, which may then contain subparticles
(which correspond to the individual `walkers' in \textbf{FCIMC}), in
a binary tree.  This makes some efficiency savings where the same
information about a determinant is not duplicated.

\textbf{FCIMC} and \textbf{MCDETS} calculations share many of the same options
(see Walker Monte Carlo options, below).

\item[\textbf{VERTEX} \textbf{RETURNPATHMC}]
Use a spawning algorithm which is constrained in three ways:
\begin{quote}

\#.  a particle can only be spawned where it will increase its
excitation level with respect to the reference determinant or
back to where it was spawned from.
\#. they will spawn back to where their parents were spawned from
with probability PRet, which is specified using \textbf{RETURNBIAS}.
\# length of spawning chain must be less than the maximum length
given by \textbf{MAXCHAINLENGTH}.

\begin{notice}{note}{Note:}
How can a particle be restricted to spawning to spawning at most
back to where it was spawned from \emph{and} have a probability of
spawning back to where its parent was spawed from?
Documentation \emph{must} be clearer.
\end{notice}
\end{quote}

This attempts to circumvent any sign problem in the double
excitations and the HF, and hopefully this will result in a more stable
MC algorithm. It remains to be seen if this approach is useful.  Should
revert to the star graph in the limit of the return bias tending to 1 or
the length of the spawn chain tending to 1.

\begin{notice}{note}{Note:}
\textbf{FCIMC}, \textbf{GRAPHMORPH}, \textbf{MCDETS} and \textbf{RETURNPATHMC} have not
been tested with complex wavefunctions.  It will almost certainly
not work for them.

All four are experimental options under development.
\end{notice}

\end{description}


\subsection{Walker Monte Carlo options}

The following options are applicable for both the \textbf{FCIMC} and \textbf{MCDETS} methods:

\begin{notice}{note}{Note:}
I have made some guesses on the following option names.  Clearly some keys are broken
on George's keyboard.  Specifically:

\begin{Verbatim}[commandchars=@\[\]]
StepsSft --@textgreater[] STEPSSHIFT
SftDamp  --@textgreater[] SHIFTDAMP
DiagSft  --@textgreater[] DIAGSHIFT
\end{Verbatim}

I also had to guess about \textbf{BINCANCEL}.  It seems to be a \textbf{FCIMC}
option, but was placed with \textbf{MCSTAR} (and was with all the \textbf{VERTEX STAR}
methods).

This section needs to be extended substantially.
\end{notice}
\begin{description}
\item[\textbf{DIAGSHIFT} {[}DiagSft{]}]
Set the initial value of the diagonal shift.

\item[\textbf{INITWALKERS} {[}nWalkers{]}]\begin{quote}

Default 3000.
\end{quote}

Set the initial population of walkers.

\item[\textbf{NMCYC} {[}NMCYC{]}]
Set the total number of timesteps to take.

\item[\textbf{SHIFTDAMP}  {[}SftDamp{]}]
Damping factor (OF WHAT?!).  \textless{}1 means more damping.

\item[\textbf{STEPSSHIFT} {[}StepsSft{]}]
Default 100.

Set the number of steps taken before the diagonal shift is updated.

\item[\textbf{TAU} {[}TIMESTEP{]}]
Default 0.0.

Set the number of evoution timesteps before
the EShift is updated.  Must be set such that
$\operatorname{max}(H_{jj},|H_{ij}|))*\tau<1$ .

\begin{notice}{note}{Note:}
Brilliantly, there's a comment in the code which states (contradictorily):

\begin{Verbatim}[commandchars=@\[\]]
For FCIMC, this can be considered the timestep of the simulation.
If not set, it will default to the value which will allow the
fastest destruction of walkers.
\end{Verbatim}
\end{notice}

\end{description}

The following options are only available in \textbf{FCIMC} calculations:
\begin{description}
\item[\textbf{READPOPS}]
Read the initial walker configuration from the file POPSFILE.
\textbf{DIAGSHIFT} and \textbf{INITWALKERS} given in the input will be
overwritten with the values read in form POPSFILE.

\item[\textbf{SCALEWALKERS} {[}fScaleWalkers{]}]
Scale the number of walkers by fScaleWalkers, after having read in data from POPSFILE.

\item[\textbf{STARTMP1}]
Set the initial configuration of walkers to be proportional to the MP1 wavefunction.

\item[\textbf{GROWMAXFACTOR} {[}GrowMaxFactor{]}]
Default 9000.

Set the factor by which the initial number of particles are allowed to grow before
they are culled.

\item[\textbf{CULLFACTOR} {[}CullFactor{]}]
Default 5.

Set the factor by which the total number of particles is reduced once it reaches the GrowMaxFactor limit

\item[\textbf{NOBIRTH}]
Force the off-diagonal $rho$ (?) matrix elements to become zero,
and hence obtain an exponential decay of the initial populations
on the determinants, at a rate which can be exactly calculated and
compared against.

\item[\textbf{MCDIFFUSE} {[}Lambda{]}]
Default 0.0.

Set the amount of diffusion compared to spawning in the \textbf{FCIMC} algorithm.

\item[\textbf{FLIPTAU} {[}FlipTauCyc{]}]
Default: off.

Cause time to be reversed every FlipTauCyc cycles in the \textbf{FCIMC}
algorithm. This might help with undersampling problems.

\item[\textbf{NON-PARTCONSDIFF}]
Use a seperate partitioning of the diffusion matrices, in which
the antidiffusion matrix (+ve connections) create a net increase of
two particles.

\item[\textbf{FULLUNBIASDIFF}]
Fully unbias for the diffusion process by summing over all connections.

\item[\textbf{NODALCUTOFF} {[}NodalCuttoff{]}]
Constrain a determinant to be of the same sign as the MP1
wavefunction at that determinant, if the normalised component of
the MP1 wavefunction is greater than the NodalCutoff value.

\item[\textbf{NOANNIHIL}]
Remove the annihilation of particles on the same
determinant step.

\end{description}

The following option are only available in \textbf{MCSTAR} calculations:
\begin{description}
\item[\textbf{BINCANCEL}]
This is a seperate method to cancel down to find the residual
walkers from a list, involving binning the walkers into their
determinants. This has to refer to the whole space, and so is
much slower.  See also the \textbf{WAVEVECTORPRINT} and \textbf{POPSFILE}
options in the \textbf{LOGGING} block.

\end{description}


\subsection{Return Path Monte Carlo options}
\begin{description}
\item[\textbf{MAXCHAINLENGTH} {[}CLMAX{]}]
Set the maximum allowed chain length before a particle is forced to
come back to its origin.

\item[\textbf{RETURNBIAS} {[}PRet{]}]
Set the bias at any point to spawn at the parent determinant.

\end{description}


\subsection{Perturbation theory options}
\begin{description}
\item[\textbf{MPTHEORY} {[}\textbf{ONLY}{]}]
In addition to doing a graph theory calculation, calculate the Moller--Plesset
energy to the same order as the maximum vertex level from the
reference determinant (e.g. with 2-vertex sum the MP2 energy is
obtained, with 3-vertex the MP3 energy etc.  Note that the MP2 energy
can be obtained in conjunction with a \textbf{VERTEX STAR} calculation.
\begin{description}
\item[\textbf{ONLY}]
Run only a MP2 calculation.  This is only available when
compiled in parallel.  The only relevant \textbf{CALC} options are the
\textbf{EXCITATIONS} options: all other \textbf{CALC} keywords are ignored
or over-ridden.  No \textbf{LOGGING} options are currently applicable.

Whilst in principle integrals are only used once, this optimal
algorithm is not currently implemented.  The speed of a \textbf{CPMD}-based
calculation thus benefits from having a \textbf{UMatCache} of non-zero size.

\begin{notice}{warning}{Warning:}
It is currently assumed that the calculation is restricted.
\end{notice}

\end{description}

\begin{notice}{note}{Note:}
INPUT\_DOC has this to say:

\begin{Verbatim}[commandchars=@\[\]]
Instead of a normal path-integral expansion **MC**, do a
Moller--Plesset.  Requires **HDIAG**, and **BIAS**=0.D0.  Can be
used without a **METHODS** section.  If a **METHODS** section is
needed to specify different numbers of cycles at each level, then
**MCDIRECTSUM** must also be set.
\end{Verbatim}

I am sure this is out of date...
\end{notice}

\item[\textbf{EPSTEIN-NESBET}]
Apply Epstein--Nesbet perturbation theory, rather than
Moller--Plesset.  Only works for \textbf{VERTEX SUM NEW} and \textbf{VERTEX
SUM HDIAG} and only at the 2-vertex level.

\item[\textbf{LADDER}]
Use ladder diagram perturbation theory, rather than Moller--Plesset.
The energy denominator is $E_0-E_I+|H_{0I}|^2$.  Only works
for \textbf{VERTEX SUM NEW} and \textbf{VERTEX SUM HDIAG} and only at the
2-vertex level.

\item[\textbf{MPMODTHEORY}]
Perform a hybrid of Epstein--Nesbet and Moller--Plesset theory,
which includes only the $\bra ij||ij ket +\bra ab||ab ket$
terms in the denominator.  Only works for \textbf{VERTEX SUM NEW} and
\textbf{VERTEX SUM HDIAG} and only at the 2-vertex level.

\end{description}


\subsection{Diagonalisation options}

Options for performing a full diagonalisation in the space of the full
basis of spin orbitals.

\begin{notice}{warning}{Warning:}
This quickly becomes prohibitively expensive as system size increases.
\end{notice}
\begin{description}
\item[\textbf{ACCURACY} {[}B2L{]}]
Desired level of accuracy for Lanczos routine.

\item[\textbf{BLOCK} {[}\textbf{ON} \textbf{OFF}{]}]
Default off.

Determines whether the Hamiltonian is calculated for each block
or not.  This only works for \textbf{COMPLETE}.

\item[\textbf{BLOCKS} {[}NBLK{]}]
Set number of blocks used in Lanczos diagonalisation.

\item[\textbf{COMPLETE}]
Perform a full diagonalisation working out all eigenvectors
and eigenvalues.  if \textbf{HAMILTONIAN} is \textbf{OFF}, discard the
eigenvectors and eigenvalues after having used them for calculation.
Relevant options are \textbf{HAMILTONIAN} and \textbf{BLOCK}.

\end{description}

\begin{notice}{note}{Note:}
When would it be advantageous to save the eigenvalues and -vectors
are a diagonalisation?
\end{notice}
\begin{description}
\item[\textbf{EIGENVALUES} {[}NEVAL{]}]
Required number of eigenvalues.

\item[\textbf{ENERGY}]
Calculate the energy by diagonalising the Hamiltonian matrix.
Requires one of \textbf{COMPLETE}, \textbf{LANCZOS}, or \textbf{READ} to be set.

Exact E(Beta) is printed out as:
\begin{align}\begin{split}\text{E(Beta)} = \frac{ \sum_{\alpha} E_{\alpha} e^{-\beta E_{\alpha}} } { \sum_{\alpha} e^{-\beta E_{\alpha}} }\end{split}\end{align}
The result will, of course, change depending upon the symmetry subspace
chosen for diagonalization for finite temperatures.

The diagonalization procedure creates a list of determinants, which
is printed out to the DETS file.

The weight, $w_{\veci}$ and weighted energy, $w_{\veci}
\tilde{E}_{\veci}$ are also calculated for all NPATH determinants.

\begin{notice}{note}{Note:}
\textbf{ENERGY} was documented twice in the INPUT\_DOC file.  This is not
particularly helful...

I have (hopefully) combined them correctly.
\end{notice}

\item[\textbf{KRYLOV} {[}NKRY{]}]
Set number of Krylov vectors.

\item[\textbf{LANCZOS}]
Perform a Lanczos block diagonalisation on the Hamiltonian matrix.

Relevant parameters are \textbf{BLOCKS}, \textbf{KRYLOV}, \textbf{ACCURACY},
\textbf{STEPS} and \textbf{EIGENVALUES}.

\item[\textbf{READ}]
Read in eigenvectors and eigenvalues of the Hamiltonian matrix from a previous calculation.

\item[\textbf{STEPS} {[}NCYCLE{]}]
Set the number of steps used in the Lanzcos diagonalisation.

\end{description}


\subsection{Graph morphing options}

A new approach developed by George Booth.  Take an initial starting graph
and over many iterations allow the determinants contained within the
graph to change, so that the resultant graph is a better approximation
to the true ground state.
\begin{description}
\item[\textbf{GRAPHBIAS} {[}GraphBias{]}]
If at each iteration the graph is being completely renewed, then this
bias specifies the probability that an excitation of the previous
graph is selected to try and be attached, rather than one of the
determinants in the previous graph.

\item[\textbf{GRAPHSIZE} {[}NDets{]}]
Specify the number of determinants in the graph to morph.

\item[\textbf{GROWGRAPHSEXPO} {[}GrowGraphsExpo{]}]
Default is 2.D0.

The exponent to which the components of the excitations vector
and the eigenvector are raised in order to turn them into
probabilities. The higher the value, the more that larger weighted
determinants will be favoured, though this might result in the graph
growing algorithm getting stuck in a region of the space.

\item[\textbf{GROWINITGRAPH}]
Grow the initial graph non-stochastically from the excitations of
consecutive determinants.

\item[\textbf{INITSTAR}]
Set up the completely connected two-vertex star graph, and use as
the starting point for the morphing.

Automatically changes the NDets parameter to reflect the number of
double excitations in the system.

\item[\textbf{ITERATIONS} {[}Iters{]}]
The number of graph morphing iterations to perform.

\item[\textbf{MAXEXCIT} {[}iMaxExcitLevel{]}]
Limit the size of the excitation space by only allowing excitations
out to iMaxExcitLevel away from HF reference determinant.

\item[\textbf{MCEXCITSPACE} {[}NoMCExcits{]}]
Stochastically sample the space of excitations from each determinant in the
graph with NoMCExcits determinants chosen per determinant.

\item[\textbf{MOVEDETS} {[}NoMoveDets{]}]
Grow the graphs using an alternative Monte Carlo, where a number
of determiants are deleted from the previous graph and reattached
elsewhere in the graph in a stochastic manner, according to the
probabilities given by the application of the $rho$ propagator
to the eigenvector of the previous graph.

\item[\textbf{NOSAMEEXCIT}]
Ignore the connections between determinants which are of the
same excitation level in comparison to the reference determinant.
Currently only available in conjunction with \textbf{INITSTAR}, so the
starting graph is simply the doubles star graph (with no cross
connections).

\item[\textbf{ONEEXCITCONN}]
Grow the graph by attaching only determinants which differ by one
excitation level to the connecting vertex in the previous graph.
Currently not implemented with MoveDets.

\item[\textbf{SINGLESEXCITSPACE}]
Restrict the space into which the current graph is allowed to morph
to just single excitations of the determinants in the current graph.
This should reduce the scaling of the algorithm.

\end{description}


\subsection{Monte Carlo options}

Options for performing a Monte Carlo calculation on a vertex sum (as
specified in the \textbf{METHODS} section).

The Monte Carlo routines have only ever been tested for molucular and
model systems and probably are not currently functional for \textbf{CPMD}
or \textbf{VASP} based calculations.

See the reports by Ramin Ghorashi (\cite{RGPtIII}) and George Booth
(\cite{GHBCPGS}).
\begin{description}
\item[\textbf{CALCVAR}]
Only available for performing full vertex sums using the \textbf{HDIAG}
formulation to evaluate the thermal density matrix elements.

Calculate a theoretical approximation to the expected variance if a
non-stochastic MC run were to be performed, with the parameters given,
at the chosen vertex level.  Currently the expected variance is sent
to STOUT as a full variance for the total energy ratio.  Causes the
calculation to take longer since the generation probabilities of
the graphs must all be calculated.  The sum over graphs of the
generation probabilities is also printed out for each vertex
level. This should equal 1, since we are working with normalised
probabilities.

\item[\textbf{POSITION} {[}IOBS JOBS KOBS{]}]
Sets the position of the reference particle.

\item[\textbf{CIMC}]
Perform a configuration interation space Monte Carlo.

\item[\textbf{BETAEQ} {[}BETAEQ{]}]
Default is set to be $\beta$, as set above.

Set $\beta$ to have a different value for the equilibriation steps.

\begin{notice}{note}{Note:}
What are the equilibriation steps?
\end{notice}

\item[\textbf{BIAS} {[}G\_VMC\_FAC{]}]
Default 16.

Vertex level bias for \textbf{FULL} \textbf{MC}. Positive values bias toward
larger graphs, negative values towards smaller graphs.

For \textbf{SINGLE} and \textbf{MULTI} level MC (using a composite 1-vertex
graph containing a full sum previously performed), this is the
probability of generating a graph which is not the composite graph.
The default is invalid, and this must be set manaully.  Stochastic
time MC is used.  If BIAS is negative, then | BIAS | is used, but
stochastic-time MC is not performed.

\begin{notice}{note}{Note:}
BIAS seems to do two very different things if it is set to a negative value.
Please clarify.
\end{notice}

\item[\textbf{DETSYM} {[}MDK(I), I=1,4{]}]
The symmetry of the \textbf{CIMC} determinant.

\begin{notice}{note}{Note:}
Specify the symmetry how?
\end{notice}

\begin{notice}{note}{Note:}
If any if the \textbf{CIMC} options are set without \textbf{CIMC} being
specified, the code will return an error and exit.**
\end{notice}

\item[\textbf{EQSTEPS} {[}IEQSTEPS{]}]
The number of equilibriation sets for the CI space Monte Carlo routine.

\item[\textbf{GRAPHEPSILON} {[}GRAPHEPSILON{]}]
Default 0.0.

The minimum significant value of the weight of a graph.

Ignore the contributions to the weight and $\tilde{E}$ of all
graphs with a weight that is smaller in magnitude than GRAPHEPSILON.

\item[\textbf{IMPORTANCE} {[}G\_VMC\_PI{]}]
Ddefault 0.95.

Set the generation probability for the MC routine.  This is the
probability that new determinants are excitations of the pivot, i.

\item[\textbf{MCDIRECTSUM}]
Perform Monte Carlo on graphs summing in energies weighted with the
weight/generation probability of the graph.

\item[\textbf{PGENEPSILON} {[}PGENEPSILON{]}]
Default 0.0.

Set the minimum significant value of the generation probability of a graph.

Because for larger graphs, the calculation of the generation
probability is subject to numerical truncation errors, generation
probabilities which are lower than a certain value are unreliable,
and can cause the Monte Carlo algorithm to get stuck: if a graph had a
very small generation probability, it would be difficult for a Monte
Carlo run to accept a move to a different graph.  If the magnitude
of the generation probability of a graph is smaller than PGENEPSILON,
then a new graph is generated.

Setting this too high could cause problems in the graph generation phase,
so NECI will exit with an error if it generates 10000 successive
graphs each with generation probabilities below PGENEPSILON.

\item[\textbf{SEED} {[}G\_VMC\_SEED{]}]
Default -7.

Set the random seed required for the Monte Carlo calculations.

\item[\textbf{STEPS} {[}IMCSTEPS{]}]
Set the number of steps for the CI space Monte Carlo routine.

\item[\textbf{VVDISALLOW}]
Disallow V-vertex to V'-vertex transitions in stochastic time Monte
Carlo: i.e. allow only transitions to graphs of the same size.

\end{description}


\subsubsection{Weighting schemes}

By default the vertex sum Monte Carlo algorithm selects excitations
with no bias.  The variance of a Monte Carlo calculation can be reduced
by preferentially selecting for certin types of excitation.
\begin{description}
\item[\textbf{EXCITWEIGHTING} {[}g\_VMC\_ExcitFromWeight g\_VMC\_ExcitToWeight G\_VMC\_EXCITWEIGHT{]} {[}g\_VMC\_ExcitToWeight2{]}]
Default 0.d0 (unweighted) for all values.

A weighting factor for the generation of random excitations in the
vertex sum Monte Carlo.  A parameter set to zero has a corresponding
weighting factor of 1.

For generating an excitation from occupied spin orbitals i and j to
unoccupied spin orbitals k and l:
\begin{itemize}
\item {} \begin{description}
\item[the probability of choosing pair (ij) is proportional to]\begin{align}\begin{split}e^{(E_i+E_j) \text{g\_VMC\_ExcitFromWeight} }.\end{split}\end{align}
\end{description}

\item {} \begin{description}
\item[the probability of choosing pair (kl) is proportional to]\begin{align}\begin{split}e^{-(E_k+E_l) \text{g\_VMC\_ExcitToWeight}} e^{|\bra ij|U|kl\ket|*\text{G\_VMC\_EXCITWEIGHT}} |E_i+E_j-E_k-E_l|^{\text{g\_VMC\_ExcitToWeight2}}.\end{split}\end{align}
\end{description}

\end{itemize}

\item[\textbf{POLYEXCITWEIGHT} {[}g\_VMC\_ExcitFromWeight g\_VMC\_PolyExcitToWeight1 g\_VMC\_PolyExcitToWeight2 G\_VMC\_EXCITWEIGHT{]}]
Default 0.0 for all values (i.e. unweighted: all weighting factors
are set to 1).

Weighting system for the choice of virtual orbitals in
the excitations.

The probability of choosing the pair of spin orbitals, kl, to excite
to is set to be constant for $E_k+E_l$ is less than
g\_VMC\_PolyExcitToWeight1.  For higher energy virtual orbitals,
the weighting applied is a decaying polynomial which goes as:
\begin{quote}
\begin{align}\begin{split}(E_k+E_l-\text{g\_VMC\_PolyExcitToWeight1}+1)^{-\text{g\_VMC\_PolyExcitToWeight2}}\end{split}\end{align}\end{quote}

g\_VMC\_PolyExcitToWeight1 is constrained to be not more than the
energy of the highest virtual orbital.

\item[\textbf{POLYEXCITBOTH} {[}g\_VMC\_PolyExcitFromWeight1 g\_VMC\_PolyExcitFromWeight2 g\_VMC\_PolyExcitToWeight1 g\_VMC\_PolyExcitToWeight2 G\_VMC\_EXCITWEIGHT{]}]
Identical to \textbf{POLYEXCITWEIGHT}, except that the polynomial weighting
function applies also to the occupied orbitals.  This means that there
is another variable, since now the `ExcitFrom' calculation also needs
a value for sigma, and for the exponent.  The sigma variables are
now both under similar constraints as specified above, which means
that they cannot be larger or smaller than the highest and lowest
energy orbital respectivly.  This prevents the PRECALC block from
getting stuck, or from finding local variance minima.

\begin{notice}{note}{Note:}
What is sigma?
\end{notice}

\item[\textbf{CHEMPOTWEIGHTING} {[}g\_VMC\_PolyExcitFromWeight2 g\_VMC\_PolyExcitToWeight2 G\_VMC\_EXCITWEIGHT{]}]
Weighting is of the same form as POLYEXCITBOTH, but sigma is
now constrained to be at the chemical potential of the molecule.
Has only two parameters with which to minimise the expected variance.

\item[\textbf{CHEMPOT-TWOFROM} {[}g\_VMC\_ExcitWeights(1) g\_VMC\_ExcitWeights(2) g\_VMC\_ExcitWeights(3) G\_VMC\_EXCITWEIGHT{]}]\begin{quote}

When choosing the electron to excite, use a a increasing polynomial
up to the chemical potential and a decaying polynomial for spin orbitals
above the chemical potential, in order to encourage mixing of
the configurations around the HF state. Contains three adjustable
parameters and testing needs to be done to see if this is
beneficial. Expected to make more of a difference as the vertex
level increases.
\end{quote}

\begin{notice}{note}{Note:}
What is the actual weighting form of \textbf{CHEMPOT-TWOFROM}?
\end{notice}

\item[\textbf{UFORM-POWER}]
New power form for the U-matrix element weighting using the
appropriate \textbf{EXCITWEIGHT} element, which is believed to be
better. This uses the form $W=1+|U|^{\text{EXCITWEIGHT}}$, rather than the
exponential form.

\end{description}


\subsection{Experimental options}

\begin{notice}{note}{Note:}
More documentation on these options needed.
\end{notice}
\begin{description}
\item[\textbf{EXCITATIONS} \textbf{FORCEROOT}]
Force all excitations in \textbf{VERTEX} {[}\textbf{SUM} \textbf{STAR}{]} \textbf{NEW}
calculations to come from the root.

\item[\textbf{EXCITATIONS} \textbf{FORCETREE}]
Disallow any excitations in a \textbf{VERTEX} \textbf{SUM} \textbf{NEW} which are
connected to another in the graph, forcing a tree to be produced.
Not all trees are produced however.

\item[\textbf{FULLDIAGTRIPS}]
An option when creating a star of triples, to do a
full diagonalisation of the triples stars, without any
prediagonalisation. Very very slow...

\item[\textbf{LINEPOINTSSTAR} {[}LinePoints{]}]
Set the number of excited stars whose eigenvalues are evaluated when
using StarStars, in order to determine linear scaling.

\item[\textbf{NOTRIPLES}]
Disallow triple-excitations of the root determinant as the 3rd vertex
in \textbf{HDIAG} calculations at the third vertex level and higher.

\end{description}

\resetcurrentobjects


\hypertarget{input-integrals}{}\section{Integrals}

\begin{notice}{note}{Note:}
INSPECT and ORDER options currently make no sense.
\end{notice}
\begin{description}
\item[\textbf{INTEGRAL}]
Starts the integral block.

\end{description}

{[}Integral options---see below.{]}
\begin{description}
\item[\textbf{ENDINT}]
End of integral block.

\end{description}


\subsection{General options}
\begin{description}
\item[\textbf{FREEZE} {[}NFROZEN NTFROZEN{]}]
Set the number of frozen core states and frozen excited states
respectively.  Both must be a multiple of two - an error is returned
if this is not the case.  The Slater determinant space of a
calculation does not include determinants which contain excitations from
frozen core states or excitations to frozen excited states.

\item[\textbf{INSPECT} {[}SPECDET(I), I=1,NEL-NFROZEN{]}]
Investigate the specified determinant.

\item[\textbf{ORDER} {[}ORBORDER(I), I=1,8){]}]
Set the prelimanary ordering of basis functions for an initial guess
at the reference determinant.  There are two ways of specifying
open orbitals:
\begin{enumerate}
\item {} 
If orborder2(I,1) is integral, then if it's odd, we have a single.

\item {} 
Non-integral.  The integral part is the number of closed oribtals,
and the fractional*1000 is the number of open orbitals.
e.g. 6.002 would mean 6 closed and 2 open
which would have orborder(I,1)=6, orborder(I,2)=4
but say 5.002 would be meaningless as the integral part must be a
multiple of 2

\end{enumerate}

\end{description}


\subsection{Density fitting options}
\begin{description}
\item[\textbf{DFMETHOD} {[}method{]}]
control the Density fitting method.
Possible methods are:
\begin{description}
\item[\textbf{DFOVERLAP}]
(ij|u|ab)= (ij|u|P)(P|ab)

\item[\textbf{DFOVERLAP2NDORD}]
(ij|u|ab)= (ij|u|P)(P|ab)+(ij|P)(P|u|ab)-(ij|P)(P|u|Q)(Q|ab)

\item[\textbf{DFOVERLAP2}]
(ij|u|ab)= (ij|P)(P|u|Q)(Q|ab)

\item[\textbf{DFCOULOMB}]
(ij|u|ab)= (ij|u|P){[}(P|u|Q)\textasciicircum{}-1{]}(Q|u|ab)

\end{description}

where the sums over P and Q are implied.

All methods are precontracted to run in order(nBasis) except
DFOVERLAP2NDORD.

\item[\textbf{DMATEPSILON} DMatEpsilon (default 0)]
The threshold for density matrix elements, below which small density
matrix elements are ignored, and conequently speeds up calculations.

\end{description}


\subsection{Hartree--Fock options}

The Hartree--Fock options have only been tested for molecular and model systems.
They allow the Hartree-Fock orbitals (in the space of the original basis) to be used
in a graph calculation instead of the original basis.

\begin{notice}{note}{Note:}
James has never used these options.  Please can those who have document them in more detail.
\end{notice}
\begin{description}
\item[\textbf{HF}]
Use a Hartree--Fock basis.

\item[\textbf{CALCULATE}]
Calculate the Hartree--Fock basis rather than reading it in.  By default,
the Hartree--Fock calculation is performed before any freezing of orbitals,
i.e. in the full original basis.

\item[\textbf{HFMETHOD} {[}HFMETTHOD{]}]
Default: \textbf{SINGLES}.

Specify the method for the Hartree-Fock routine.  Options are:
\begin{description}
\item[\textbf{STANDARD}]
Use normal Hartree--Fock process.

\item[\textbf{DESCENT} {[}\textbf{SINGLES}, \textbf{OTHER}{]}]
Use singles or other gradient descent.

\item[\textbf{MODIFIED}]
Modify virtuals.  Experimental.

\end{description}

\item[\textbf{MAXITERATIONS} {[}NHFIT{]}]
Set the maximum number of Hartree--Fock iterations.

\item[\textbf{MIX} {[}HFMIX{]}]
Set the mixing parameter for each Hartree--Fock iteration.

\item[\textbf{POSTFREEZEHF}]
Do Hartree--Fock after freezing instead of before (still needs \textbf{HF}
and \textbf{CALCULATE}).  The Hartree--Fock calculation is performed only
in the space of the unfrozen orbitals.

\item[\textbf{RAND} {[}HFRAND{]}]
Default 0.01.

Set the maximum magnitude of the random numbers added to the starting density matrix.
Use to perturb away from an initially converged Hartree--Fock solution.

\item[\textbf{READ} {[}\textbf{MATRIX} \textbf{BASIS}{]}]
Read in U matrix and/or Hartree--Fock basis in terms of the original basis.

\item[\textbf{RHF}]
Use restricted Hartree-Fock theory.

\item[\textbf{THRESHOLD} {[} \textbf{ENERGY} {[}HFEDELTA{]} \textbf{ORBITAL} {[}HFCDELTA{]} {]}]
Set the convergence threshold for the energy and/or the orbitals.

\item[\textbf{UHF}]
Use unrestricted Hartree-Fock theory.

\end{description}


\subsection{Partioning options}

If the weight and energy contribution from a graph are evaulated from
diagonalising the $\rho$ matrices, then various schemes are
available to deal with the $e^{-\beta\hat{H}/P}$ operator.

\begin{notice}{note}{Note:}
More detail on these needed.
\end{notice}
\begin{description}
\item[\textbf{FOCK-PARTITION}]
For calculation of $\rho$ operator with the Trotter
approximation, partition the Hamiltonian according to the N-electron
Fock operator and Coulomb perturbation.

\item[\textbf{FOCK-PARTITION-LOWDIAG}]
For calculation of $\rho$ operator with Trotter approximation,
partition the Hamiltonian according to the N-electron Fock operator
and coulomb perturbation.  Take just the first order approximation
(i.e. ignore the $\beta/P$ term) for the diagonal terms of the
$\rho$ matrix.

\item[\textbf{FOCK-PARTITION-DCCORRECT-LOWDIAG}]
For calculation of $\rho$ operator with Trotter approximation,
partition the Hamiltonian according to the N-electron Fock operator
and Coulomb perturbation.  Remove the Coulomb double counting in the
Fock operator.Take just the first order approximation (i.e. ignore
the $\beta/P$ term) for the diagonal terms of the $\rho$
matrix.

\item[\textbf{DIAG-PARTITION}]
Default partitioning scheme.

For calculation of $\rho$ operator with Trotter approximation,
partition the Hamiltonian as the diagonal and non-diagonal matrix
elements between the determinants.

\item[\textbf{RHO-1STORDER}]
Calculate rho elements to only 1st order Taylor expansion (without
applying a Trotter approximation).

\end{description}


\subsection{VASP and CPMD options}

There are too many 2-electron integrals to store for periodic systems
(\textbf{CPMD} or \textbf{VASP} based calculations).  Instead, as many integrals as
possible are cached.  Each four-index integral is reduced to two indices,
A and B.  Each A index has so many slots associated with it in which
the integral involving A and B can be stored.  The cache stores
as many integrals as possible.  If the cache is full and a new integral
is calculated, then an element in the cache is over-written.

The efficiency of a calculation is heavily dependent on the size of the
integral cache.
\begin{description}
\item[\textbf{UMATCACHE} {[}\textbf{SLOTS}{]} {[}nSlots{]}]
Default nSlots=1024.

Set the number of slots for each A index.

The total amount of memory used by the cache will be in the order of
NSLOTS*NSTATES*(NSTATES-1)/2  words.

If nSlots=0, then disable caching of integrals calculated on the fly,
but retain precomputation of 2-index 2-electron integrals ($\bra
ij | ij \ket$ and $\bra ij | ji \ket$.

If nSlots=-1, no 2-electron integrals are stored.

Disabling the cache is very expensive.

The keyword \textbf{SLOTS} is optional and is present to contrast with
the \textbf{MB} keyword.

\item[\textbf{UMATCACHE} \textbf{MB} {[}MB{]}]
Number of megabytes to allocate to the UMAT cache.  The number of
slots is then set accordingly.

\item[\textbf{NOUMATCACHE}]
Disable all UMAT caching (idential to \textbf{UMATCACHE} -1).

\end{description}


\subsection{Experimental options}

\begin{notice}{note}{Note:}
Please document in more detail!
\end{notice}
\begin{description}
\item[\textbf{NRCONV} {[}NRCONV{]}]
Default $10^{-13}$.

This sets the convergence criteria for the Newton-Raphson algorithm
in findroot. This takes place after initial bounds for the root are
calculated using regular falsi (see above). Values smaller than
$10^{-15}$ tend to create a fault since the Newton-Raphson
algorithm cannot converge given the number of iterations allowed.

\item[\textbf{NRSTEPSMAX} {[}NRSTEPSMAX{]}]
This sets the maximum number of Newton Raphson steps allowed to try
and converge upon a root to the accuracy given in \textbf{NRCONV}. This
is only applicable for the star graph, when trying to find
the roots of the polynomial using \textbf{POLY} \textbf{NEW}, \textbf{POLY} \textbf{OLD} or
\textbf{POLYCONVERGE}. Default value is 50.

\item[\textbf{RFCONV} {[}RFCONV{]}]
Default $10^{-8}$.

Set the convergence criteria for the Regular falsi algorithm in
findroot. Only used with a star calculation which involves calculating
the roots of a polynomial to find the eigenvalues. A Newton-Raphson
convergence takes place after.

\item[\textbf{INCLUDEQUADRHO}]
This changes the rho matrix for stars so that it includes the square
of the eigenvalues - rho -\textgreater{} rho + rho\textasciicircum{}2/2. This is in an attempt to
improve size consistency for the star graph. No change for large beta,
and only very small changes for smaller betas.

\item[\textbf{EXPRHO}]
The rho matrix is exponentiated, 1 is subtracted, and this is used as
the matrix to be diagonalised. This is the full expansion for which
\textbf{INCLUDEQUADRHO} is a truncation. Again, this is used to achieve
size consistency with the star, although seems to have little effect,
and no effect at high beta.

\item[\textbf{DISCONNECTNODES}]
If using a nodal approximation, the connections between determinants
in the same nodes are ignored - should then be equivalent to the
original star calculation.

\item[\textbf{CALCEXCITSTAR}]
Used with \textbf{STARSTARS}, it explicitly calculates each excited star
and diagonalises them seperatly. This removes the approximation of
cancelling ficticious excitations if the original star is used as
a template for higher excitations. Scaling is bad, and all matrix
elements have to be calculated exactly.

\item[\textbf{STARNODOUBS}]
Only to be used with \textbf{CALCEXCITSTAR} when explicitly calculating
excited stars, it forbids the excited stars to have excitations
which are double excitations of the Hartree--Fock determinant.

\item[\textbf{STARQUADEXCITS}]
Only to be used with \textbf{CALCEXCITSTAR}, when calculating the excited
stars, it only allow the excited stars to have excitations which
are quadruple excitations of the Hartree--Fock determinant.

\item[\textbf{QUADVECMAX}]
Used with STARSTARS, it uses only the largest first element of the
eigenvectors as the connection to each excited star. This means
that for each excited star, only one connection is made back to the
original star, meaning that the scaling is reduced. This seems to
be a good approximation.

\item[\textbf{QUADVALMAX}]
Same as QUADVECMAX, only the largest eigenvalue for each excited
star is used. Seems to be little difference in results.

\item[\textbf{DIAGSTARSTARS}]
Used with \textbf{STARSTARS}, it performs a full diagonalisation on
each excited star, using the original star as a template, i.e. same
excitations, and same offdiagonal elements. All that occurs is that
the diagonal elements are multiplied by rho\_jj. Large Scaling.

\item[\textbf{EXCITSTARSROOTCHANGE}]
Used with \textbf{DIAGSTARSTARS} only at the moment, when this is set,
only the root element of the excited star matrices changes when
constructing excited stars with roots given by rho\_jj. The remainder
of the excited star matrix is identical to the original star matrix.

\item[\textbf{RMROOTEXCITSTARSROOTCHANGE}]
Another option for use with \textbf{DIAGSTARSTARS}, when this is set, the
same occurs as for \textbf{EXCITSTARSROOTCHANGE}, apart from the fact that
the root is removed as an excited determinant in each excited star.

\end{description}

\resetcurrentobjects


\hypertarget{input-logging}{}\section{Logging}

\begin{notice}{note}{Note:}
All the logging options should say to which file they print output.  Please correct this!
\end{notice}
\begin{description}
\item[\textbf{LOGGING}]
Start the logging input block.  The logging options allow additional
(potentially expensive, potentially verbose) information to be
printed out during a calculation.  By default, all logging options
are turned off.

\end{description}

{[}Logging options---see below.{]}
\begin{description}
\item[\textbf{ENDLOG}]
End the logging input block.

\end{description}


\subsection{General options}
\begin{description}
\item[\textbf{FMCPR} {[}\textbf{LABEL}, \textbf{RHO}, \textbf{1000}, \textbf{EXCITATION}{]}]
More than one of the options can be specified.

Log the following to the PATHS file:
\begin{description}
\item[\textbf{LABEL}]
Logs the determinants contained by each graph as each determinant
is generated in the format:
{[}$(D_0),(D_1),...,D_v),${]}
where each determinant given as a comma-separated list of the
indices of the occupied orbitals:
e.g. $D_0 =$ (    1,    2,    9,   10,).

If CSFs are being used, then the CSF is printed.  There is no newline after this.

For \textbf{MULTI MC} or \textbf{SINGLE MC}, only the non-composite graphs are printed.

\item[\textbf{EXCITATION}]\begin{description}
\item[Log each graph in excitation format instead of full format above.  The format is]
{[}A(    i,    j)-\textgreater{}(    a,    b),B(    k,    l)-\textgreater{}(    c,    d),...,C(    m,    n)-\textgreater{}(    e,    f){]}

\item[where]
A, B,..., C are determinants in the graph from which the excitation is made.
i, j,... are the orbitals within that determinant which are excited from, where (i\textless{}j, k\textless{}l,...).
a, b,... are the orbitals they are excited to, where (a\textless{}b, c\textless{}d, ...).

\end{description}

This format does not in general provide a unique way of
specifying multiply connected graphs, but the first possible
determinant to which the next det in the graph is connected is
chosen, so what is output should be unique.  Single excitations
are written as e.g. A(    i,    0)-\textgreater{}(    a,    0).

\item[\textbf{RHO}]\begin{description}
\item[Log the $\rho$ matrix for each graph in the form:]
($\rho_{11}, \rho_{12}, \cdots, \rho_{1v},| \rho_{21}, \rho_{22}, \cdots, \rho_{2v},| \cdots | \rho_{v1}, \rho_{v2}, \cdots, \rho_{vv},|$),

\end{description}

where the graph consists of $v$  vertices.  A newline is appended.

\item[\textbf{XIJ}]
Log the xij matrix, which contains the generation probabilities
of one determinant in the graph from all the others.  For MC this
is already generated, but for full sums this must be generated,
so will be slower.  Generation probabilities are set with the
\textbf{EXCITWEIGHTING} option.
\begin{description}
\item[The format is:]
\{$x_{11}, x_{12}, ..., x_{1v},| ... | x_{v1}, x_{v2}, ..., x_{vv},|$\}

\end{description}

In general $x_{ij} \ne x_{ji}$.  The $x_{kk}$ element lists
the number of possible excitations from $k$ determinant.
The matrix is followed by a newline.
\begin{description}
\item[After all these possible options, the following are printed:]
Weight {[}pGen{]} ETilde*Weight Class {[}Accepted{]}

\end{description}

pGen is only printed for: Monte Carlo, or if doing a full sum
and the XIJ logging option is set.  Accepted is only printed
for Monte Carlo calculations.  A newline is placed at the end
of this data.  For Monte Calo calculations, the values printed
depend on the options.  If \textbf{LABEL} is set, then all generated
graphs and their values are printed, otherwise only values of
accepted graphs are printed.

\end{description}

\item[\textbf{CALCPATH} {[}\textbf{LABEL} \textbf{RHO}{]}]
Log CALCPATH\_R to PATHS.  Either just label logging or also
log the $\rho$ matrix, with the same format as above.

\item[\textbf{HAMILTONIAN}]
Log HAMIL, PSI and PSI\_LONG.

\item[\textbf{HFBASIS}]
Log HFBASIS.

\item[\textbf{HFLOGLEVEL} {[}LEVEL{]}]
Default 0.

If LEVEL is set to be positive, the density matrices, fock matrices and
eigenvectors during a Hartree--Fock calculation are printed out to SDOUT.

\item[\textbf{MCPATHS}]
Log MCPATHS data to the MCPATHS file for full vertex sum and MCSUMMARY
file when using a METHODS section.  Also log to the RHOPII file.

\item[\textbf{PSI}]
Log PSI\_COMP.

\item[\textbf{TIMING} {[}iGlobalTimerLevel{]} (default 40)]
Timing information is only recorded for routines with level less than
or equal to iGlobalTimerLevel.  Less than 10 means general high level
subroutines. Greater than 50 means very low level.  Routines without
a level are always timed (most of them).

\item[\textbf{XIJ}]
Synonym for \textbf{FMCPR XIJ}.

\end{description}


\subsection{FCIMC options}
\begin{description}
\item[\textbf{POPSFILE}]
Print out the determinants at the end of the MC run. A calculation
can then be restarted at a later date by reading the determinants
back in using \textbf{READPOPS} in the \textbf{CALC} section.
at a later date.

\end{description}


\subsection{GraphMorph options}
\begin{description}
\item[\textbf{DISTRIBS}]
Write out the distribution of the excitations in each graph as it
morphs over the iterations. The first column is the iteration number, and
then subsequent columns denote the number of n-fold excitations in
the graph.

\end{description}


\subsection{PRECALC options}
\begin{description}
\item[\textbf{PREVAR}]
Print the vertex level, Iteration number, parameter, and expected
variance, for each parameter which was searched for in the \textbf{PRECALC}
block, showing the convergence on the optimum value, to the PRECALC
file.

\item[\textbf{SAVEPRECALCLOGGING}]
Allows different logging levels to be used in the \textbf{PRECALC} block
than for the main calculation.

All logging options specified before \textbf{SAVEPRECALCLOGGING} are only
used in the the \textbf{PRECALC} part of the calculation.  All logging
options specified after  \textbf{SAVEPRECALCLOGGING} are only used in the
the main part of the calculation.

\end{description}


\subsection{Monte Carlo options}
\begin{description}
\item[\textbf{BLOCKING}]
Perform a blocking analysis on the MC run.  An MCBLOCKS file will be
produced, which lists log(2){[}blocksize{]}, the average of the blocks,
the error in the blocks(where the blocks are the energy ratio),
and the full error, treating the energy estimator as a correlated
ratio of two quantities.

\item[\textbf{VERTEX} {[}\textbf{EVERY} n{]}]
Log the vertex MC with $\tilde{E}$ every n (real) cycles
and/or log the vertex MC contribution every cycle.  Setting
Delta $=\tilde{E}-\tilde{E}_{\textrm{ref}}$, where
$\tilde{E}_{\textrm{ref}}$ is usually the 1-vertex graph:
\begin{description}
\item[\textbf{EVERY}]
write a VMC file with the following info, with a new line each
time the current graph changes:
\begin{quote}

tot \# virt steps, \# steps in this graph, \#verts, Class, Weight, Delta, \textless{}sign(W)\textgreater{}, \textless{}Delta sign(W)\textgreater{}, \textasciitilde{}standard deviation \textless{}Delta sign\textgreater{}/\textless{}sign\textgreater{},pgen
\end{quote}

\item[n:]
write a VERTEXMC file with the following info:
\begin{quote}

0, \#graphs, \textless{}sign(W)\textgreater{}, stdev(sign(W)), \textless{}Delta\textgreater{}, \textless{}sign Delta\textgreater{}/\textless{}sign\textgreater{}, \textless{}Delta\textasciicircum{}2\textgreater{}, acc ratio, trees ratio, nontree+ ratio, non-tree- ratio, \textless{}Delta sign(W)\textgreater{}, E\textasciitilde{} reference, \#sequences,w reference
\end{quote}

\end{description}

\end{description}

\begin{notice}{note}{Note:}
George, what are most of these values?
\end{notice}
\begin{description}
\item[\textbf{WAVEVECTORPRINT} {[}nWavevectorPrint{]}]
Relevant only for Monte Carlo star calculations.

Calculate the exact eigen-vectors and -values initially, and
print out the running wavevector every nWavevectorPrint Monte Carlo
steps. This is slows the calculation down substantially.

\end{description}

\resetcurrentobjects


\hypertarget{output-index}{}\chapter{Output files}

\resetcurrentobjects


\hypertarget{output-blocks}{}\section{BLOCKS}

BLOCKS contains the list of blocks used if the \textbf{BLOCK} option is used to
calculate the Hamiltonian in the form:

\begin{Verbatim}[commandchars=@\[\]]
BlockIndex   K(1)   K(2)   K(3)   MS SYM   nDets nTotDets
\end{Verbatim}

where:
\begin{quote}

BlockIndex starts from 1.

K(1:3) are momentum values for the \textbf{HUBBARD} and \textbf{UEG} symmetries and
are irrelevant for other systems.

MS is $2S_z$.

Sym is the spatial symmetry index of the determinant.  It can be a single
number or a set of numbers enclosed in parentheses.  Relevant for systems
other than \textbf{HUBBARD} and \textbf{UEG}.

nDets is the number of symmetry unique determinants in the block.

nTotDets is the total number of determinants in the block.
\end{quote}

\resetcurrentobjects


\hypertarget{output-classpaths}{}\section{CLASSPATHS}

CLASSPATHS is calculated when a vertex sum is performed and lists properties of the graphs by their class:

\begin{Verbatim}[commandchars=@\[\]]
Class nGs TotWeight   TotwEt TotWeightPos   TotWeightNeg
\end{Verbatim}

where:
\begin{quote}

Class is a binary string indicating the connectivity of the graph.
\begin{description}
\item[The connectivity matrix has 1 if there's a line in the graph]
e.g. a 3-vertex graph.  The bits are labelled from the bottom right starting from 0:

\begin{Verbatim}[commandchars=@\[\]]
(. 2 1 )  (bits)                   (. 1 1 )
(  . 0 )  -@textgreater[] 210     connections:  (  . 0 ) -@textgreater[] 110 (binary) -@textgreater[] 6 (decimal)
(    . )                           (    . )
\end{Verbatim}

\end{description}

nGs is the number of graphs that fell in this class.
TotWeight is the total weight, $w_i$, of those graphs.
TotEt is the total value of $w_i \tilde{E}_i$ for those graphs.
TotWeightPos is the total weight of graphs(?) with positive weights.
TotWeightNeg is the total weight of graphs(?) with negative weights.
\end{quote}

\resetcurrentobjects


\hypertarget{output-classpaths2}{}\section{CLASSPATHS2}

CLASSPATHS2 is calculated when a vertex sum is performed and gives a histogram of the weights for each graph type.

For each graph type, a header is printed out followed by the histogram data.

Header:

\begin{Verbatim}[commandchars=@\[\]]
Class nGs   TotWeightPos   TotWeightNeg
\end{Verbatim}

Body:

\begin{Verbatim}[commandchars=@\[\]]
log@_10(weight)   nPos   nNeg
\end{Verbatim}

where:
\begin{quote}

Class, nGs, TotWeightPos, TotWeightNeg are the same as in CLASSPATHS.

The body consists of lines from -1 to -15 listing number of +ve and -ve graphs
with a weight in that band.  The top and bottom bands catch any overspills.
\end{quote}

\resetcurrentobjects


\hypertarget{output-dets}{}\section{DETS}

DETS contains a list of determinants when they are enumerated.  This consists of two columns:

\begin{Verbatim}[commandchars=@\[\]]
Determinant   Symmetry
\end{Verbatim}

where:
\begin{quote}

Determinant contains an ordered list of NEL numbers, separated by commas,
surrounded by parentheses:  e.g. (    1,    2,   13,   14,).

Symmetry varies with the system type and is the internal symmetry label of the
determinant.  It is either an integer or a propogation vector which corresponds
to an irreducible representation of an Abelian group.
\end{quote}

\resetcurrentobjects


\hypertarget{output-energies}{}\section{ENERGIES}

ENERGIES contains the list of eigenvalues in the order they are generated.  If the
Hamiltonian is \textbf{BLOCK} ed, then these will not all be in ascending order, but
rather ascending order within each block.

\resetcurrentobjects


\hypertarget{output-hamil}{}\section{HAMIL}

HAMIL contains the non-zero elements of the Hamiltonian in three columns:

\begin{Verbatim}[commandchars=@\[\]]
i  j  Hij
\end{Verbatim}

where:
\begin{quote}

Hij $=\bra D_{\veci} | H | D_{\vecj} \ket$.

i,j are indices for the $\veci,\vecj$ determinants, with i:math:\emph{le}
j, and increasing i.

$D_i$ corresponds to the i-th determinant as given in DETS.
\end{quote}

\resetcurrentobjects


\hypertarget{output-mcpaths}{}\section{MCPATHS and MCSUMMARY}

The MCPATHS logging file (or MCSUMMARY if a \textbf{METHODS} block is used) has the following layout:

\begin{Verbatim}[commandchars=@\[\]]
@textless[]Header Line@textgreater[]
@textless[]Vertex Sum Section for Det 1@textgreater[]
@textless[]Vertex Sum Section for Det 2@textgreater[]
...
@textless[]MC Summary information@textgreater[]
\end{Verbatim}

The Header Line is:

\begin{Verbatim}[commandchars=@\[\]]
\"Calculating  XXX W@_Is...\"
\end{Verbatim}

where XXX is the number of determints calculated, and the number of Vertex Sum sections.

Each Vertex Sum Section consists of:

\begin{Verbatim}[commandchars=@\[\]]
(Determinant Calculated)
@textless[]method level 1 line@textgreater[]
@textless[]method level 2 line@textgreater[]
...
\end{Verbatim}

The Method level line is:

\begin{Verbatim}[commandchars=@\[\]]
@textless[]level@textgreater[]  @textless[]weight@textgreater[]  @textless[]cumlweight@textgreater[]  @textless[]timing@textgreater[] @textless[]GraphsSummed@textgreater[]  @lb[]@textless[]PartGraphs@textgreater[]@rb[] @textless[]w E@textasciitilde[]@textgreater[] @lb[]@textless[]MP2 contrib@textgreater[] @lb[]@textless[]MP3 contrib@textgreater[] @lb[]..@rb[]@rb[]@rb[]@rb[]
\end{Verbatim}
\begin{description}
\item[where:]\begin{description}
\item[level]
The vertex level as specified by the METHODS section.

\item[weight]
The contribution of this level to s\_i (see RHOPII or RHOPIIex file section).  This can be further analysed in CLASSPATHS\{,2\}.

\item[cumlweight]
The sum of weights up to and including this level.

\item[timing]
(double) The number of seconds calculating this level took.

\item[GraphsSummed]
The total number of graphs summed together at this level.

\item[PartGraphs]
Recursively, how many times FMCPR?  is called.  It is called once as each node is added to a graph.

\item[$w \tilde{E}$]
The contribution of this level to $w \tilde{E}$.  This can be further analysed in CLASSPATHS\{,2\}

\item[MPn contrib]
The contribution of graphs at this level to MPn theory.

\end{description}

\end{description}

There are is one method level line for each method level specified in the \textbf{METHODS} section, plus one  for the 1-vertex graph.

The MC Summary information is split into 4 parts:
\begin{quote}

TotalStats:

\begin{Verbatim}[commandchars=@\[\]]
GRAPHS(V)...WGHT-(V)
\end{Verbatim}

GenStats:

\begin{Verbatim}[commandchars=@\[\]]
GEN-@textgreater[] *
\end{Verbatim}

AccStats:

\begin{Verbatim}[commandchars=@\[\]]
ACC-@textgreater[] *
\end{Verbatim}

Sequences:

\begin{Verbatim}[commandchars=@\[\]]
Sequences, Seq Len
\end{Verbatim}
\end{quote}
\begin{description}
\item[TotalStats:]
The output is split into columns depending on the levels sampled in the Monte Carlo:

DataType    Total    1-vertex    2-vertex    3-vertex    ...
\begin{description}
\item[The DataTypes are:]\begin{description}
\item[GRAPHS(V)]
The number of graphs sampled with this number of vertices.

\item[TREES(V)]
The number of trees sampled with this number of vertices.  Trees contain no cycles.

\item[NON-TR+(V)]
The number of non-trees sampled with this number of vertices whose weight is positive.

\item[NON-TR-(V)]
The number of non-trees sampled with this number of vertices whose weight is negative.

\item[WGHTT(V)]
The total weight of trees with this number of vertices.

\item[WGHT+(V)]
The total weight of positive non-trees with this number of vertices.

\item[WGHT-(V)]
The total weight of negative non-trees with this number of vertices.

\end{description}

\end{description}

\item[GenStats:]
Statistics on the number of graph-graph transitions generated.
\begin{description}
\item[The columns correspond to the graph FROM which the transition was generated:]
DataType    1-vertex    2-vertex    3-vertex    ...

\item[The rows correspond to the graph TO which the transition was generated:]
GEN-\textgreater{} 1
GEN-\textgreater{} 2
...

\end{description}

\item[AccStats:]
Format as GenStats, but has the number of transitions accepted.

\item[Sequences:]
Records the number of sequences of consecutive graphs accepted with the same weight.

\end{description}

\resetcurrentobjects


\hypertarget{output-rhopii}{}\section{RHOPII}

RHOPII is produced during the calculation of a vertex sum.  It contains:

\begin{Verbatim}[commandchars=@\[\]]
Index   Determinant=Di   w@_i   P ln rho@_ii   ln s@_i   E@textasciitilde[]@_i   Degeneracy
\end{Verbatim}

where:
\begin{quote}

Index begins at 0.

\begin{notice}{note}{Note:}
This is not true for the test jobs which are not model systems.  What does Index mean?
\end{notice}

Determinant is formed by the list of spin-obitals enclosed in parentheses.

w\_i is the calcuated value of math:\emph{w\_\{veci\}=bra D\_\{veci\} | e\textasciicircum{}\{-beta H\} | D\_\{veci\} ket}.

P ln rho\_ii is formed by P, the path length, and rho\_ii is
$\rho_{\veci\veci}=\bra D_{\veci} | e^{-(\beta/P) H} | D_{\veci} \ket$
calculaed to the approximation specified by the input parameters.

s\_i is defined from $\operatorname{ln} w_i = P \operatorname{ln} \rho_{ii} + \operatorname{ln} s_i$.

E\textasciitilde{}\_i is the value of $\tilde{E}_{\veci}=\frac{\bra D_{\veci} | H e^{-\beta H} | D_{\veci} \ket}{w_i}$.
\end{quote}

\resetcurrentobjects


\hypertarget{output-rhopiiex}{}\section{RHOPIIex}

RHOPIIex is produced if a diagonalization is performed. The data is calculated
by CalcRhoPII.  It contains:

\begin{Verbatim}[commandchars=@\[\]]
Determinant=Di   w@_i   P ln rho@_ii   ln s@_i   E@textasciitilde[]@_i   Degeneracy
\end{Verbatim}

where:
\begin{quote}

Determinant is formed by the list of spin-obitals enclosed in parentheses.

w\_i is the calcuated value of $w_{\veci}=\bra D_{\veci} | e^{-\beta H} | D_{\veci} \ket$.

P ln rho\_ii is formed by P, the path length, and rho\_ii is
$\rho_{\veci\veci}=\bra D_{\veci} | e^{-(\beta/P) H} | D_{\veci} \ket$
calculaed to the approximation specified by the input parameters.

s\_i is defined from $\operatorname{ln} w_i = P \operatorname{ln} \rho_{ii} + \operatorname{ln} s_i$.

E\textasciitilde{}\_i is the value of $\tilde{E}_{\veci}=\frac{\bra D_{\veci} | H e^{-\beta H} | D_{\veci} \ket}{w_i}$.

Degeneracy is the number of symmetry related determinants which are not explicitly calculated.
\end{quote}

\resetcurrentobjects


\hypertarget{example-inputs-index}{}\chapter{Example Input Files}

\resetcurrentobjects


\hypertarget{input-examples}{}\section{Standalone MP2 calculations}

The \textbf{CALC} block is simply:

\begin{Verbatim}[commandchars=@\[\]]
Calc
    MPTheory only
EndCalc
\end{Verbatim}


\subsection{CPMD}

For a straight-forward \textbf{CPMD}-based calculation using, say, 200 spin-virtuals, the input file is:

\begin{Verbatim}[commandchars=@\[\]]
System CPMD
EndSys

Calc
    MPTheory only
EndCalc

Integrals
    UMatCache MB 750
    Freeze 0,-200
EndInt
\end{Verbatim}

where we have also allocatabed a maximum of 750MB to be used in caching the integrals.

If we wish to ignore the single excitations of the reference (Kohn--Sham) determinant, then we can use:

\begin{Verbatim}[commandchars=@\[\]]
System CPMD
EndSys

Calc
    MPTheory only
    Excitations doubles
EndCalc

Integrals
    UMatCache MB 750
    Freeze 0,-200
EndInt
\end{Verbatim}


\subsection{Molecular systems}

Similarly, for molecular systems, a valid input file is of the form:

\begin{Verbatim}[commandchars=@\[\]]
System READ
    Electrons 36
EndSys

Calc
    MPTheory only
EndCalc
\end{Verbatim}

More to come.

\begin{thebibliography}{ThomPhDThesis}
\bibitem[SumPaper]{SumPaper}{
A combinatorial approach to the electron correlation problem, Alex J. W. Thom and Ali Alavi, J. Chem. Phys. 123, 204106, (2005).
}
\bibitem[StarPaper]{StarPaper}{
Electron correlation from path resummations: the double-excitation star, Alex J. W. Thom, George H. Booth, and Ali Alavi, Phys. Chem. Chem. Phys., 10, 652-657 (2008).
}
\bibitem[ThomPhDThesis]{ThomPhDThesis}{
Towards a quantum Monte Carlo approach based on path resummations, Alex J.W. Thom, PhD Thesis (2006).
}
\bibitem[DALTON]{DALTON}{
DALTON, a molecular electronic structure program, Release 2.0 (2005), see \href{http://daltonprogram.org/}{http://daltonprogram.org/}
}
\bibitem[MolPro]{MolPro}{
MOLPRO is a package of ab initio programs written by H.-J. Werner, P. J. Knowles, R. Lindh, F. R. Manby,  M. Schütz, P. Celani, T. Korona, G. Rauhut, R. D. Amos, A. Bernhardsson, A. Berning, D. L. Cooper, M. J. O. Deegan, A. J. Dobbyn, F. Eckert, C. Hampel, G. Hetzer, A. W. Lloyd, S. J. McNicholas, W. Meyer, M. E. Mura, A. Nicklaß, P. Palmieri, R. Pitzer, U. Schumann, H. Stoll, A. J. Stone, R. Tarroni, and T. Thorsteinsson, see \href{http://www.molpro.net}{http://www.molpro.net}
}
\bibitem[CPMD]{CPMD}{
CPMD, \href{http://www.cpmd.org/}{http://www.cpmd.org/}, Copyright IBM Corp 1990-2008, Copyright MPI für Festkörperforschung Stuttgart 1997-2001.
}
\bibitem[VASP]{VASP}{
VASP
}
\bibitem[TwoElBox]{TwoElBox}{
Two interacting electrons in a box: An exact diagonalization study, Ali Alavi, JCP 113 7735 (2000).
}
\bibitem[AttenEx]{AttenEx}{
Efficient calculation of the exact exchange energy in periodic systems using a truncated Coulomb potential, James Spencer and Ali Alavi, PRB, 77 193110 (2008).
}
\bibitem[CamCasp]{CamCasp}{
Cambridge package for Calculation of Anisotropic Site Properties, Alston Misquitta and Anthony Stone.  \href{http://www-stone.ch.cam.ac.uk/programs.html\#Camcasp}{http://www-stone.ch.cam.ac.uk/programs.html\#Camcasp}
}
\bibitem[StarPaper]{StarPaper}{
Electron correlation from path resummations: the double-excitation star, Alex J. W. Thom, George H. Booth, and Ali Alavi, Phys. Chem. Chem. Phys., 10, 652-657 (2008).
}
\bibitem[GHBCPGS]{GHBCPGS}{
CPGS report, George Booth.
}
\bibitem[RGPtIII]{RGPtIII}{
Part III report, Ramin Ghorashi.
}
\end{thebibliography}

\printindex
\end{document}
