#####################
# Scalar values
#####################
[int]
type1=integer(int32)
mpitype=MPI_INTEGER
mpilen=1
mpilen2=1
ndim=0

[int64]
type1=integer(int64)
mpitype=MPI_INTEGER8

[double]
type1=real(dp)
mpitype=MPI_DOUBLE_PRECISION

[logical]
type1=LOGICAL
mpitype=MPI_LOGICAL

[complex]
type1=complex(dp)
mpitype=MPI_DOUBLE_COMPLEX


#####################
# 1-D Arrays
#####################
[arr_int]
type1=integer(int32), dimension(:)
mpitype=MPI_INTEGER
mpilen=(ubound(v,1)-lbound(v,1)+1)
mpilen2=(ubound(v2,1)-lbound(v2,1)+1)
ndim=1

[arr_int64]
type1=integer(int64), dimension(:)
mpitype=MPI_INTEGER8

[arr_double]
type1=real(dp), dimension(:)
mpitype=MPI_DOUBLE_PRECISION

[arr_complex]
type1=complex(dp), dimension(:)
mpitype=MPI_DOUBLE_COMPLEX

[arr_logical]
type1=logical, dimension(:)
mpitype=MPI_LOGICAL

######################
# 2-D Arrays
######################
[arr2_int]
type1=integer(int32), dimension(:,:)
mpitype=MPI_INTEGER
mpilen=((ubound(v,1)-lbound(v,1)+1)*(ubound(v,2)-lbound(v,2)+1))
mpilen2=((ubound(v2,1)-lbound(v2,1)+1)*(ubound(v2,2)-lbound(v2,2)+1))
ndim=2

[arr2_int64]
type1=integer(int64), dimension(:,:)
mpitype=MPI_INTEGER8

[arr2_double]
type1=real(dp), dimension(:,:)
mpitype=MPI_DOUBLE_PRECISION

[arr2_complex]
type1=complex(dp), dimension(:,:)
mpitype=MPI_DOUBLE_COMPLEX

[arr2_logical]
type1=logical, dimension(:,:)
mpitype=MPI_LOGICAL

######################
# 3-D Arrays
######################
[arr3_int]
type1=integer(int32), dimension(:,:,:)
mpitype=MPI_INTEGER
mpilen=((ubound(v,1)-lbound(v,1)+1)*(ubound(v,2)-lbound(v,2)+1)*(ubound(v,3)-lbound(v,3)+1))
mpilen2=((ubound(v2,1)-lbound(v2,1)+1)*(ubound(v2,2)-lbound(v2,2)+1)*(ubound(v2,3)-lbound(v2,3)+1))
ndim=3

[arr3_double]
type1=real(dp), dimension(:,:,:)
mpitype=MPI_DOUBLE_PRECISION

[arr3_complex]
type1=complex(dp), dimension(:,:,:)
mpitype=MPI_DOUBLE_COMPLEX

######################
# 4-D Arrays
######################
[arr4_int]
type1=integer(int32), dimension(:,:,:,:)
mpitype=MPI_INTEGER
mpilen=((ubound(v,1)-lbound(v,1)+1)*(ubound(v,2)-lbound(v,2)+1)*(ubound(v,3)-lbound(v,3)+1)*(ubound(v,4)-lbound(v,4)+1))
mpilen2=((ubound(v2,1)-lbound(v2,1)+1)*(ubound(v2,2)-lbound(v2,2)+1)*(ubound(v2,3)-lbound(v2,3)+1)*(ubound(v2,4)-lbound(v2,4)+1))
ndim=4

[arr4_double]
type1=real(dp), dimension(:,:,:,:)
mpitype=MPI_DOUBLE_PRECISION

=========================

!For a full header, see the supermodule at the end of the file.
!NOTE: Values -> V - ghb24 4/3/11


   ! mpi USE is in mixed case to avoid it being picked up by the the Configure
   ! script, as it doesn't require a module file.
module Parallel
#ifdef PARALLEL
   uSe MPI
#endif
   use ParallelHelper,  only: CommI, GetComm, MPIErr, iProcIndex
   use constants
   implicit none


#ifndef PARALLEL
! These don't exist in serial, so fudge them
   integer(MPIArg), private, parameter :: MPI_MIN=0
   integer(MPIArg), private, parameter :: MPI_MAX=0
   integer(MPIArg), private, parameter :: MPI_SUM=0
   integer(MPIArg), private, parameter :: MPI_MAXLOC=0
   integer(MPIArg), private, parameter :: MPI_MAX_ERROR_STRING=255
#endif


    interface MPIReduce
        module procedure MPIReduce_len_%(name)s
        module procedure MPIReduce_auto_%(name)s
    end interface

    interface MPISum
        module procedure MPISum_len_%(name)s
        module procedure MPISum_auto_%(name)s
    end interface

    interface MPIBcast
        module procedure MPIBcast_lenroot_%(name)s
        module procedure MPIBcast_len_%(name)s
        module procedure MPIBcast_auto_%(name)s
        module procedure MPIBcast_logic_%(name)s
    end interface

    interface MPISumAll
        module procedure MPISumAll_len_%(name)s
        module procedure MPISumAll_auto_%(name)s
    end interface

    interface MPIAllReduce
        module procedure MPIAllReduce_len_%(name)s
        module procedure MPIAllReduce_auto_%(name)s
    end interface

    interface MPIScatter
        module procedure MPIScatter_len_%(name)s
        module procedure MPIScatter_auto_%(name)s
    end interface

    interface MPIAllGather
        module procedure MPIAllGather_len_%(name)s
        module procedure MPIAllGather_auto_%(name)s
    end interface

    interface MPIGather
        module procedure MPIGather_len_%(name)s
        module procedure MPIGather_auto_%(name)s
    end interface

    interface MPIGatherV
        module procedure MPIGatherV_len_%(name)s
        module procedure MPIGatherV_len2_%(name)s
        module procedure MPIGatherV_auto_%(name)s
        module procedure MPIGatherV_auto2_%(name)s
    end interface

    interface MPIScatterV
        module procedure MPIScatterV_len_%(name)s
        module procedure MPIScatterV_len2_%(name)s
    end interface

contains

subroutine MPIReduce_len (v, iLen, iType, Ret, Node)

    ! Call MPI_REDUCE of type iType on the elements v --> ret. The
    ! number of elements to transmit is specified by iLen.
    !
    ! In:  v - The elements to be reduced over the processors.
    !      iLen   - The length of the data (in elements of its type)
    !      iType  - MPI specification (e.g. MPI_MAX)
    ! Out: Ret    - The reduced elements are returned in this array
    !               *** ON ROOT ONLY ***
    %(type1)s, intent(in)  :: v()
    %(type1)s, intent(out) :: Ret()
    integer, intent(in) :: iLen, iType
    type(CommI), intent(in),optional :: Node
    integer Comm,rt
    integer(MPIArg)  ierr
#if PARALLEL
    call GetComm(Comm,Node,rt)
    call MPI_REDUCE (v, Ret, &
    iLen, %(mpitype)s, iType, rt, &
                     Comm, ierr)
    if (ierr .ne. MPI_SUCCESS) then
        call Stop_All("MPIReduce", 'Error in MPI_REDUCE. Terminating.')
    end if
#else
    Ret=v
#endif
end subroutine

subroutine MPIReduce_auto (v, iType, Ret, Node)

    ! The same as MPIReduce_len, without the iLen specification. The number
    ! of elements is determined automatically.

    %(type1)s, intent(in)  :: v()
    %(type1)s, intent(out) :: Ret()
    integer, intent(in) :: iType
    integer(MPIArg) ierr
    type(CommI), intent(in),optional :: Node
    integer Comm,rt
#if PARALLEL
    call GetComm(Comm,Node,rt)
    call MPI_REDUCE (v, Ret, &
%(mpilen)s, &
    %(mpitype)s, iType, rt, &
                     Comm, ierr)
    if (ierr .ne. MPI_SUCCESS) then
        call Stop_All("MPIReduce", 'Error in MPI_REDUCE. Terminating.')
    end if
#else
    Ret=v
#endif
end subroutine


subroutine MPIReduce_inplace (v, iType, Node)

    ! The same as MPIReduce, but using the v array as the return
    ! array on the target node.

    %(type1)s, intent(inout)  :: v()
    integer, intent(in) :: iType
    integer(MPIArg) :: ierr
    type(CommI), intent(in), optional :: node
    integer :: comm, rt

#if PARALLEL
    call GetComm (Comm, Node, rt)
    if (iProcIndex == rt) then
        call MPI_REDUCE (MPI_IN_PLACE, v, &
%(mpilen)s, &
        %(mpitype)s, &
        iType, rt, Comm, ierr)
    else
        call MPI_REDUCE (v, MPI_IN_PLACE, &
%(mpilen)s, &
        %(mpitype)s, &
        iType, rt, Comm, ierr)
    endif
    if (ierr /= MPI_SUCCESS) &
        call stop_all('MPIReduce_inplace', 'Error in MPI_REDUCE')
#endif

end subroutine


subroutine MPIAllReduce_len (v, iLen, iType, Ret, Node)

    ! Call MPI_REDUCE with the type iType on the array v (with length
    ! iLen) outputting the results on ALL processors in the array Ret.
    !
    ! In:  v - Data to reduce
    !      iLen   - Number of elements in v and Ret
    !      iType  - Reduction operation to perform
    ! Out: Ret    - Reduced data

    %(type1)s, intent(in) :: v()
    %(type1)s, intent(out) :: Ret()
    integer, intent(in) :: iLen, iType
    integer(MPIArg) :: ierr
    type(CommI), intent(in),optional :: Node
    integer Comm
#if PARALLEL
    call GetComm(Comm,Node)

    call MPI_ALLREDUCE (v, Ret, iLen, &
    %(mpitype)s, iType, &
    Comm, ierr)
    if (ierr /= MPI_SUCCESS) then
        call stop_all ("MPIAllReduce", 'Error in MPI_ALLREDUCE. Terminating.')
    endif
#else
    Ret = v
#endif
end subroutine

subroutine MPIAllReduce_auto (v, iType, Ret, Node)

    ! The same as MPIAllReduce_len, but the length of array Value (and thus
    ! Ret) is determinend automagically

    %(type1)s, intent(in) :: v()
    %(type1)s, intent(out) :: Ret()
    integer, intent(in) :: iType
    integer(MPIArg) :: ierr
    type(CommI), intent(in),optional :: Node
    integer Comm
#if PARALLEL
    call GetComm(Comm,Node)
    call MPI_ALLREDUCE (v, Ret, &
%(mpilen)s,&
    %(mpitype)s, iType, &
    Comm, ierr)
    if (ierr /= MPI_SUCCESS) then
        call stop_all ("MPIAllReduce", 'Error in MPI_ALLREDUCE. Terminating.')
    endif
#else
    Ret = v
#endif
end subroutine

subroutine MPIAllReduce_inplace (v, iType, Node)

    ! The same as MPIAllReduce_len, but the length of array Value (and thus
    ! Ret) is determinend automagically

    %(type1)s, intent(inout) :: v()
    integer, intent(in) :: iType
    integer(MPIArg) :: ierr
    type(CommI), intent(in),optional :: Node
    integer Comm
#if PARALLEL
    call GetComm(Comm,Node)
    call MPI_ALLREDUCE (MPI_IN_PLACE, v, &
%(mpilen)s,&
    %(mpitype)s, iType,&
    Comm, ierr)
    if (ierr /= MPI_SUCCESS) then
        call stop_all ("MPIAllReduce", 'Error in MPI_ALLREDUCE. Terminating.')
    endif
#endif
end subroutine

Subroutine MPIAllReduceDatatype(v, iLen, iType, iDatatype, Ret, Node)
   !=  In:
   !=     v         The corresponding elements for each
   !=                    processor are reduced over the processors and returned in 
   !=                    Ret on all processors
   !=     iLen           Length of the data (in elements of its type)
   !=     iType          an MPI call (e.g. MPI_MAX)
   !=     iDatatype      a custom data type to pass to MPI (e.g. MPI_2INTEGER)
   !=  Out:
   !=     Ret            data store to get the results.
   %(type1)s, intent(in)  :: v()
   %(type1)s, intent(out) :: Ret()
   integer, intent(in) :: iLen, iDatatype
   integer g, itype
   integer(MPIArg) :: ierr 
   type(CommI), intent(in),optional :: Node
   integer Comm
#if PARALLEL
   call GetComm(Comm,Node)
!   write(6,*) "AllReduceDatatype %(type1)s",iLen,iType
   call MPI_ALLREDUCE(v,Ret,iLen,iDatatype,iType,Comm,ierr)
   if (ierr .ne. MPI_SUCCESS) then
      call Stop_All("MPIAllReduceDatatype",'Error in MPIAllReduce. Terminating.')
   end if
#else
   Ret=v
#endif
end subroutine

subroutine MPISumAll_len (v, iLen, Ret, Node)

    ! Sum data on different processors, leaving the result in Ret on all 
    ! of the processors
    !
    ! In:  v   - Array of data to contribute to the sum
    !      iLen     - Number of data elements in v
    ! Out: Ret      - An array of the same size as v to contain the 
    !                 summed v

    %(type1)s, intent(in) :: v()
    %(type1)s, intent(out) :: Ret()
    integer, intent(in) :: iLen
    type(CommI), intent(in),optional :: Node

    call MPIAllReduce (v, iLen, MPI_SUM, Ret, Node)

end subroutine

subroutine MPISumAll_auto (v, Ret, Node)

    ! The same as MPISumAll_auto, but the length of v is determined
    ! automagically

    %(type1)s, intent(in) :: v()
    %(type1)s, intent(out) :: Ret()
    type(CommI), intent(in),optional :: Node

    call MPIAllReduce (v, MPI_SUM, Ret, Node)

end subroutine

subroutine MPISumAll_inplace (v, Node)

    ! The same as MPISumAll, but returning the results destructively 
    ! in v

    %(type1)s, intent(inout) :: v()
    type(CommI), intent(in),optional :: Node

    call MPIAllReduce_inplace (v, MPI_SUM, Node)

end subroutine


subroutine MPISum_len(v, iLen, Ret, Node)

    ! Sum data on different processors, leaving the result only on the root
    ! node. (Or the node specified)
    !
    ! In:  v  - Array of data to contribute to the sum
    !      iLen    - Number of data elements in v
    !      Node    - The node leave the final values on.
    ! Out: Ret     - An array of the same size as v to contain the
    !                summed v.

    %(type1)s, intent(in)  :: v()
    %(type1)s, intent(out) :: Ret()
    integer, intent(in) :: iLen
    type(CommI), intent(in), optional :: Node

    call MPIReduce (v, iLen, MPI_SUM, Ret, Node)

end subroutine

subroutine MPISum_auto(v, Ret, Node)

    ! Sum data on different processors, leaving the result only on the root
    ! node. (Or the node specified). We don't need to specify the length.
    !
    ! In:  v  - Array of data to contribute to the sum
    !      Node    - The node leave the final values on.
    ! Out: Ret     - An array of the same size as v to contain the
    !                summed v.

    %(type1)s, intent(in)  :: v()
    %(type1)s, intent(out) :: Ret()
    type(CommI), intent(in), optional :: Node

    call MPIReduce (v, MPI_SUM, Ret, Node)

end subroutine

subroutine MPISum_inplace (v, Node)

    %(type1)s, intent(inout) :: v()
    type(CommI), intent(in), optional :: Node

    call MPIReduce_inplace (v, MPI_SUM, Node)

end subroutine

subroutine MPIBCast_lenroot (v, iLen, rt)

    ! Call MPI_BCAST to broadcast the value(s) in array v on processor
    ! Root to all processors, where the number of elements in array v is
    ! specified by iLen.
    !
    ! In:    iLen   - The number of elements in v
    ! InOut: v - The data to broadcast, and the returned v
    use ParallelHelper, only: CommGlobal
    %(type1)s, intent(inout) :: v()
    integer, intent(in) :: iLen,rt
    integer(MPIArg) :: error
#if PARALLEL
    call MPI_BCAST (v, iLen, &
    %(mpitype)s, rt, CommGlobal, error)
    if (error /= MPI_SUCCESS) then
        call stop_all("MPIBCast", 'Error in MPI_BCAST. Terminating.')
    endif
#endif
end subroutine
subroutine MPIBCast_len (v, iLen, Node)

    ! Call MPI_BCAST to broadcast the value(s) in array v on processor
    ! Root to all processors, where the number of elements in array v is
    ! specified by iLen.
    !
    ! In:    iLen   - The number of elements in v
    ! InOut: v - The data to broadcast, and the returned v

    %(type1)s, intent(inout) :: v()
    integer, intent(in) :: iLen
    integer(MPIArg) :: error
    type(CommI), intent(in),optional :: Node
    integer Comm,rt
#if PARALLEL
    call GetComm(Comm,Node,rt)
    call MPI_BCAST (v, iLen, &
    %(mpitype)s, rt, Comm, error)
    if (error /= MPI_SUCCESS) then
        call stop_all("MPIBCast", 'Error in MPI_BCAST. Terminating.')
    endif
#endif
end subroutine

subroutine MPIBCast_auto (v, Node)

    ! The same as MPIBcast_len, but the number of elements in v is 
    ! determined automagically
    !
    ! In:    Root   - The processor to broadcast from
    ! InOut: v - The data to broadcast, and the returned v

    %(type1)s, intent(inout) :: v()
    integer(MPIArg) :: error
    type(CommI), intent(in),optional :: Node
    integer Comm,rt

#if PARALLEL
    call GetComm(Comm,Node,rt)
    call MPI_BCAST (v, &
%(mpilen)s,&
    %(mpitype)s, rt, Comm, &
    error)
    if (error /= MPI_SUCCESS) then
        call MPIErr(error)
        call stop_all("MPIBCast", 'Error in MPI_BCAST. Terminating.')
    endif
#endif
end subroutine

subroutine MPIBCast_logic (v, tMe, Node)

    ! The same as MPIBcast_len, but the number of elements in v is 
    ! determined automagically
    !
    ! In:    tMe    - Set to be true by the processor which is sending the info
    ! InOut: v - The data to broadcast, and the returned v

    %(type1)s, intent(inout) :: v()
    logical, intent(in) :: tMe
    integer(MPIArg) :: error
    type(CommI), intent(in),optional :: Node
    integer Comm,rt,nrt
#if PARALLEL
    call GetComm(Comm,Node,rt,tMe)
    call MPI_AllReduce(rt,nrt,1,MPI_INTEGER, MPI_MAX,Comm,error)  !Work out which processor is the root
    call MPI_BCAST (v, &
%(mpilen)s,&
    %(mpitype)s, nrt, Comm, &
    error)
    if (error /= MPI_SUCCESS) then
        call stop_all("MPIBCast", 'Error in MPI_BCAST. Terminating.')
    endif
#endif
end subroutine

Subroutine MPIAlltoAll(SendBuf,SendSize,RecvBuf,RecvSize,ierr, Node)
    INTEGER,intent(in) :: SendSize,RecvSize
    INTEGER, intent(out) :: ierr
    INTEGER(MPIArg) :: error
    %(type1)s, intent(in) :: SendBuf(:)
    %(type1)s, intent(inout):: RecvBuf(:)
    type(CommI), intent(in),optional :: Node
    integer Comm
#if PARALLEL
    call GetComm(Comm,Node)
    CALL MPI_AlltoAll(SendBuf,SendSize,%(mpitype)s,&
    RecvBuf,RecvSize,%(mpitype)s,Comm,error)
    ierr=error
#else
    RecvBuf=SendBuf
    ierr=0
#endif
end subroutine

Subroutine MPIAlltoAllV(SendBuf,SendSizes,SendOffsets,RecvBuf,RecvSizes,RecvOffsets,ierr,Node)
    INTEGER,intent(in) :: SendSizes(:),SendOffsets(:),RecvSizes(:),RecvOffsets(:)
    INTEGER, intent(out) :: ierr
    INTEGER(MPIArg) :: error
    %(type1)s, intent(in) :: SendBuf(:)
    %(type1)s, intent(inout):: RecvBuf(:)
!    write(6,*) "AllToAllV %(type1)s",SendSizes,SendOffSets,RecvSizes,RecvOffsets
!    write(6,*) "AllToAllV %(type1)s",shape(SendBuf),shape(RecvBuf)
    type(CommI), intent(in),optional :: Node
    integer Comm
#if PARALLEL
    call GetComm(Comm,Node)
    CALL MPI_AlltoAllV(SendBuf,SendSizes,SendOffsets,&
    %(mpitype)s,RecvBuf,RecvSizes,RecvOffsets,&
    %(mpitype)s,Comm,error)
    ierr=error
#else
    RecvBuf=SendBuf
    ierr=0
#endif
end subroutine

Subroutine MPIAllGather_len(SendBuf,SendSize,RecvBuf,RecvSize,ierr,Node)
    INTEGER,intent(in) :: SendSize,RecvSize
    INTEGER, intent(out) :: ierr
    INTEGER(MPIArg) :: error
    %(type1)s,intent(in) :: SendBuf()
    %(type1)s,intent(inout) :: RecvBuf(:)
    type(CommI), intent(in),optional :: Node
    integer Comm,rt
#if PARALLEL
    call GetComm(Comm,Node)
    CALL MPI_AllGather(SendBuf,SendSize,%(mpitype)s,RecvBuf,&
    RecvSize,%(mpitype)s,Comm,error)
    ierr=error
#else
    RecvBuf(1)=SendBuf
    ierr=0
#endif
end subroutine

! v is the Send Buffer
! v2 is the Receive Buffer
Subroutine MPIAllGather_auto(v,v2,ierr,Node)
    INTEGER, intent(out) :: ierr
    INTEGER(MPIArg) :: error
    %(type1)s,intent(in) :: v()
    %(type1)s,intent(inout) :: v2(:)
    type(CommI), intent(in),optional :: Node
    integer Comm,rt
#if PARALLEL
    call GetComm(Comm,Node,rt)
    CALL MPI_AllGather(v,&
%(mpilen)s,&
      %(mpitype)s,v2,&
%(mpilen2)s,&
      %(mpitype)s,Comm,error)
      ierr=error
#else
    v2(1)=v
    ierr=0
#endif
end subroutine

Subroutine MPIGather_len(SendBuf,SendSize,RecvBuf,RecvSize,ierr,Node)
    INTEGER,intent(in) :: SendSize,RecvSize
    INTEGER, intent(out) :: ierr
    INTEGER(MPIArg) :: error
    %(type1)s,intent(in) :: SendBuf()
    %(type1)s,intent(inout) :: RecvBuf(:)
    type(CommI), intent(in),optional :: Node
    integer Comm,rt
#if PARALLEL
    call GetComm(Comm,Node,rt)
    CALL MPI_Gather(SendBuf,SendSize,%(mpitype)s,&
    RecvBuf,RecvSize,%(mpitype)s,rt,Comm,error)
    ierr=error
#else
    RecvBuf(1)=SendBuf
    ierr=0
#endif
end subroutine

! v is the Send Buffer
! v2 is the Receive Buffer
Subroutine MPIGather_auto(v,v2,ierr,Node)
    INTEGER, intent(out) :: ierr
    INTEGER(MPIArg) :: error
    %(type1)s,intent(in) :: v()
    %(type1)s,intent(inout) :: v2(:)
    type(CommI), intent(in),optional :: Node
    integer Comm,rt
#if PARALLEL
    call GetComm(Comm,Node,rt)
    CALL MPI_Gather(v, &
%(mpilen)s,&
      %(mpitype)s,v2, &
%(mpilen2)s,&
      %(mpitype)s,rt,COMM,error)
      ierr=error
#else
    v2(1)=v
    ierr=0
#endif
end subroutine

!Gather from many processors to the root.
! Each Processor has data in SendBuf() with length SendSize.
! The root knows the lengths of the data from each proc, and they are placed on the root into RecvBuf at displacements Disps.
Subroutine MPIGatherV_len(SendBuf,SendSize,RecvBuf,Lengths,Disps,ierr,Node)
    INTEGER,intent(in) :: SendSize,Lengths(:),Disps(:)
    INTEGER, intent(out) :: ierr
    INTEGER(MPIArg) :: error
    %(type1)s,intent(in) :: SendBuf()
    %(type1)s,intent(inout) :: RecvBuf(:)
    type(CommI), intent(in),optional :: Node
    integer Comm,rt
#if PARALLEL
    call GetComm(Comm,Node,rt)
    CALL MPI_GatherV(SendBuf,SendSize,%(mpitype)s,&
    RecvBuf,Lengths,Disps,%(mpitype)s,rt,COMM,error)
    ierr=error
#else
    RecvBuf(1)=SendBuf()
    ierr=0
#endif
end subroutine

! This gathers an array into another array with the same number of dims.
Subroutine MPIGatherV_len2(SendBuf,SendSize,RecvBuf,Lengths,Disps,ierr,Node)
    INTEGER,intent(in) :: SendSize,Lengths(:),Disps(:)
    INTEGER, intent(out) :: ierr
    INTEGER(MPIArg) :: error
    %(type1)s,intent(in) :: SendBuf(:)
    %(type1)s,intent(inout) :: RecvBuf(:)
    type(CommI), intent(in),optional :: Node
    integer Comm,rt
#if PARALLEL
    call GetComm(Comm,Node,rt)
    CALL MPI_GatherV(SendBuf,SendSize,%(mpitype)s,&
    RecvBuf,Lengths,Disps,%(mpitype)s,rt,COMM,error)
    ierr=error
#else
!    write(6,*) lbound(RecvBuf),ubound(RecvBuf),lbound(SendBuf),ubound(SendBuf)
    RecvBuf(lbound(RecvBuf,%(ndim)s+1):(lbound(RecvBuf,%(ndim)s+1)&
    +size(SendBuf,%(ndim)s+1)-1))=SendBuf(:)
    ierr=0
#endif
end subroutine

! v is the Send Buffer
! v2 is the Receive Buffer
Subroutine MPIGatherV_auto(v,v2,Lengths,Disps,ierr,Node)
    INTEGER, intent(out) :: ierr
    INTEGER(MPIArg) :: error
    INTEGER, intent(in) :: Disps(:),Lengths(:)
    %(type1)s,intent(in) :: v()
    %(type1)s,intent(inout) :: v2(:)
    type(CommI), intent(in),optional :: Node
    integer Comm,rt
#if PARALLEL
    call GetComm(Comm,Node,rt)
    CALL MPI_GatherV(v,&
%(mpilen)s,&
      %(mpitype)s,v2,Lengths,&
      Disps,%(mpitype)s,rt,COMM,error)
      ierr=error
#else
    v2(1)=v
    ierr=0
#endif
end subroutine

! This gathers an array into another array with the same number of dims.
! v is the Send Buffer
! v2 is the Receive Buffer
Subroutine MPIGatherV_auto2(v,v2,Lengths,Disps,ierr,Node)
    INTEGER, intent(out) :: ierr
    INTEGER(MPIArg) :: error
    INTEGER, intent(in) :: Disps(:),Lengths(:)
    %(type1)s,intent(in) :: v(:)
    %(type1)s,intent(inout) :: v2(:)
    type(CommI), intent(in),optional :: Node
    integer Comm,rt
#if PARALLEL
    call GetComm(Comm,Node,rt)
    CALL MPI_GatherV(v,&
%(mpilen)s,&
      %(mpitype)s,v2,Lengths,&
      Disps,%(mpitype)s,rt,COMM,error)
      ierr=error
#else
    v2(Disps(1):Disps(1)+Lengths(1))=v(:)
    ierr=0
#endif
end subroutine

!From SendBuff send to all processors
Subroutine MPIScatterV_len(SendBuf,SendSizes,Disps,RecvBuf,Length,ierr,Node)
    INTEGER,intent(in) :: SendSizes(:),Length,Disps(:)
    INTEGER, intent(out) :: ierr
    INTEGER(MPIArg) :: error
    %(type1)s,intent(in) :: SendBuf()
    %(type1)s,intent(inout) :: RecvBuf(:)
    type(CommI), intent(in),optional :: Node
    integer Comm,rt
#if PARALLEL
    call GetComm(Comm,Node,rt)
!    write(6,*) "AllToAll %(type1)s",SendSize,RecvSize
    CALL MPI_ScatterV(SendBuf,SendSizes,Disps,%(mpitype)s,&
    RecvBuf,Length,%(mpitype)s,rt,COMM,error)
    ierr=error
#else
    RecvBuf(1)=SendBuf()
    ierr=0
#endif
end subroutine

! This scatters an array into another array with the same number of dims. SendSizes are the lengths to send to each proc
!  and Disps are the displacements of the data in SendBuf.  Each processor should know its own Length
Subroutine MPIScatterV_len2(SendBuf,SendSizes,Disps,RecvBuf,Length,ierr,Node)
    INTEGER,intent(in) :: SendSizes(:),Length,Disps(:)
    INTEGER, intent(out) :: ierr
    INTEGER(MPIArg) :: error
    %(type1)s,intent(in) :: SendBuf(:)
    %(type1)s,intent(inout) :: RecvBuf(:)
    type(CommI), intent(in),optional :: Node
    integer Comm,rt
#if PARALLEL
    call GetComm(Comm,Node,rt)
!    write(6,*) "AllToAll %(type1)s",SendSize,RecvSize
    CALL MPI_ScatterV(SendBuf,SendSizes,Disps,%(mpitype)s,&
    RecvBuf,Length,%(mpitype)s,rt,COMM,error)
    ierr=error
#else
!    write(6,*) lbound(RecvBuf),ubound(RecvBuf),lbound(SendBuf),ubound(SendBuf)
    RecvBuf(:)=SendBuf(lbound(SendBuf,%(ndim)s+1):(lbound(SendBuf,%(ndim)s+1)+Length-1))
    ierr=0
#endif
end subroutine


Subroutine MPIScatter_len(SendBuf,SendSize,RecvBuf,RecvSize,ierr,Node)
    INTEGER, intent(in)  :: SendSize,RecvSize
    INTEGER, intent(out) :: ierr
    INTEGER(MPIArg) :: error
    %(type1)s, intent(in) :: SendBuf(:)
    %(type1)s, intent(inout) :: RecvBuf()
    type(CommI), intent(in),optional :: Node
    integer Comm,rt
#if PARALLEL
    call GetComm(Comm,Node,rt)
    CALL MPI_Scatter(SendBuf,SendSize,%(mpitype)s,RecvBuf,RecvSize,%(mpitype)s,rt,COMM,error)
    ierr=error
#else
    RecvBuf=SendBuf(1)
    ierr=0
#endif
end subroutine

! v is the Send Buffer
! v2 is the Receive Buffer
Subroutine MPIScatter_auto(v,v2,ierr,Node)
    INTEGER, intent(out) :: ierr
    INTEGER(MPIArg) :: error
    %(type1)s, intent(in) :: v(:)
    %(type1)s, intent(inout) :: v2()
    type(CommI), intent(in),optional :: Node
    integer Comm,rt
#if PARALLEL
    call GetComm(Comm,Node,rt)
    CALL MPI_Scatter(v,&
%(mpilen2)s,&
    %(mpitype)s,v2,&
%(mpilen2)s,&
    %(mpitype)s,rt,COMM,error)
    ierr=error
#else
    v2=v(1)
    ierr=0
#endif
end subroutine

subroutine MPIRecv(Buffer,BuffSize,Source,Tag,ierr)
    INTEGER, intent(in)  :: BuffSize,Source,Tag
    INTEGER, intent(out) :: ierr
    INTEGER(MPIArg) :: error
    %(type1)s, intent(out) :: Buffer(:)
#ifdef PARALLEL
    INTEGER :: Stat(MPI_STATUS_SIZE)
    CALL MPI_Recv(Buffer,BuffSize,%(mpitype)s,Source,Tag,MPI_COMM_WORLD,Stat,error)
    ierr=error
#else
    ierr=0
    Buffer(1)=Buffer(1)
#endif
end subroutine

subroutine MPISend(Buffer,BuffSize,Dest,Tag,ierr)
    INTEGER, intent(in)  :: BuffSize,Dest,Tag
    INTEGER, intent(out) :: ierr
    INTEGER(MPIArg) :: error
    %(type1)s, intent(in) :: Buffer(:)
#ifdef PARALLEL
    CALL MPI_Send(Buffer,BuffSize,%(mpitype)s,Dest,Tag,MPI_COMM_WORLD,error)
    ierr=error
#else
    ierr=0
#endif
end subroutine

end module

supermodule Parallel

!=  MPI interface and helper routines for parallel work.
!=  If compiled without the PARALLEL c pre-processing statement, then
!=  contains dummy routines to enable the parallel calculation algorithms to
!=  be used in serial.  This is useful for testing, development and as (in some 
!=  cases) the parallel algorithms are much more efficient that the serial
!=  analogues, as the latter are much more general.

!=  NECI will run as a standalone parallel app or in conjunction with CPMD.
!=  Only standalone is currently implemented.

!=  For some jobs,
!=  Parallelization is done over occupied electrons.  These are split among each
!=  processor such that there are an approximately equal number of pairs of
!=  electrons on each processor.
!=
!=  Each processor is given a set of 'Electron 1'.  From this it can generate
!=  a set of single excitations as well as a set of double excitations.  Double
!=  excitations can have any electron past Electron 1 as Electron 2.  This means
!=  that the lower electron numbers will have more possible pairs.
!=
!=  Parallelization is supported by the symmetry excitation generators,
!=  interfaced through GenSymExcitIt3Par There is no clean way to automatically
!=  parallelize high-vertex graph code, so each parallel routine must be
!=  specifically written.

!=  FCIMC is parallelized over determinants, using a hashing routine.  This is not dealt with here

!=  It is highly advised that the following routines are used in parallel work, as they exist both in
!=  serial and parallel compiles and aid interoperability.  The extra overhead is likely minimal compared
!=  to the communication time.

!=
!=  MPI ROUTINES
!=     MPIInit     Setup MPI and init Nodefiles if we're standalone
!=     MPIEnd      Shutdown MPI
!=     MPIStopAll  Abort all processors
!=     MPIBarrier  Wait until all processors have synched.

!   These routines exist for all types specified in the header. (Ah the beauty of templates!)
!=     MPISum      Sum data among all processors, and give the results to all
!=     MPISumRoot  Sum data among all processors, and give the results to the root
!=     MPIBCast    Send from processor Root to all other processors
!=     MPIAllReduceDatatype Like MPIAllReduce, but allows a custom datatype (like MPI_2INTEGER) to be specified
!=     MPIAllGather Gathers data from all tasks and sends it to all.
!=     MPIRecv     Receive data from a single node

!=

!=  OTHER ROUTINES
!=     GetProcElectrons


   ! mpi USE is in mixed case to avoid it being picked up by the the Configure
   ! script, as it doesn't require a module file.
#ifdef PARALLEL
   uSE mpi

#endif
   use ParallelHelper
   use constants, only: MPIArg
   IMPLICIT NONE
   interface
        subroutine gethostname(nm, sz) bind(c)
            use, intrinsic :: iso_c_binding
            implicit none
            integer(c_size_t), value :: sz
            character(kind=c_char), intent(out) :: nm
        end subroutine
   end interface
   save
   integer iProcMinE,iProcMaxE
   integer nProcessors

#ifndef PARALLEL
   integer(MPIArg), parameter :: MPI_2INTEGER=0
   integer(MPIArg), parameter :: MPI_MIN=0
   integer(MPIArg), parameter :: MPI_MAX=0
   integer(MPIArg), parameter :: MPI_SUM=0
   integer(MPIArg), parameter :: MPI_LOR=0
   integer(MPIArg), parameter :: MPI_MAXLOC=0
   integer(MPIArg), parameter :: MPI_MINLOC=0
   integer(MPIArg), parameter :: MPI_MAX_ERROR_STRING=255
#endif


   
Contains



Subroutine MPIInit(tExternal)
   != Determine the number of processors, and fork each off to its own NodeFile output file
   !=
   != In:
   !=   tExternal True if using VASP/CPMD's MPI interface, so we don't have to initialise our own.
   implicit none
   logical, intent(in) :: tExternal
   integer(MPIArg) :: ierr
   integer a,g
   integer(MPIArg) :: iProcInd,nProcess
   character*20 NodeFile
   logical, save :: initialised=.false.
#if PARALLEL
   nNodes=0  !Indicate we haven't setup nodes yet
   CommGlobal=MPI_COMM_WORLD
   if (.not.initialised) then

       if(tExternal) then
         write(6,*) 'Using CPMD MPI configuration'
       else 
         write(6,*) 'Initing MPI'

         call MPI_INIT(ierr)
         if (ierr .ne. MPI_SUCCESS) then
            call Stop_All("MPIInit",'Error starting MPI program. Terminating.')
         end if
       endif
       call MPI_COMM_RANK(MPI_COMM_WORLD, iProcInd, ierr)
       iProcIndex=iProcInd
       call MPI_COMM_SIZE(MPI_COMM_WORLD, nProcess, ierr)
       nProcessors=nProcess
       WRITE(6,*) "Number of processors: ",nProcessors

       if(tExternal) then
          write(6,*) "NECI Processor ",iProcIndex+1,'/',nProcessors
       else
!Test if I/O is allowed on all processors - get res, the attribute attached to the communicator concerning I/O
!This does not seem to work...
!      CALL MPI_Comm_get_attr(MPI_COMM_SELF,MPI_IO,res,flag,ierr)
!flag will say if can do I/O
!      Local_IO=(ierr.eq.MPI_SUCCESS.and.flag.and.res.ne.MPI_PROC_NULL)
!      IF(.not.Local_IO) THEN
!          WRITE(6,*) ierr,Local_IO,flag,res
!          CALL Stop_All('MPIInit',"IO not possible on this processor")
!      ELSE
!          WRITE(6,*) "IO possible on this processor"
!          CALL FLUSH(6)
!      ENDIF

          if(iProcIndex.eq.0) then
             write(6,*) "Processor ",iProcIndex+1,'/',nProcessors, ' as head node.'
          else

             write(6,*) "Processor ",iProcIndex+1,'/',nProcessors, ' moving to local output.'
             if (iProcIndex.lt.9) then
                 write (NodeFile,'(a,i1)') 'NodeFile',iProcIndex+1
             elseif(iProcIndex.lt.99) then
                 write (NodeFile,'(a,i2)') 'NodeFile',iProcIndex+1
             elseif(iProcIndex.lt.999) then
                 write (NodeFile,'(a,i3)') 'NodeFile',iProcIndex+1
             else
                 write (NodeFile,'(a,i4)') 'NodeFile',iProcIndex+1
             end if
             write(6,*) "outfile=",NodeFile
             close(6,status="keep")
             open(6,file=NodeFile)
             write(6,*) "Processor ",iProcIndex+1,'/',nProcessors, ' on local output.'
          endif
          call GetProcElectrons(iProcIndex,iProcMinE,iProcMaxE) 
!  Just synchronize everything briefly
          a=iProcIndex+1
          call MPISumAll(a,1,g)
          WRITE(6,*) "Sum: ",g
       endif

       call MPI_ERRHANDLER_SET(MPI_COMM_WORLD,MPI_ERRORS_RETURN,ierr)
   end if

#else 

   ! Dummy set up for serial work.
   iProcIndex=0
   nProcessors=1
#endif
   RETURN
   
end subroutine

! Create communicators for within and between each of the nodes
! As yet a work in progress, but hopefully the structure is there and usable.
! ParallelHelper.F90 contains various globals for this work.

! A parallel job is parallellized over many cores (denoted processors here).
! Various parallel architectures make it useful to introduce the concept of nodes,
!  which are a group of processors with a (fast-access) shared memory space.  
! These may be physical nodes (in which case all the cores on all the physical CPUs
!  sharing the same motherboard and memory) would be grouped together into a node,
!  or alternatively, may be subgroups of this set.
! Currently nodes are determined as those cores which have the same hostname. 
!  This is not overly portable, and will need to be customized for different parallel architectures.
! Each node has a node root core designated.

!
!  With the concept of nodes comes the introduction of different levels of communicators.
!   CommGlobal      is a synonym for MPI_COMM_WORLD and communicates between all cores
!   CommNodes(i)    is the communicator for node i (zero-based)
!   CommRoots       is the communicator between roots of all nodes
!
!  Explicit use of these shoudl be avoided, as almost all the MPI... functions in this module 
!  take some optional arguments to make the communicator selction easier.
!
!  e.g.
!    !The following work for most MPI... functions
!    integer :: i
!    MPIBCast(i)            !This will broadcast with CommGlobal
!    MPIBCast(i,Node)       !This will broadcast with the communicator within each node
!    if(bNodeRoot) then
!       MPIBCast(i,Roots)   !Broadcast between roots of all nodes.  Remember the IF, otherwise horrors occur.
!    endif                       ! (TODO: Perhaps the if should be put inside the MPIBCast?)
!
!
!    !Now for BCast-specfic ones.
!    MPIBCast(i,.true.)     !Broadcast to all cores from this core
!    ! MPIBCast(i,.false.)  ! This should be on the other cores otherwise more horrors
!    MPIBCast(i,.true.,Node)!Broadcast to the node from this code
!    ! etc
!
!    Node is of the TYPE(CommI) which allows overloading of these functions. For actual indices, use
!      iProcIndex       ! index of this Core.  0...(nProcessors-1)
!      iNodeIndex       ! index of this node.  0...(nNodes-1)
!      iIndexInNode     ! index of this core in this node.  0...(NodeLengths(iNodeIndex)-1)
!      ProcNode(i)      ! The node which Core i is on
!
!    Currently FCIMCPar is setup to allocate one core per node.  This is a hack for the moment.
!    There's a more sophisticated system of logical nodes, specified by CALC/LOGICALNODESIZE
!    Shared memory has been modified to deal with this.  If shared memory within only a logical
!    node is required use a form like
!
!      call shared_allocate_iluts("DetList",DetList,(/nIfTot,nMaxAmpl/),iNodeIndex)
!
!     where iNodeIndex is the logical node.  Otherwise shared memory is over physical nodes,
!       irrespective of the logical node structure.

subroutine MPINodes(tUseProcsAsNodes)
   use, intrinsic :: iso_c_binding
   use CalcData, only : iLogicalNodeSize
   implicit none
   logical, intent(in) :: tUseProcsAsNodes !Set if we use Procs as Nodes.
   integer Group,i,j,n
   integer(c_size_t) length
   character(len=30) nm
   character(len=30) nm2
   character(len=30) nms(0:nProcessors-1)
   integer ierr
   integer GroupProc(nProcessors)
   logical, save :: initialised=.false.
   if(initialised) return
   initialised=.true.
   allocate(Nodes(0:nProcessors-1))
   allocate(ProcNode(0:nProcessors-1))
   allocate(NodeRoots(0:nProcessors-1))
#ifdef PARALLEL
   allocate(NodeLengths(0:nProcessors))  !Temp allocate for now
   NodeLengths=0
   if(iLogicalNodeSize/=0) write(6,*)  "Using Logical Node Size ", iLogicalNodeSize
   if(tUseProcsAsNodes) then
      call MPI_Comm_Group(CommGlobal,Group,ierr)
      write(6,*) "Allocating each processor as a separate node."
      if(iProcIndex==root) then
         nNodes=0
         write(6,*) "Processor      Node"
         do i=0,nProcessors-1
            NodeRoots(nNodes)=i
            ProcNode(i)=nNodes
            Nodes(i)%n=nNodes
            nNodes=nNodes+1
            write(6,"(2I10)") i,Nodes(i)%n
         enddo
      endif
   else 
      call MPI_Comm_Group(CommGlobal,Group,ierr)
      length=30
      call gethostname(nm,length)
      do i=1,30
         if (nm(i:i)==char(0)) then
            nm(i:30)=' '
            exit
         endif
      enddo
      nm2=nm
      call MPI_Gather(nm2,30,MPI_CHARACTER,nms,30,MPI_CHARACTER,root,CommGlobal,ierr)
      write(6,*) "Processor      Node hostname"
      if(iProcIndex==root) then
         nNodes=0
         do i=0,nProcessors-1
            Nodes(i)%n=-1
            do j=0,nNodes-1
               if (nms(i)==nms(NodeRoots(j))) then
                  if((iLogicalNodeSize==0).or.NodeLengths(j)< iLogicalNodeSize) then
                     Nodes(i)%n=j
                     ProcNode(i)=j
                     NodeLengths(j)=NodeLengths(j)+1
                     exit
                  endif
               endif 
            enddo
            if (Nodes(i)%n==-1) then
               NodeRoots(nNodes)=i
               Nodes(i)%n=nNodes
               ProcNode(i)=nNodes
               NodeLengths(nNodes)=NodeLengths(nNodes)+1
               nNodes=nNodes+1
            endif
            write(6,"(2I10,A,A)") i,Nodes(i)%n," ",nms(i)
         enddo
      endif
   endif
   deallocate(NodeLengths)
   call MPI_BCAST (nNodes, 1, MPI_INTEGER, root, CommGlobal, ierr)
   call MPI_BCAST (Nodes, nProcessors, MPI_INTEGER, root, CommGlobal, ierr)
   call MPI_BCAST (ProcNode, nProcessors, MPI_INTEGER, root, CommGlobal, ierr)
   call MPI_BCAST (NodeRoots, nProcessors, MPI_INTEGER, root, CommGlobal, ierr)
   allocate(CommNodes(0:nNodes-1))
   allocate(GroupNodes(0:nNodes-1))
   allocate(NodeLengths(0:nNodes-1))
   Node=Nodes(iProcIndex)
   iNodeIndex=Node%n  !Used as a plain integer version.
   if (iProcIndex==NodeRoots(Node%n)) then
      bNodeRoot=.true.
      write(6,*) "I am the node root for node ", Node%n
   else
      bNodeRoot=.false.
   endif
   do i=0, nNodes-1
      n=0
      do j=0,nProcessors-1
         if(Nodes(j)%n==i) then
            if(j==iProcIndex) iIndexInNode=n 
            n=n+1
            GroupProc(n)=j
         endif
      enddo
      NodeLengths(i)=n
      call MPI_Group_incl(Group,n,GroupProc,GroupNodes(i),ierr)  !Create a group 
      call MPI_Comm_Create(CommGlobal,GroupNodes(i),CommNodes(i),ierr) !Create the Communicator
   enddo
   call MPI_Group_incl(Group,nNodes,NodeRoots,GroupRoots,ierr)  !Create a group 
   call MPI_Comm_Create(CommGlobal,GroupRoots,CommRoot,ierr) !Create the Communicator
#else
   nNodes=1
   iIndexInNode=0
   ProcNode(0)=0
   NodeRoots(0)=0
   bNodeRoot=.true.
   allocate(NodeLengths(0:nNodes-1))
   allocate(CommNodes(0:nNodes-1))
   allocate(GroupNodes(0:nNodes-1))
   NodeLengths(0)=1
#endif //def PARALLEL
   Roots%n=-1  !A communicator index between roots
end subroutine

Subroutine MPIEnd(tExternal)
   !=  Shutdown our MPI Interface if we're not using CPMD/VASP's
   !=
   != In:
   !=   tExternal Set if using an external program's MPI interface
   !=             (currently CPMD or VASP), in which case the external
   !=             program handles MPI termination.
   implicit none
   logical tExternal
   integer(MPIArg) ierr
#if PARALLEL
   if(.not.tExternal) then
      call MPI_FINALIZE(ierr)
   endif
#endif
end subroutine



Subroutine MPIStopAll(error_str)
   !=  Abort all processors.
   !=  
   !=  In:
   !=     error_str: parameter string containing error used as argument to STOP.
   character(3) :: error_str
   integer(MPIArg) :: error_code, ierror
#if PARALLEL
   ! errorcode: Error returned to invoking environment.
   ! ierror: error status (of abort: was abort successful?)
   ! Currently neither are analysed.
   call MPI_ABORT(MPI_COMM_WORLD, error_code, ierror)
   WRITE(6,*) error_str
   CALL FLUSH(6)
   stop
#endif
end subroutine

Subroutine GetProcElectrons(iProcIndex,iMinElec,iMaxElec)
   !=  Choose min and max electrons such that ordered pairs are distributed evenly across processors
   !=
   !=  In:
   !=     iProcIndex  Index of this processor (starting at 1).
   !=  Out:
   !=     iMinElec    First electron to allocate to this processor.
   !=     iMaxElec    Last electron to allocate to this processor.
   use SystemData, only: nEl
   implicit none
   integer iProcIndex,iMinElec,iMaxElec
   real*8 nCur
#ifdef PARALLEL
!Invert X=n(n-1)/2
   nCur=((nProcessors+1-iProcIndex)*nEl*(nEl-1.d0)/nProcessors)

   nCur=nEl+1-(1+sqrt(1.d0+4*nCur))/2
 !Hitting smack bang on an integer causes problems
   if(ceiling(nCur).eq.floor(nCur)) nCur=nCur-1e-6
   iMinElec=ceiling(nCur)
   if(iProcIndex.eq.1) iMinElec=1
   nCur=((nProcessors-iProcIndex)*nEl*(nEl-1.d0)/nProcessors)
   nCur=nEl+1-(1+sqrt(1.d0+4*nCur))/2
 !Hitting smack bang on an integer causes problems
   if(ceiling(nCur).eq.floor(nCur)) nCur=nCur-1e-6
   iMaxElec=floor(nCur)
   if(iProcIndex.eq.nProcessors) iMaxElec=nEl
#else
   ! Serial calculation: all electrons on one processor.
   iMinElec=1
   iMaxElec=nEl
#endif
end subroutine

end supermodule
