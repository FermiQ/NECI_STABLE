[int]
type1=integer(kind=int32)
mpitype=MPI_INTEGER
mpilen=1

[int64]
type1=integer(kind=int64)
mpitype=MPI_INTEGER8

[double]
type1=real*8
mpitype=MPI_DOUBLE_PRECISION

[logical]
type1=LOGICAL
mpitype=MPI_LOGICAL

[arr_int]
type1=integer(kind=int32), dimension(:)
mpitype=MPI_INTEGER
mpilen=ubound(Values,1)

[arr2_int]
type1=integer(kind=int32), dimension(:,:)
mpitype=MPI_INTEGER
mpilen=(ubound(Values,1)*ubound(Values,2))

[arr_int64]
type1=integer(kind=int64), dimension(:)
mpitype=MPI_INTEGER8
mpilen=ubound(Values,1)

[arr_logical]
type1=logical, dimension(:)
mpitype=MPI_LOGICAL

[arr3_double]
type1=real*8, dimension(:,:,:)
mpitype=MPI_DOUBLE_PRECISION
mpilen=(ubound(Values,1)*ubound(Values,2)*ubound(Values,3))

[arr2_double]
type1=real*8, dimension(:,:)
mpitype=MPI_DOUBLE_PRECISION
mpilen=(ubound(Values,1)*ubound(Values,2))

[arr_double]
type1=real*8, dimension(:)
mpitype=MPI_DOUBLE_PRECISION
mpilen=ubound(Values,1)
===================


!For a full header, see the supermodule at the end of the file.


   ! mpi USE is in mixed case to avoid it being picked up by the the Configure
   ! script, as it doesn't require a module file.
module Parallel
#ifdef PARALLEL
   uSe MPI
#endif
   use constants
   implicit none
#ifndef PARALLEL
! These don't exist in serial, so fudge them
   integer, private, parameter :: MPI_MIN=0
   integer, private, parameter :: MPI_MAX=0
   integer, private, parameter :: MPI_SUM=0
   integer, private, parameter :: MPI_MAXLOC=0
   integer, private, parameter :: MPI_MAX_ERROR_STRING=255
#endif

    interface MPIReduce
        module procedure MPIReduce_len_%(name)s
        module procedure MPIReduce_auto_%(name)s
    end interface

    interface MPIBcast
        module procedure MPIBcast_len_%(name)s
        module procedure MPIBcast_auto_%(name)s
    end interface

contains

subroutine MPIReduce_len (Values, iLen, iType, Ret)

    ! Call MPI_REDUCE of type iType on the elements values --> ret. The
    ! number of elements to transmit is specified by iLen.
    !
    ! In:  Values - The elements to be reduced over the processors.
    !      iLen   - The length of the data (in elements of its type)
    !      iType  - MPI specification (e.g. MPI_MAX)
    ! Out: Ret    - The reduced elements are returned in this array
    !               *** ON ROOT ONLY ***
    %(type1)s :: Values()
    %(type1)s :: Ret()
    integer iLen
    integer ierr, itype
#if PARALLEL
    call MPI_REDUCE (Values, Ret, iLen, %(mpitype)s, iType, 0, &
                     MPI_COMM_WORLD, ierr)
    if (ierr .ne. MPI_SUCCESS) then
        call Stop_All("MPIReduce", 'Error in MPI_REDUCE. Terminating.')
    end if
#else
    Ret=Values
#endif
end subroutine

subroutine MPIReduce_auto (Values, iType, Ret)

    ! The same as MPIReduce_len, without the iLen specification. The number
    ! of elements is determined automatically.

    %(type1)s :: Values()
    %(type1)s :: Ret()
    integer ierr, itype
#if PARALLEL
    call MPI_REDUCE (Values, Ret, %(mpilen)s, %(mpitype)s, iType, 0, &
                     MPI_COMM_WORLD, ierr)
    if (ierr .ne. MPI_SUCCESS) then
        call Stop_All("MPIReduce", 'Error in MPI_REDUCE. Terminating.')
    end if
#else
    Ret=Values
#endif
end subroutine

Subroutine MPIAllReduce(Values, iLen, iType, Ret)
   !=  In:
   !=     Values         The corresponding elements for each
   !=                    processor are reduced over the processors and returned in 
   !=                    Ret on all processors
   !=     iLen           Length of the data (in elements of its type)
   !=     iType          an MPI call (e.g. MPI_MAX)
   !=  Out:
   !=     Ret            data store to get the results.
   %(type1)s :: Values()
   %(type1)s :: Ret()
   integer iLen
   integer g, ierr,rc,itype
#if PARALLEL
!   write(6,*) "AllReduce %(type1)s",iLen,iType
   g=MPI_COMM_WORLD
   call MPI_ALLREDUCE(Values,Ret,iLen,%(mpitype)s,iType,g,ierr)
   if (ierr .ne. MPI_SUCCESS) then
      call Stop_All("MPIAllReduce",'Error in MPIAllReduce. Terminating.')
   end if
#else
   Ret=Values
#endif
end subroutine

Subroutine MPIAllReduceDatatype(Values, iLen, iType, iDatatype, Ret)
   !=  In:
   !=     Values         The corresponding elements for each
   !=                    processor are reduced over the processors and returned in 
   !=                    Ret on all processors
   !=     iLen           Length of the data (in elements of its type)
   !=     iType          an MPI call (e.g. MPI_MAX)
   !=     iDatatype      a custom data type to pass to MPI (e.g. MPI_2INTEGER)
   !=  Out:
   !=     Ret            data store to get the results.
   %(type1)s :: Values()
   %(type1)s :: Ret()
   integer iLen,iDatatype
   integer g, ierr,rc,itype
#if PARALLEL
!   write(6,*) "AllReduceDatatype %(type1)s",iLen,iType
   g=MPI_COMM_WORLD
   call MPI_ALLREDUCE(Values,Ret,iLen,iDatatype,iType,g,ierr)
   if (ierr .ne. MPI_SUCCESS) then
      call Stop_All("MPIAllReduce",'Error in MPIAllReduce. Terminating.')
   end if
#else
   Ret=Values
#endif
end subroutine

Subroutine MPISum(Values, iLen, Ret)
   !=  Sum data among all processors, and give the results to the all
   !=  In:
   !=     Values(iLen)   Data (array possibly)  The corresponding elements for each
   !=                    processor are summed and returnd into Ret on all processors
   !=     iLen           Length of the data (in elements of its type)
   !=  Out:
   !=     Return(iLen)   Variable (array possibly) to get the results.
   %(type1)s :: Values()
   %(type1)s :: Ret()
   integer iLen
   call MPIAllReduce(Values,iLen,MPI_SUM,Ret)
end subroutine

Subroutine MPISumRoot(Values, iLen, Ret, root)
   !=  Sum data among all processors, and give the results to the root 
   !=  In:
   !=     Values(iLen)   Data (array possibly)  The corresponding elements for each
   !=                    processor are summed and returnd into Ret
   !=     iLen           Length of the data (in elements of its type)
   !=     root           Processor which to sum the result to.
   !=  Out:
   !=     Return(iLen)   Variable (array possibly) to get the results.
   %(type1)s :: Values()
   %(type1)s :: Ret()
   integer iLen,i,root
   integer ierr,rc
#ifdef PARALLEL
!   write(6,*) "SumRoot %(type1)s",iLen,Root
   call MPI_REDUCE(Values,Ret,iLen,%(mpitype)s,MPI_SUM,root,MPI_COMM_WORLD,ierr)
   if (ierr .ne. MPI_SUCCESS) then
      call Stop_All("MPIDSumRootArr",'Error in MPIDSumRootArr. Terminating.')
   end if
#else
   Ret=Values
#endif
end subroutine

subroutine MPIBCast_len (Values, iLen, Root)

    ! Call MPI_BCAST to broadcast the value(s) in array Values on processor
    ! Root to all processors, where the number of elements in array Values is
    ! specified by iLen.
    !
    ! In:    iLen   - The number of elements in Values
    !        Root   - The processor to broadcast from
    ! InOut: Values - The data to broadcast, and the returned values

    %(type1)s, intent(inout) :: Values()
    integer, intent(in) :: iLen, Root
    integer :: error
#ifdef PARALLEL
    call MPI_BCAST (Values, iLen, %(mpitype)s, Root, MPI_COMM_WORLD, error)
    if (error /= MPI_SUCCESS) then
        call stop_all("MPIBCast", 'Error in MPI_BCAST. Terminating.')
    endif
#endif
end subroutine

subroutine MPIBCast_auto (Values, Root)

    ! The same as MPIBcast_len, but the number of elements in values is 
    ! determined automagically
    !
    ! In:    Root   - The processor to broadcast from
    ! InOut: Values - The data to broadcast, and the returned values

    %(type1)s, intent(inout) :: Values()
    integer, intent(in) :: Root
    integer :: error
#ifdef PARALLEL
    call MPI_BCAST (Values, %(mpilen)s, %(mpitype)s, Root, MPI_COMM_WORLD, &
                    error)
    if (error /= MPI_SUCCESS) then
        call stop_all("MPIBCast", 'Error in MPI_BCAST. Terminating.')
    endif
#endif
end subroutine

Subroutine MPIAlltoAll(SendBuf,SendSize,RecvBuf,RecvSize,ierr)
    INTEGER :: SendSize,ierr,RecvSize
    %(type1)s :: SendBuf(:)
    %(type1)s :: RecvBuf(:)
#ifdef PARALLEL
!    write(6,*) "AllToAll %(type1)s",SendSize,RecvSize
    CALL MPI_AlltoAll(SendBuf,SendSize,%(mpitype)s,RecvBuf,RecvSize,%(mpitype)s,MPI_COMM_WORLD,ierr)
#else
    RecvBuf=SendBuf
#endif
end subroutine



end module

supermodule Parallel

!=  MPI interface and helper routines for parallel work.
!=  If compiled without the PARALLEL c pre-processing statement, then
!=  contains dummy routines to enable the parallel calculation algorithms to
!=  be used in serial.  This is useful for testing, development and as (in some 
!=  cases) the parallel algorithms are much more efficient that the serial
!=  analogues, as the latter are much more general.

!=  NECI will run as a standalone parallel app or in conjunction with CPMD.
!=  Only standalone is currently implemented.

!=  For some jobs,
!=  Parallelization is done over occupied electrons.  These are split among each
!=  processor such that there are an approximately equal number of pairs of
!=  electrons on each processor.
!=
!=  Each processor is given a set of 'Electron 1'.  From this it can generate
!=  a set of single excitations as well as a set of double excitations.  Double
!=  excitations can have any electron past Electron 1 as Electron 2.  This means
!=  that the lower electron numbers will have more possible pairs.
!=
!=  Parallelization is supported by the symmetry excitation generators,
!=  interfaced through GenSymExcitIt3Par There is no clean way to automatically
!=  parallelize high-vertex graph code, so each parallel routine must be
!=  specifically written.

!=  FCIMC is parallelized over determinants, using a hashing routine.  This is not dealt with here

!=  It is highly advised that the following routines are used in parallel work, as they exist both in
!=  serial and parallel compiles and aid interoperability.  The extra overhead is likely minimal compared
!=  to the communication time.

!=
!=  MPI ROUTINES
!=     MPIInit     Setup MPI and init Nodefiles if we're standalone
!=     MPIEnd      Shutdown MPI
!=     MPIStopAll  Abort all processors
!=     MPIBarrier  Wait until all processors have synched.

!   These routines exist for all types specified in the header. (Ah the beauty of templates!)
!=     MPISum      Sum data among all processors, and give the results to all
!=     MPISumRoot  Sum data among all processors, and give the results to the root
!=     MPIBCast    Send from processor Root to all other processors
!=     MPIAllReduceDatatype Like MPIAllReduce, but allows a custom datatype (like MPI_2INTEGER) to be specified
!=

!=  OTHER ROUTINES
!=     GetProcElectrons


   ! mpi USE is in mixed case to avoid it being picked up by the the Configure
   ! script, as it doesn't require a module file.
#ifdef PARALLEL
   uSE mpi

#endif
   IMPLICIT NONE
   save
   integer iProcIndex,iProcMinE,iProcMaxE
   integer nProcessors
#ifndef PARALLEL
   integer, parameter :: MPI_2INTEGER=0
   integer, parameter :: MPI_MIN=0
   integer, parameter :: MPI_MAX=0
   integer, parameter :: MPI_SUM=0
   integer, parameter :: MPI_MAXLOC=0
   integer, parameter :: MPI_MAX_ERROR_STRING=255
#endif

Contains



Subroutine MPIInit(tExternal)
   != Determine the number of processors, and fork each off to its own NodeFile output file
   !=
   != In:
   !=   tExternal True if using VASP/CPMD's MPI interface, so we don't have to initialise our own.
   implicit none
   logical, intent(in) :: tExternal
   integer numtasks, rank, ierr, rc
   integer a,b,g
   character*20 NodeFile
   logical :: Local_IO,flag
   logical, save :: initialised=.false.
#if PARALLEL
   integer(Kind=MPI_ADDRESS_KIND) :: res

   if (.not.initialised) then

       if(tExternal) then
         write(6,*) 'Using CPMD MPI configuration'
       else 
         write(6,*) 'Initing MPI'

         call MPI_INIT(ierr)
         if (ierr .ne. MPI_SUCCESS) then
            call Stop_All("MPIInit",'Error starting MPI program. Terminating.')
         end if
       endif
       call MPI_COMM_RANK(MPI_COMM_WORLD, iProcIndex, ierr)
       call MPI_COMM_SIZE(MPI_COMM_WORLD, nProcessors, ierr)
       WRITE(6,*) "Number of processors: ",nProcessors

       if(tExternal) then
          write(6,*) "NECI Processor ",iProcIndex+1,'/',nProcessors
       else
!Test if I/O is allowed on all processors - get res, the attribute attached to the communicator concerning I/O
!This does not seem to work...
!      CALL MPI_Comm_get_attr(MPI_COMM_SELF,MPI_IO,res,flag,ierr)
!flag will say if can do I/O
!      Local_IO=(ierr.eq.MPI_SUCCESS.and.flag.and.res.ne.MPI_PROC_NULL)
!      IF(.not.Local_IO) THEN
!          WRITE(6,*) ierr,Local_IO,flag,res
!          CALL Stop_All('MPIInit',"IO not possible on this processor")
!      ELSE
!          WRITE(6,*) "IO possible on this processor"
!          CALL FLUSH(6)
!      ENDIF

          if(iProcIndex.eq.0) then
             write(6,*) "Processor ",iProcIndex+1,'/',nProcessors, ' as head node.'
          else

             write(6,*) "Processor ",iProcIndex+1,'/',nProcessors, ' moving to local output.'
             if (iProcIndex.lt.9) then
                 write (NodeFile,'(a,i1)') 'NodeFile',iProcIndex+1
             elseif(iProcIndex.lt.99) then
                 write (NodeFile,'(a,i2)') 'NodeFile',iProcIndex+1
             elseif(iProcIndex.lt.999) then
                 write (NodeFile,'(a,i3)') 'NodeFile',iProcIndex+1
             else
                 write (NodeFile,'(a,i4)') 'NodeFile',iProcIndex+1
             end if
             write(6,*) "outfile=",NodeFile
             close(6,status="keep")
             open(6,file=NodeFile)
             write(6,*) "Processor ",iProcIndex+1,'/',nProcessors, ' on local output.'
          endif
          call GetProcElectrons(iProcIndex,iProcMinE,iProcMaxE) 
!  Just synchronize everything briefly
          a=iProcIndex+1
          call MPISum(a,1,g)
          WRITE(6,*) "Sum: ",g
       endif

       initialised=.true.
       call MPI_ERRHANDLER_SET(MPI_COMM_WORLD,MPI_ERRORS_RETURN,ierr)
   end if

#else 

   ! Dummy set up for serial work.
   iProcIndex=0
   nProcessors=1
#endif

   RETURN
   
end subroutine



Subroutine MPIEnd(tExternal)
   !=  Shutdown our MPI Interface if we're not using CPMD/VASP's
   !=
   != In:
   !=   tExternal Set if using an external program's MPI interface
   !=             (currently CPMD or VASP), in which case the external
   !=             program handles MPI termination.
   implicit none
   logical tExternal
   integer ierr
#if PARALLEL
   if(.not.tExternal) then
      call MPI_FINALIZE(ierr)
   endif
#endif
end subroutine



Subroutine MPIStopAll(error_str)
   !=  Abort all processors.
   !=  
   !=  In:
   !=     error_str: parameter string containing error used as argument to STOP.
   character(3) :: error_str
   integer error_code,ierror
#if PARALLEL
   ! errorcode: Error returned to invoking environment.
   ! ierror: error status (of abort: was abort successful?)
   ! Currently neither are analysed.
   call MPI_ABORT(MPI_COMM_WORLD, error_code, ierror)
   WRITE(6,*) error_str
   CALL FLUSH(6)
   stop
#endif
end subroutine

Subroutine GetProcElectrons(iProcIndex,iMinElec,iMaxElec)
   !=  Choose min and max electrons such that ordered pairs are distributed evenly across processors
   !=
   !=  In:
   !=     iProcIndex  Index of this processor (starting at 1).
   !=  Out:
   !=     iMinElec    First electron to allocate to this processor.
   !=     iMaxElec    Last electron to allocate to this processor.
   use SystemData, only: nEl
   implicit none
   integer iProcIndex,iMinElec,iMaxElec
   real*8 nCur
#ifdef PARALLEL
!Invert X=n(n-1)/2
   nCur=((nProcessors+1-iProcIndex)*nEl*(nEl-1.d0)/nProcessors)

   nCur=nEl+1-(1+sqrt(1.d0+4*nCur))/2
 !Hitting smack bang on an integer causes problems
   if(ceiling(nCur).eq.floor(nCur)) nCur=nCur-1e-6
   iMinElec=ceiling(nCur)
   if(iProcIndex.eq.1) iMinElec=1
   nCur=((nProcessors-iProcIndex)*nEl*(nEl-1.d0)/nProcessors)
   nCur=nEl+1-(1+sqrt(1.d0+4*nCur))/2
 !Hitting smack bang on an integer causes problems
   if(ceiling(nCur).eq.floor(nCur)) nCur=nCur-1e-6
   iMaxElec=floor(nCur)
   if(iProcIndex.eq.nProcessors) iMaxElec=nEl
#else
   ! Serial calculation: all electrons on one processor.
   iMinElec=1
   iMaxElec=nEl
#endif
end subroutine

Subroutine MPIBarrier(error)
    INTEGER :: error
#ifdef PARALLEL
    CALL MPI_Barrier(MPI_COMM_WORLD,error)
#endif
end subroutine

end supermodule
