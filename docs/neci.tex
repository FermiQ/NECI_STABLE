\documentclass[a4paper,notitlepage]{scrreprt}
\usepackage{listings} % For source code presentation
\usepackage{needspace}
\usepackage{color}
\usepackage[hyphens]{url}
\usepackage{pifont}
\usepackage{multicol}
\usepackage{blindtext}
\usepackage{braket}
\usepackage{amsmath}
\usepackage[framemethod=tikz]{mdframed}
\usepackage[top=3cm, bottom=3cm]{geometry}
\usepackage{hyperref}

% Set up clickable links in contents
\hypersetup{
    colorlinks=true,
    linktoc=all,
    linkcolor=blue,
    urlcolor=blue
}
%	citecolor=black,
%	filecolor=black,
%	linkcolor=black,
%	urlcolor=black
%}

% Use paragraph spacing, rather than indentation. This is generally easier
% to read on a screen (especially for technical documentation).
\usepackage{parskip}

% An itemize with a bit less space floating around
\newenvironment{packed_enum}{
	\begin{enumerate}
		\setlength{\itemsep}{1pt}
		\setlength{\parskip}{0pt}
		\setlength{\parsep}{0pt}
	}{\end{enumerate}}
\newenvironment{packed_itemize}{
	\begin{itemize}
		\setlength{\itemsep}{1pt}
		\setlength{\parskip}{0pt}
		\setlength{\parsep}{0pt}
	}{\end{itemize}}

% Default formatting for code
\definecolor{commentgreen}{rgb}{0,0.6,0}
\lstset{
	keywordstyle=\color{blue},
	commentstyle=\color{commentgreen},
	language=Fortran,
	tabsize=4,
	basicstyle=\footnotesize\ttfamily,
	morekeywords={HElement_t,pure,elemental,supermodule,ubound,lbound,abstract},
	belowskip=-3pt,
	lineskip=-1pt    % Bring the lines a bit closer together (eliminate spaces)
}

% Define INI-file syntax
\lstdefinelanguage{ini}
{
    morecomment=[s][\color{blue}]{[}{]},
    morecomment=[l]{\#},
	morekeywords={integer,real},
} 

% Headitem is a new command for the description environment, which is roughly
% equivalent to "\item[title] \hfill \\ ...", but ensuring that space is used
% correctly. May need to tweak needspace to be better.
\newcommand\headitem[1]{\needspace{1.5\baselineskip}\item[{\boldmath #1 \nopagebreak}] \hfill \\ \nopagebreak}
\newcommand\headit[1]{\needspace{1.5\baselineskip}\item[\textnormal{\itshape #1 \nopagebreak}] \hfill \\ \nopagebreak}
\newcommand\codeitem[1]{\needspace{1.5\baselineskip}\item[\textnormal{\ttfamily #1 \nopagebreak}] \hfill \\ \nopagebreak}

% Use a defined command instead of lstinline, as it breaks texmaker
% syntax highlighting
\let\code\lstinline

% A nice box for warnings and exclamations
\newenvironment{warningbox}
	{\par\begin{mdframed}[%
		linewidth = 2pt, %
	    linecolor = red, %
	    roundcorner = 6pt, %
		backgroundcolor = gray!20
	]\begin{list}{}{\leftmargin=1cm
			           \labelwidth=\leftmargin}\item[\Large\ding{43}]}
	{\end{list}\end{mdframed}\par}
	
% A nice mathematics abs command
\providecommand{\abs}[1]{\ensuremath{\left\lvert#1\right\lvert}}
	
% Modify TOC command so it can start on the same page as the title
\makeatletter
\newcommand*{\toccontents}{\@starttoc{toc}}
\makeatother

\begin{document}

\author{Simon Smart, Nick Blunt and George Booth}
\title{Unravelling the mysteries of NECI}
\subtitle{The $n$-Electron Configuration Interaction solver}
\maketitle

\toccontents

\chapter{Using NECI}
\section{Getting into the game}
\subsection{Getting the code}
	The NECI repository is stored on bitbucket. To gain access you need to be
	invited. Contact one of the repository administrators
	[Simon Smart (\url{simondsmart@gmail.com}), George Booth
	(\url{george.booth24@gmail.com}) and Nick Blunt (\url{nsb37@cam.ac.uk})] who will
	invite you. If you already have a bitbucket account let the repository
	administrators know the email address associated with your account.

	You will receive an invitation email. Please accept this invitation, and
	create a bitbucket account as prompted if necessary.

	To gain access to the NECI repository, an ssh key is required. This can
	be generated on any linux machine using the command\footnote{%
		\code{ssh-keygen} can also generate DSA keys. Some ssh clients and
		servers will reject DSA keys longer than 1024 bits, and 1024 bits is
		currently on the margin of being crackable. As such 2048 bit RSA keys
		are preferred. Top secret this code is. Probably. Apart from the master branch which hosted for all on github. And in molpro.
		And anyone that wants it obviously.
	}
	\begin{lstlisting}[gobble=4]
		ssh-keygen -t rsa -b 2048
	\end{lstlisting}%
	This will create a private (\code{~/.ssh/id_rsa}) and a public key
	file (\code{~/.ssh/id_rsa.pub}).
	
	The private key must be kept private. On the bitbucket homepage, go to
	account settings (accessible from the top-right	of the main page), and
	navigate to ``SSH keys''. Click ``Add key'' and add the contents of the
	public key. This will give you access to the repository.

	You can now clone the code into a new directory using the command
	\begin{lstlisting}[gobble=4]
		git clone git@bitbucket.org:neci_developers/neci.git [target_dir]
	\end{lstlisting}

\subsection{Required libraries}
	NECI requires some external software and library support to operate:
	\begin{description}
		\headitem{MPI}
			For builds of NECI intended to be run in parallel, an
			implementation of MPI is required. NECI has been heavily tested
			with OpenMPI, and MPICH2 and its derivatives (IBM MPI, Cray MPI,
			and Intel MPI).
		\headitem{Linear algebra}
			NECI makes use of the linear algebra routines normally contained in
			BLAS/LAPACK. There are a number of different packages which provide
			these routines, and are optimised for different compilers and
			platforms. NECI has been built and tested with either the AMD Core
			Math Library (ACML), the Intel Math Kernel Library (MKL), or the
			more general Basic Linear Algebra Subprograms (BLAS)/Linear Algebra
			Package (LAPACK) combination.
		\headitem{HDF5}
			To make use of the structured HDF5 format for reading/writing
			POPSFILES (files storing the population of walkers, and other
			information, to restart calculations). This library should be
			built with MPI and fortran support
			(\code{--enable-parallel} \code{--enable-fortran}\
			\code{--enable-fortran2003}).
		\headitem{FFTW}
			A small number of options in NECI, which are not enabled by
			default, require Fast Fourier Transforms. If these are re-enabled
			then the Fastest Fourier Transform in the West (FFTW3) library
			is required.
	\end{description}
	If combinations of these choices are made other than those most commonly
	used then either the configuration files, or the resultant Makefile, will
	need to be modified.
	
	On the majority of machines available to the Alavi group and department,
	the compilation environment is managed using the
	\code[language=bash]{module} command. Documentation for that command
	is available from the command \code[language=bash]{module help}. The
	most commonly used command is to load a module, using the command
	\begin{lstlisting}[gobble=4,language=bash]
		module load <module_name>
	\end{lstlisting}
	Installing and configuring the module system on private machines is far
	beyond the scope of this document. Configuring your user account to use
	modules may required modifications to your \code{.bashrc} file,
	depending on the local machine configuration. Please contact the local
	IT administrators or Simon Smart for further advice.

	A number of standard combinations of modules present themselves. Where an
	asterisk is presented, any version of the module can be used. Where the
	version is under specified, the latest module should be used. This will
	occur by default.
	\begin{description}
		\headitem{gfortran (Cambridge)}
			\code{mpi/mpich2/gnu/1.4.1p1 acml/64/gfortran/*/up}
		\headitem{ifort (Cambridge)}
			\code{ifort/64 mpi/openmpi/64/intel12 acml/64/ifort/*/up}
		\headitem{PGI (Cambridge)}
			\code{pgi/64 mpi/openmpi/64/pgi12 lapack/64/pgi} \linebreak
			The lapack module might not be available to all users. Please
			contact Simon Smart if required.
		\headitem{ifort (Max Planck FKF)}
			\code{ifort mpi.intel}\linebreak
			Note that the MKL library is included in the ifort module.
		\headitem{ifort (hydra)}
			\code{git intel mkl mpi.ibm hdf5-mpi cmake}
	\end{description}

\subsection{Building the code (using CMake)}
	There are two ways of building NECI. The recommended approach is to use
	the cmake build system. For legacy purposes, and for more explicitly
	customised build configurations, the older Makefile system may also
	be used.

	CMake allows building the code in a separate directory. One directory
	should be used per configuration that is to be built. This module can
	be a subdirectory of the NECI directory, or otherwise.
	With the \code{build} directory as the current working directory,
	execute
	\begin{lstlisting}[gobble=4]
	cmake [-DCMAKE_BUILD_TYPE=<type>] <path_to_neci>
	\end{lstlisting}
	pointing CMake at the root directory of the cloned NECI repository.

	At this point CMake will automatically configure NECI according to the
	currently loaded modules, and available compilers and libraries. If a
	different set of modules or compilers are to be used a fresh directory
	should be initialised (or equivalently all contents of the directory
	deleted).

	By default CMake will configure a Release build. If this is not desired
	an alternative build type may be specified by setting the
	\code{CMAKE_BUILD_TYPE} above. The available options are \code{Debug} (no
	optimisations, all checking enabled), \code{Release} (optimisations
	enabled), \code{RelWithDebInfo} (same as release, but with debugging
	symbols retained) or \code{Cluster} (inter-procedural optimisations
	enabled, with very long compile times).

	\begin{mdframed}[ %
		linewidth = 2pt, %
		linecolor = red, %
		roundcorner = 6pt, %
		leftmargin = 10, %
		rightmargin = 10, %
		backgroundcolor = gray!20
	]
	If there is an available HDF5 library, which is compiled with support for
	MPI and for the Fortran compiler in use, then CMake will happily make use
	of it. Otherwise support for HDF5 POPSFILES will be disabled by default.

	The CMake configuration for NECI contains the functionality to download,
	compile and use HDF5. To do this run CMake with the
    \code{-DENABLE_BUILD_HDF5=ON}.
	\end{mdframed}

	The code is then built using the command
	\begin{lstlisting}[gobble=4]
		make [-j [n]] [neci|kneci|dneci|mneci]
	\end{lstlisting}
	The optional flag \code{-j} specifies that the build should be
	performed in parallel (with up to an optional number, \code{n},
	threads).

	The final argument specifies whether the normal code (\code{neci})
	should be built, the complex code (\code{kneci}) the double run
	code (\code{dneci}) or the multi run code (\code{mneci}) are
	built. If not specified, all targets are built.

\subsubsection{Options}
    Whilst every effort has been made to provide NECI with sensible default options, the user
    may wish to play around further. To (dis)able an option, the following should be passed as
    an argument to cmake:
    \begin{lstlisting}[gobble=4]
        -DENABLE_<option>=<(ON|OFF)>
    \end{lstlisting}
    The following options are available. Where an option is default "on", if the required libraries
    are not available, the option will be disabled and this will be noted in the build summary.
    \begin{description}
        \codeitem{BUILD\_HDF5}
            Build the hdf5 library from source, and use that instead of one provided by the system.
        \codeitem{HDF5}
            Make use of hdf5 for popsfiles (default=on).
        \codeitem{FFTW}
            Functionality requiring FFTW (default=on).
        \codeitem{MOLCAS}
            Build with the \_MOLCAS\_ flag (default=off).
        \codeitem{MPI}
            Build with parallel functionality (default=off).
        \codeitem{SHARED\_MEMORY}
            Use shared memory for storing integrals (default=on).
        \codeitem{WARNINGS}
            Compile with verbose compiler warnings (default=on).
    \end{description}

\subsection{Overriding configuration options}

    One of the aims of the CMake tool is to make build configuration as black-box as possible.
    The build system should normally detect the compilers in use automatically. The detected compilers
    may not, however, be the ones desired, or the build system may fail to find functionality that
    exists. The complier to use can be overridden by arguments passed to CMake:
    \begin{lstlisting}[gobble=4]
        cmake -DCMAKE_<lang>_COMPILER=XXX <neci_dir>
    \end{lstlisting}
    where \code{<lang>} may be \code{Fortran}, \code{CXX} or \code{C} as appropriate. This should
    specify the command to use which may be a compiler available in the environmental \code{PATH},
    or and absolute path to the compiler to use.

    Once CMake has determined the compiler to use, it determines the compilation and linker flags
    automatically. A number of overrides have been definied for NECI. These may be found in the
    \code{cmake/compiler_flags} director, and are segregated by files named according to both the
    compiler vendor and the language.

    Compiler flags are added in a specific order. The flags defined in the \code{NECI_<lang>_FLAGS}
    variable are applied to all builds, whereas those in \code{NECI_<lang>_FLAGS_<type>} are only
    applied to builds with the appropriate build type (with the exception of when type is equal to
    \code{CLUSTER} when the flags are appended to those used in \code{RELEASE} mode to enable
    extra inter-file optimisations).

    There are also flags to control how things are linked, what options are passed to the compiler
    to enable compiler warnings, and flags that depend on 32 or 64 bit builds. These should be
    self explanatory from reading the files in \code{cmake/compiler_flags}.

    If these defaults are insufficient, the compilation flags may also be overridden. Any arguments
    passed to cmake of the form
    \begin{lstlisting}[gobble=4]
        cmake -DFORCE_<lang>_FLAGS[_<type>]
    \end{lstlisting}
    override the corresponding \code{NECI_<lang>_FLAGS[_<type>]} flags. Essentially any
    \code{NECI_*} flag may be overridden on the command line with a \code{FORCE_*} flag (although
    it is possible that some of these have been accidentally ommitted from the implmentation).

\subsubsection{Toolchain files}

    Overriding all of the CMake variables on the command line is cumbersome and error prone.
    Various sets of overrides can be combined into a toolchain file, which can be passed
    to CMake:
    \begin{lstlisting}[gobble=4]
        cmake -DCMAKE_TOOLCHAIN_FILE=<toolchain_file> <neci_dir>
    \end{lstlisting}
    These toolchain files can specify the entire chain of compilers, flags and libraries if
    desired. For examples see the toolchains/ directory in the NECI repository.

    Additionally to the flags described above, the \code{CMakeForceCompiler} functionality may
    be used (over and above just setting the \code{CMAKE_<lang>_COMPILER} variable).
    These macros entirely disable the autodetection of compiler properties within
    CMake (per language), and will require all flags that are not in the
    \code{cmake/compiler_flags} directories to be specified manually. As an example:
    \begin{lstlisting}[gobble=4]
        include(CMakeForceCompiler)

        CMAKE_FORCE_C_COMPILER       ( gcc GNU )
        CMAKE_FORCE_CXX_COMPILER     ( g++ GNU )
        CMAKE_FORCE_Fortran_COMPILER ( mpif90 GNU )
    \end{lstlisting}
    This will force the use of the commands \code{gcc}, \code{g++}, and \code{mpif90}, and will
    set the \code[breaklines=true]{CMAKE_<lang>_COMPILER_ID} variable to \code{GNU} such that
    the compiler flags set in the \code[breaklines=true]{cmake/compiler_flags/GNU_*.cmake}
    files are used. Because this turns off
    any autodetection, CMake will not autodetect that the c++ standard library needs to be
    linked in to combine c++ and Fortran files. This will need to be corrected manually, using:
    \begin{lstlisting}[gobble=4]
        set( NECI_Fortran_STATIC_LINK_LIBRARIES stdc++ )
    \end{lstlisting}
    Further internally required libraries may be required. In a normal build, these are
    output in the build summary under "Implicit C++ linker flags".
    A comprehensive documented example is found in \code{toolchains/gfortran-openmpi.cmake}.

    \textbf{Archer}

    As an example, we can consider the Archer supercomputer, located at EPCC in Edinburgh. This
    machine uses compiler wrapper scripts (written by Cray) to provide much of the functionality
    automatically, but this defeats the CMake auto-configuration system. To build on archer the
    cmake command
    \begin{lstlisting}[gobble=4]
        cmake -DCMAKE_TOOLCHAIN_FILE=<neci_dir>/toolchains/archer.cmake <neci_dir>
    \end{lstlisting}

    (As we already know about Archer, we autodetect that you are running on it, and CMake will
    fail with a message containing these instructions).
	used, then the build will fail.

\subsubsection{Compilation on ADA}
	Begin by clearing out the build environment. At the terminal execute:
	\begin{lstlisting}[gobble=4]
		module purge
		module load environments/addons/cmake-2.8.2
	\end{lstlisting}
	Next, for GNU:
	\begin{lstlisting}[gobble=4]
		module load environments/programming/gcc-4.8.2
	\end{lstlisting}
	Or Intel:
	\begin{lstlisting}[gobble=4]
		module load compilers/intel/15.0.0.090
		module load mpi/openmpi/1.8.2/intel15.0-threads
	\end{lstlisting}
	then run Cmake as normal.

\subsubsection{Overriding packages required for options}

    There are a number of packages that are required to build NECI (such as something providing
    a LAPACK-like interface), or which are required to enable certain options (librt is required
    to enable shared memory).

    If the package searching fails, there are a number of variables that can be set on the CMake
    command line, or in a toolchain file as appropriate:

    \begin{description}
        \codeitem{NECI\_FIND\_<package>}
            If this is set to OFF, the package searcher will not be executed, and the associated
            option will be enabled without disabling the option.
        \codeitem{<package>\_FOUND}
            For many of the package searchers, this disables the searching from inside the package
            rather than outside. In general, the first option is preferred.
        \codeitem{<package>\_LIBRARIES}
            The libraries to be linked in for use of the specifide package. If package searching is
            disabled, then to use the package this needs to be filled in explicitly.
        \codeitem{<package>\_DEFINITIONS}
            Any additional compiler flags that are required to use the package.
        \codeitem{<package>\_INCLUDE\_PATH}
            The location of any C or C++ header files, or fortran module files to be used during
            compilation
    \end{description}

    NECI has a some special package finders, called \code{MPI_NECI}, \code{LAPACK_NECI} and
    \code{HDF5_NECI}. To a large
    extent they are just wrappers around the underlying finders provided with CMake, but they
    implement some additional logic, such as automatically substituting MKL for LAPACK, checking
    the MPI compiler being used, and providing the capacity to build hdf5 in the source tree.

    As a result, when overriding these packages, \code{MPI_NECI}, \code{LAPACK_NECI} and
    \code{HDF5_NECI} should be substituted for \code{<package>} above.

\subsection{Configuring builds (Makefile system)}
	A specific configuration for building NECI is initialised by using the
	command
	\begin{lstlisting}[gobble=4]
		./tools/mkconfig.py config_name [-g]
	\end{lstlisting}
	The configuration names correspond to the configuration files contained in
	the config directory. If the flag \code{-g} is used, then a debug
	configuration will be created, and otherwise an optimised one.
	
	There are a number of different configurations for
	differing systems, library and compiler setups. The following are some of
	the more important, and most likely to be used.
	\begin{description}
		\codeitem{gfortran\_simple, ifort\_simple}
			These are the basic configurations, set up for the gfortran and
			ifort compilers. For development these are the most likely
			configurations to be used. On personal machines the easiest
			environment to install is gfortran and openMPI, using the
			\code{gfortran\_simple} - see note regarding libraries below.
		\codeitem{fkf\_ifort}
			The MPI and ifort installation at the FKF in Stuttgart is different
			to that available in most locations. Use this config file to
			compile there.
		\codeitem{PC-ifort64-MPI-TARDIS, PC-ifort64-MPI-HYDRA}
			The ifort compiler supports additional (expensive) optimisations
			during the link stage. For production runs on supercomputers these
			should be used. The -TARDIS config file is the normal one to use,
			with the -HYDRA for the differing environment available on HYDRA.
	\end{description}
	To specify a default configuration file to use on a particular machine,
	create a symbolic link called \code{.default} in the config directory
	to the appropriate configuration file. This allows \code{mkconfig.py}
	to be usedwithout specifying the configuration name.

	To compile NECI, a number of linear algebra routines are required. The
	relevant routines are available in the AMD Core Math Library (ACML), the
	Intel Math Kernel Library (MKL) and in the more widely available
	combination of Basic Linear Algebra Subprograms (BLAS) and the Linear
	Algebra Package (LAPACK). The configuration files above make some
	assumptions about which packages are available, which are not always
	correct.

	In particular, the elements of the linker lines
	\begin{lstlisting}[gobble=4]
		-lacml
		-lmkl_intel_ilp64 -lmkl_core -lmkl_sequential
		-lblas -llapack
	\end{lstlisting}
	are in principle interchangeable. On personal development machines it is
	easiest to install BLAS and LAPACK, but these are generally less
	performant so are not used by default. These lines can be substituted in
	the generated \code{Makefile} before compilation.

\subsection{Building the code (Makefile system)}
	The code is built using the command
	\begin{lstlisting}[gobble=4]
		make [-j [n]] [neci.x|kneci.x|dneci.x|mneci.x|both|all]
	\end{lstlisting}
	The optional flag \code{-j} specifies that the build should be
	performed in parallel (with up to an optional number, \code{n},
	threads).

	The final argument specifies whether the normal code (\code{neci.x})
	should be built, the complex code (\code{kneci.x}), the double run
	code (\code{dneci.x}), the multiple run code (\code{mneci.x}) or
	both the normal and complex codes (\code{both}) or all of the above
	and various extra utilities
	(\code{all}).
	
\subsection{Git overview}
	It is essential if you plan to do developmental work to get familiar with the source-code management software `git'. The code will get unusable exponentially quickly if all development and new ideas are hacked into the master branch of the code. The nature of research is that most things probably won't work, but you want to implement them and test relatively quickly, without requiring a standard of code that will remain usable in perpetuity. To avoid an inexorable increase in code `clutter', it is essential to work in `branches' off the main code. For a more detailed introduction to the git package, see \url{git-scm.com/book/en/v2/getting-started-git-basics}. In short, the workflow should be:
	\begin{enumerate}
	\item Branch off a clean master version to implement something
	\item Test and develop in the branch
	\item Regularly merge the new code from the master branch into your personal development branch
	\item Once satisfied with the development, and that it is an improvement in scope or efficiency of the existing code, ensure it is tidy, commented, documented, as bug-free as possible, and tests added to the test suite for it. This may involve reimplementing it from a clean version of master if it can be done more efficiently
	\item Merge code back into master branch
	\end{enumerate} 
	
	A few potentially useful git commands in roughly the workflow described above:
	\begin{description}
	\codeitem{git branch}
	See what branch I am on. -a flag for all (inc. remote) branches.
	\codeitem{git pull origin master}
	Update the master branch into the current local repository
	\codeitem{git checkout -b newbranchname}
	Fork off current branch to a new branch called `newbranchname'
	\codeitem{git commit -a -m `Commit message'}
	Commit a set of changes for the current branch to your local repository.
	\codeitem{git push origin branchname}
		Push your current local branch called branchname to a new remote
		branch of the same name to allow access to others and secure storage
		of the work
	\codeitem{git checkout -b newbranchname --track origin/remotebranch}
		Check out a branch stored on the remote repository, and allow pushing
		and pulling from the remote repository for that branch.
	\codeitem{git push}
		Push the current branch to the remote branch that it is tracking.
	\codeitem{git merge master}
		Merge the recent changes in master into your local branch (requires a
		pull first)
	\codeitem{git checkout master}
	Switch branches to the master branch
	\codeitem{git merge newbranch}
	Merge your code in `newbranch' into your current branch (potentially master)
	\end{description}

    Each commit should contain one logical idea and the commit message should
    clearly describe \emph{everything} that is done in that commit. It is fine
    for one commit to only contain a very minor change. Try and commit regularly
    and avoid large commits. It is also a good idea to make sure that code
    compiles before commiting. This helps catch errors that you may be
    introducing and also allows the use of debugging tools such as git bisect.
	
	It should be noted that the `stable' branch of the code, automatically
	merged into from master upon successful completion of nightly tests, is
	hosted on github on a public repository, and also pushed to the molpro
	source code. The molpro developers will quickly send us angry emails if
	poor code gets pushed into it from NECI, and I will be sure to forward
	complaints onto the relevant parties!

\section{Calculation inputs}

\begin{lstlisting}
title

system read
freeformat
electrons 10
(nonuniformrandexcits 4ind-weighted)
(hphf 0)
(molpromimic)
endsys

calc

methods
method vertex fcimc
endmethods

time 2000.0
memoryfacspawn 1.5
memoryfacpart 0.3
totalwalkers 100000
(tau 0.001)
(tau 0.001 search)
(max-tau 0.01)
startsinglepart 50
shiftdamp 0.02
stepsshift 10
diagshift 0.1
truncinitiator
addtoinitiator 3
allrealcoeff
(realcoeffbyexcitlevel 4)
realspawncutoff 0.01
semi-stochastic 500
(doubles-core)
(pops-core 1000)
trial-wavefunction 500
(doubles-trial)
(pops-trial 1000)
(readpops)
(readpops-changeref)
jump-shift
proje-changeref 1.5
maxwalkerbloom 1
(nmcyc 1000000) Necessary with molpromimic...
(definedet 1 2 3 4 5 6 7 8 9)
endcalc

integral
(freeze 2, 0)
(partiallyfreeze 6 2)
(partiallyfreezevirt 10 2)
endint

logging
popsfile
(popsfile -1)
popsfiletimer 10.0
incrementpops
binarypops
hdf5-pops
(calcrdmonfly ...)
(fcimcdebug 5)
write-core
endlog
end
\end{lstlisting}

\section{Useful References Containing Technical Details}

Original FCIQMC method:
\begin{itemize}
\item Fermion Monte Carlo without fixed nodes: a game of life, death, and annihilation in Slater determinant space. \newline
GH Booth, AJ Thom, A Alavi – The Journal of chemical physics (2009) 131, 054106
\end{itemize}

Quite a bit of symmetries some stuff on the initiator method that is actually implemented:
\begin{itemize}
\item Breaking the carbon dimer: the challenges of multiple bond dissociation with full configuration interaction quantum Monte Carlo methods.
GH Booth, D Cleland, AJ Thom, A Alavi – The Journal of chemical physics (2011) 135, 084104
\end{itemize}

Linear scaling algorithm, (uniform) excitation generation and overall algorithm of FCIQMC:
\begin{itemize}
\item Linear-scaling and parallelisable algorithms for stochastic quantum chemistry
GH Booth, SD Smart, A Alavi – Molecular Physics (2014) 112, 1855
\end{itemize}

Density matrices, real walker weights and sampling bias:
\begin{itemize}
\item Unbiased Reduced Density Matrices and Electronic Properties from Full Configuration Interaction Quantum Monte Carlo.
Catherine Overy, George H. Booth, N. S. Blunt, James Shepherd, Deidre Cleland, Ali Alavi, http://arxiv.org/abs/1410.6047
\end{itemize}

KP-FCIQMC:
\begin{itemize}
\item Krylov-projected quantum Monte Carlo
N. S. Blunt, Ali Alavi, George H. Booth, http://arxiv.org/abs/1409.2420
\end{itemize}

\section{Real walker coefficients}

    Real walker coefficients can be turned on in NECI in one of two ways. The
    recommended method is to use the `RealCoeffByExcitLevel' option. If one adds
    `RealCoeffByExcitLevel 4' to the Calc block then real coefficients will be
    used for all determinants up to 4 excitations of the reference determinant.

    The following options relate to real coefficients, and all should be
    inserted in the Calc block.
    \begin{description}
		\codeitem{RealCoeffByExcitLevel n}
            Use real walkers on all determinants which are up to n excitations
            of the reference determinant.
		\codeitem{AllRealCoeff}
            Use real walkers for all determinants.
		\codeitem{RealSpawnCutoff x}
            Continuous real spawning will be performed, unless the spawn has
            weight less than x. In this case, the weight of the spawning will
            be stochastically rounded up to x or down to zero, such that the
            average weight of the spawning does not change. This is a method of
            removing very low weighted spawnings from the spawned list, which
            require extra memory, processing and communication. A reasonable
            value for x is 0.01.
		\codeitem{SetOccupiedThresh x}
            Set the value for the minimum walker weight in the main walker
            list. If, after all annihilation has been performed, any
            determinants have a total weight of less than x, then the weight
            will be stochastically rounded up to x or down to zero such that
            the average weight is unchanged.

            Default: x=1.0. This value should usually be used, and should only
            be changed if you have a good reason.
    \end{description}

\section{Semi-stochastic}

    To use semi-stochastic in NECI, the \code{semi-stochastic} option should
    be added to the Calc block, along with a further option to specify what
    core space to use. The following options are available and should be added
    to the Calc block.
    \begin{description}
		\codeitem{semi-stochastic}
            Turn on the semi-stochastic adaptation.
		\codeitem{doubles-core}
            Use the reference determinant and all single and double excitations
            from it to form the core space.
		\codeitem{cas-core cas1 cas2}
            Use a CAS space to form the core space. The parameter cas1 specifies
            the number of electrons in the cas space and cas2 specifies the
            number of virtual spin orbitals (the cas2 highest energy orbitals
            will be virtuals).
		\codeitem{ras-core ras1 ras2 ras3 ras4 ras5}
            Use a RAS space to form the core space. Suppoose the list of spatial
            orbitals are split into three sets, RAS1, RAS2 and RAS 3, ordered
            by their energy. ras1, ras2 and ras3 then specify the number of
            spatial orbitals in RAS1, RAS2 and RAS3. ras4 specifies the minimum
            number of electrons in RAS1 orbitals. ras5 specifies the maximum
            number of electrons in RAS3 orbitals. These together define the RAS
            space used to form the core space.
		\codeitem{optimised-core}
            Use the iterative approach of Petruzielo \emph{et al.} (see PRL,
            109, 230201). One also needs to use either optimised-core-cutoff-amp
            or optimised-core-cutoff-num with this option.
		\codeitem{optimised-core-cutoff-amp $x1$, $x2$, $x3$...}
            Perform the optimised core option, and in iteration $i$, choose
            which determinants to keep by choosing all determinants with an
            amplitude greater than $xi$ in the ground state of the space (see
            PRL 109, 230201). The number of iterations is determined by the
            number of parameters provided. 
		\codeitem{optimised-core-cutoff-num $n1$, $n2$, $n3$...}
            Perform the optimised core option, and in iteration $i$, choose
            which determinants to keep by choosing the ni most significant
            determinants in the ground state of the space (see PRL 109, 230201).
            The number of iterations is determined by the number of parameters
            provided. 
		\codeitem{fci-core}
            Use all determinants to form the core space. A fully deterministic
            projection is therefore performed with this option.
		\codeitem{pops-core $n$}
            When starting from a POPSFILE, this option will use the $n$ most
            populated determinants from the popsfile to form the core space.
		\codeitem{read-core}
            Use the determinants in the CORESPACE file to form the core space.
            A CORESPACE file can be created by using the write-core option in
            the Logging block.
    \end{description}
    The following semi-stochastic related options should be added to the
    Logging block.
    \begin{description}
		\codeitem{write-core}
            When performing a semi-stochastic calculation, adding this option
            to the Logging block will cause the core space determinants to be
            written to a file called CORESPACE. These can then be further read
            in and used in subsequent semi-stochastic options using the
            read-core option in the Calc block.
		\codeitem{write-most-pop-core-end $n$}
            At the end of a calculation, output the $n$ most populated
            determinants to a file called CORESPACE. This can further be read
            in and used as the core space in subsequent calculations using the
            read-core option.
    \end{description}

\section{Trial wave functions}

    By default, NECI uses a single reference determinant, $|D_0\rangle$, in the
    projected energy estimator, or potentially a linear combination of two 
    determinants if the the HPHF code is being used.
    \begin{equation}
        E_0 = \frac{\braket{D_0 | \hat{H} | \Psi}}{\braket{D_0 | \Psi}}.
    \end{equation}
    This estimator can be improved by using a more accurate estimate of the
    true ground state, a trial wave function, $\ket{\Psi^T}$,
    \begin{equation}
        E_0 = \frac{\braket{\Psi^T | \hat{H} | \Psi}}{\braket{\Psi^T | \Psi}}.
    \end{equation}
    Such a trial wave function can be used in NECI using by adding the
    \code{trial-wavefunction} option to the Calc block.
    You must also specify a
    trial space. The trial wave function used will be the ground state of the
    Hamiltonian projected into this trial space.

    The trial spaces available are the same as the core spaces available for
    the semi-stochastic option. However, you must replace \code{core} with
    \code{trial}. For
    example, to use all single and double excitations of the reference
    determinant, one should use the `doubles-trial' option.

\section{Sampling excited states with FCIQMC}

    As well as sampling the ground state, NECI can be used to estimate
    excited-state properties using an orthogonalisation procedure.
    Specifically, by performing $m$ FCIQMC simulations simultaneously,
    the lowest $m$ energy states can be sampled.

    To do this, one must use the mneci compilation. Using dneci is not
    sufficient.

    To specify how many states are to be sampled, one should use the
    \code{system-replicas} option in the System block of the input file.

    Then, the \code{orthogonalise-replicas} option should be included in
    the Calc section of the input file. This will tell NECI to orthogonalise
    the FCIQMC wave functions against each other. States representing
    high-energy wave functions will be orthogonalised against those
    representing low-energy wave functions. This prevents higher-energy
    states being projected to the ground state, and instead allows
    excited states to be converged upon.

    Also, one must tell NECI how to initialise each FCIQMC wave function.
    It is a bad idea to start from single determinants, as many of these
    will be poor estimates to the desired excited states, and so
    convergence will be slow. Instead, one should start from trial estimates
    to the desired excited states. These trial states are generated by
    calculating the lowest-energy states within a subspace.
    Thus, one must simply tell NECI what subspace to use. The options
    available are the same as for semi-stochastic and trial spaces (see above).

    For example, if one wants to initialise from the lowest-energy CISD states,
    one should use the \code{doubles-init} option in the Calc block. If one
    wants to start from the lowest-energy states in a $(10,10)$ CAS space, you
    should put \code{cas-init 10 10} in the Calc block.

    Also, single-determinant energy estimators can be give poor results for
    excited states. Instead, trial wave function-based estimators should be
    used. This should be done exactly as for the ground state -- see above for
    more details on this. For example, to use CISD wave functions in the trial
    energy estimators, include both the \code{trial-wavefunction} and
    \code{doubles-trial} options in the Calc block of the input file.

    For some example excited-state calculations with NECI, see the
    test\_suite/mneci/excited\_state directory and the tests therein.

\section{Davidson RAS code}

    NECI has an option to find the ground state of a RAS space using a
    direct CI davidson approach, which does not require the Hamiltonian to be
    stored. This code is particularly efficient for FCI and CAS spaces, but is
    less efficient for CI spaces.

    To perform a davidson calculation, put
    \begin{lstlisting}[gobble=4]
    	davidson ras1 ras2 ras3 ras4 ras5
    \end{lstlisting}
    in the Methods block, inside the Calc block. The parameters ras1-ras5 define
    the RAS space that will be used. These are defined as follows. First,
    split all of the spatial orbitals into theree sets, RAS1, RAS2 and RAS3,
    so that RAS1 contains the lowest energy orbitals, and RAS3 the highest.
    Then, ras1, ras2 and ras3 define the the number of spatial orbitals in
    RAS1, RAS2 and RAS3. ras4 defines the minimum number of electrons in RAS1.
    ras4 defines the maximum number of electrons in RAS3. These 5 parameters
    define the ras space.

    This method will allocate space for up to 25 Krylov vectors. It will iterate
    until the norm of the residual vector is less than $10^{-7}$. If this is
    not achieved in 25 iterations, the calculation will simply stop and output
    whatever the current best estimate at the ground state is.

    This code should be able to perform FCI or CAS calculations for spaces up
    to around $5\times10^6$ or so, but will probably struggle for spaces much
    larger than this.

    The method has only been implemented with RHF calculations and with $M_s=0$.

\section{KP-FCIQMC}

    The Krylov-projected FCIQMC (KPFCIQMC) method is available on the master
    branch of NECI. The theory behind the method is described in the article,
    currently available on arXiv (arXiv:1409.2420). Here we simply describe the
    main input options available.

    KP-FCIQMC is accessed via its own block in the input file. This block is
    started with 'kp-fciqmc' and ended with 'end-kp-fciqmc'. The methods block
    should be removed.

    KP-FCIQMC is only implemented with the linear scaling algorithm. It should
    be run in double-run mode.

    The main options are:
    \begin{description}
        \codeitem{num-krylov-vecs $N$}
            $N$ specifies the total number of Krylov vectors to sample.
        \codeitem{num-iters-between-vecs $N$}
            $N$ specifies the (constant) number of iterations between each Krylov
            vector sampled. The first Krylov vector is always the starting wave
            function.
        \codeitem{num-iters-between-vecs-vary $i_{12}$, $i_{23}$, $i_{34}$...}
            $i_{n,n+1}$ specifies the number of iterations between the nth and
            (n+1)th Krylov vectors. The number of parameters input should be
            the number of Krylov vectors asked for minus one. The first Krylov
            vector is always the starting wave function.
        \codeitem{num-repeats-per-init-config $N$}
            $N$ specifies the number repeats to perform for each initial
            configuration, i.e. the number of repeats of the whole evolution,
            from the first sampled Krylov vector to the last. The projected
            Hamiltonian and overlap matrix estimates will be output for each
            repeat, and the averaged values of these matrices used to compute
            the final results.
        \codeitem{averagemcexcits-hamil $N$}
            When calculating the projected Hamiltonian estimate, an FCIQMC-like
            spawning is used, rather than calculating the elements exactly,
            which would be too computationally expensive. Here, $N$ specifies the
            number of spawnings to perform from each walker from each Krylov
            vector when calculating this estimate. Thus, increasing $N$ should
            improve the quality of the Hamiltonian estimate.
        \codeitem{finite-temperature}
            If this option is included then a finite-temperature calculation is
            performed. This involves starting from several different random
            configurations, whereby walkers are distributed on random
            determinants. The number of initial configurations should be
            specified with the num-init-configs option.
        \codeitem{num-init-configs $N$}
            $N$ specifies the number of initial configurations to perform the
            sampling over. An entire FCIQMC calculation will be performed, and
            an entire subspace generated, for each of these configurations.
            This option should be used with the finite-temperature option, but
            is not necessary for spectral calculations where one always starts
            from the same initial vector.
        \codeitem{memory-factor $x$}
            This option is used to specify the size of the array allocated for
            storing the Krylov vectors. The number of slots allocated to store
            unique determinants in the array holding all Krylov vectors will be
            equal to $ABx$, where here $A$ is the length of the main walker
            list, $B$ is the number of Krylov vectors, and $x$ is the value input
            with this option.
        \codeitem{num-walker-per-site-init $x$}
            For finite-temperature jobs, $x$ specifies the number of walkers to
            place on a determinant when it is chosen to be occupied.
        \codeitem{exact-hamil}
            If this option is specified then the projected Hamiltonian will
            be calculated exactly for each set of Krylov vectors sampled,
            rather than randomly sampling the elements via an FCIQMC-like
            spawning dynamic.
        \codeitem{fully-stochastic-hamil}
            If this option is specified then the projected Hamiltonian will be
            estimated without using the semi-stochastic adaptation. This will
            decrease the quality of the estimate, but may be useful for
            debugging or analysis of the method.
        \codeitem{init-correct-walker-pop}
            For finite-temperature calculations on multiple cores, the initial
            population may not be quite as requested. This is because the
            quickest (and default) method involves generating determinants
            randomly and sending them to the correct processor at the end. It
            is possible in this process that walkers will die in annihilation.
            However, if this option is specified then each processor will throw
            away spawns to other processors, thus allowing the correct total
            number of walkers to be spawned.
        \codeitem{init-config-seeds seed1, seed2...}
            If this option is used then, for finite-temperature calculations,
            at the start of each calculation over an initial configuration,
            the random number generator will be re-initialised with the
            corresponding input seed. The number of seeds provided should be
            equal to the number of initial configurations.
        \codeitem{all-sym-sectors}
            If this option is specified then the FCIQMC calculation will be
            run in all symmetry sectors simultaneously. This is an option
            relevant for finite-temperature calculations.
        \codeitem{scale-population}
            If this option is specified then the initial population will be
            scaled to the populaton specified with the `totalwalkers' option
            in the Calc block. This is relevant for spectral calculations when
            starting from a perturbed \code{POPSFILE} wave function, where the
            initial population is not easily controlled.
    \end{description}

    In spectral calculations, one also typically wants to consider a particular
    perturbation operator acting on the ground state wave functions. Therefore,
    you must first perform an FCIQMC calculation to evolve to the ground state
    and output a \code{POPSFILE}. You should then start the KP-FCIQMC
    calculation from that \code{POPSFILE}. To apply a perturbation operator to
    the \code{POPSFILE} wave function as it is read in, use the pops-creation
    and pops-annihilate options. These allow operators such as
    \begin{equation}
        \hat{V} = \hat{c}_i \hat{c}_j + \hat{c}_k \hat{c}_l \hat{c}_m
    \end{equation}
    to be applied to the \code{POPSFILE} wave function. The general form is
    \code{pops-annihilate n_sum}
    \code{orb1 orb2...}
    \code{...}
    where $n_{sum}$ is the number of terms in the sum for $\hat{V}$ (2 in the
    above example), and $orbi$ specify the spin orbital labels to apply. The
    number of lines of such orbitals provided should be equal to $n_{sum}$. The
    first line provides the orbital labels for the first term in the sum, the
    second line for the second term, etc...
    
\section{RDM generation}

Currently the 2-RDMs can only be calculated for closed shell systems.  However, calculation and
diagonalisation of only the 1-RDM is set up for either open shell or closed shell systems.

The original theory behind the calculation of the RDMs (including details of parallelisation) can be found in the paper:
\url{http://arxiv.org/abs/1410.6047}. The most accurate RDM method (which is also unbiased) is the double-run approach, which requires
the code to be compiled with the \code{-D__DOUBLERUN} flag in \code{CPPFLAGS} in the Makefile. This propagates
two completely independent populations of walkers, and calculates an unbiased RDM by taking cross-terms
between the two populations.

The calculation of the diagonal elements is done by keeping track of the average walker populations of each
occupied determinant, and how long it has been occupied.  The diagonal element from Di is then calculated
as <Ni>(pop1) x <Ni>(pop2) x [No. of iterations in this block], and this is included every time we start a new
averaging block, which can occur when a determinant becomes unoccupied in either population, or when we require
the calculation of the RDM energy during the simulation. As such, the exact RDM accumulated is dependent on
the interval of RDM energy calculations, but in an unbiased way.

The off diagonal elements are sampled through spawning events, and use instantaneous walker populations.
Subtle details in the code are:

1. RDMs take contributions to diagonal elements and HF connections whenever the RDM energy is calculated.
   As the averaging blocks are now reset at this point too, this change is unbiased.
2. The off-diagonal contributions (both HF connections and other contributions sampled through spawning
   events) contain contributions from both cross-terms. I.e $N_i(pop1)*N_j(pop2)$ as well as
   $N_i(pop2)*N_j(pop1)$.
   
There is also the facility to do a single-run calculation of the RDM.  This method is BIASED, so should
not be used for high accuracy calculations.  However, it is cheaper than the double run method, both in
memory and in simulation time, so may be useful for rough-and-ready calculations.  
%With no cutoff applied
%this is very similar to the method described in DMC's thesis, which gives poor RDM energies. Note that the
%main difference is that the SR method used here resets the population averages when the RDM energy is calculated
%rather than just when the determinant becomes unoccupied. Note also that this SR method is only identical to that
%tested in CMO's thesis if the RDM energy is only calculated in the final iteration. Without this condition,
%the current implementation of SR is probably more biased in the diagonal elements than the version tested in CMO thesis,
%as squared terms are added in more regularly. However, dealing with this makes the code very untidy and requires significantly
%more memory.  As the SR method is inherently biased anyway, this is not too great a concern.
Reducing the effect of the bias in the SR method can be done by applying a cutoff to the diagonal contributions, such that
contributions are only added in if the average sign of the determinant at the time of adding in the contribution exceeds
some preset parameter.  (See THRESHOCCONLYRDMDIAG)

To perform a calculation, use
    \begin{lstlisting}[gobble=4]
		calcrdmonfly RDMLevel RDMIterStart RDMEnergyIter
    \end{lstlisting}

    It requires 3 integers.
    The first refers to the type of RDMs to calculate.  A value of 1 will calculate only the 1-RDM.  Any other
    value will calculate the 2-RDM (which contains the information of the 1-RDM).  The second integer
    is the number of iterations after the shift has begun to change that we want to begin filling the RDMs.
    Finally, if the 2-RDMs are being calculated, the RDM energy will be automatically obtained at the
    end of the calculation.  The 3rd integer refers to how often (every RDMEnergyIter iterations) we want
    to additionally calculate and print the energy during the calculation.  This will be ignored if only
    calculating the 1-RDM.
    Clearly making RDMEnergyIter very large will mean the energy is only calculated with a softexit, or this can
    also be achieved by using \code{CalcRdmEnergy OFF}.

    The RDM energy is one measure of the accuracy of the RDMs.  Also printed by default are the maximum error in the
    hermiticity (2-RDM(i,j;a,b) - 2-RDM(a,b;i,j)) and the sum of the absolute errors.

\subsection{Reading in / Writing out the RDMs for restarting calculations}

	Two types of 2-RDMs can be printed out.  The final normalised hermitian 2-RDMs of the form \code{TwoRDM_a***}, or the
	binary files \code{TwoRDM_POPS_a***}, which are the unnormalised RDMs, before hermiticity has been enforced.  The
	first are the \code{2-RDM(i,j;a,b)} matrices, which are printed in
    spatial orbitals with $i<j$, $a<b$ and $i,j<a,b$.  The second are the ones to read back in if a calculation
    is restarted (they are also printed in spatial orbitals with $i<j$ and $a<b$, but for both $i,j,a,b$ and $a,b,i,j$
    because they are not yet hermitian).  These are the matrices exactly as they are at that point in the calculation.
	By default the final normalised 2-RDMs will always be printed, and the \code{TwoRDM_POPS_a***} files are connected to the
    \code{popsfile/binarypops} keywords - i.e. if a wavefunction popsfile is being printed and the RDMs are being filled,
	a RDM \code{POPSFILE} will be also.
	If only the 1-RDM is being calculated, \code{OneRDM_POPS/OneRDM} files will be printed in the same way.
    The following options can override/modify these defaults.

	\begin{description}
		\codeitem{writerdmstoread off}
	The presence of this keyword overrides the default.  If the OFF word is present, the unnormalised \code{TwoRDM_POPS_a***}
    files will definitely not be printed, otherwise they definitely will be, regardless of the state of the
    \code{popsfile/binarypops} keywords.

\codeitem{readrdms}
	This keyword tells the calculation to read in the \code{TwoRDM_POPS_a***} files from a previous calculation.  The
    restarted calc then continues to fill these RDMs from the very first iteration regardless of the value put with
	the \code{calcrdmonfly} keyword.  The calculation will crash if one of the \code{TwoRDM_POPS_a***} files are missing.  If
	the \code{readrdms} keyword is present, but the calc is doing a \code{StartSinglePart} run, the \code{TwoRDM_POPS_a***} files
    will be ignored.

\codeitem{nonormrdms}
	This will prevent the final, normalised \code{TwoRDM_a***} matrices from being printed.  These files can be quite
    large, so if the calculation is definitely not going to be converged, this keyword may be useful.

\codeitem{writerdmsevery iter}
	This will write the normalised \code{TwoRDM_a***} matrices every \code{iter} iterations while the RDMs are being
    filled.  At the moment, this must be a multiple of the frequency with which the energy is calculated.  The
	files will be labelled with incrementing values - \code{TwoRDM_a***.1} is the first, and then next \code{TwoRDM_a***.2} etc.
\end{description}

\section{Outputs of NECI}

TODO

\section{Performing error analysis}
    Data from an FCIQMC calculation is usually correlated. As a result,
    standard error analysis for uncorrelated data cannot be used. Instead we
    perform a so-called blocking analysis (JCP 91, 461). In this, data is
    grouped into blocks of increasing size until the data in subsequent blocks
    becomes uncorrelated, to a good approximation.

    A blocking analysis can be performed in NECI in one of two ways. Firstly,
    a rough blocking analysis is performed automatically after a job is finished.
    The final result is output to standard output and further information about
    the blocking analysis at various block sizes is output to separate files, 
	such as \code{Blocks_num} and \code{Blocks_denom}. This should only be
	used as a rough
    and quick estimate as there are issues with this approach. For example, the
    analysis starts as soon as the shift is turned on. This is before the
    population has stabilised, and so unusual results can occur in the analysis
    of the denominator and numerator. Also, data is not taken from the optimal
    block size.

    A better approach for a more careful analysis is to use the blocking script
    in the utils directory, called blocking.py. The key command is
	\begin{lstlisting}[gobble=4]
		./blocking.py -f start_iter -d24 -d23 -o/ FCIMCStats
    \end{lstlisting}

	This will perform a blocking analysis starting from iteration
	\code{start_iter}.
    The analysis should be started only once the energy estimate, (column 11 in
	\code{FCIMCStats}) and the numerator and denominator (columns 24 and
	25) have
    stabilised and are fluctuating about some final value. Just because the
    energy looks stable, it does not mean that the populations is not still
    growing!

	\code{-d24 -d23'} tells the script to perform the blocking on columns 25 and
	24 of the \code{FCIMCStats} file, which correspond to the numerator and
	denominator of the energy estimator, respectively. \code{-o/} tells the script
    to also provide data for the results of dividing columns 25 and 24, which
    gives the energy estimate that we want.

    Running this will produce a graph of the errors for both the numerator and
    denominator as a function of the number of blocks (and therefore of the
    block size). As the block size increases, the error estimates should
    increase, tending towards the true values. Eventually the estimates will
    plateau. This indicates that, at this block length, the data in the blocks
    are uncorrelated to a good approximation, and the error estimate calculated
    is accurate. The data from this block length should therefore be used.

    Each estimate of the error will also have an error on it. As the block
    length increases this `error on the error' will increase. One should
    therefore use the \emph{first} block length where the plateau is reached,
    so as to minimise the error on the final error estimate.

    If no plateau is seen in the plot then the simulation has not been run for
	long enough, and needs to be continued by restarting from the \code{POPSFILE}.
    It can take on the order of $10^5-10^6$ iterations to perform an accurate
    blocking analysis.

	The \code{blocking.py} script will also output the final estimates on the energy
    at the different block lengths. You should find the blocking length where
    the errors plateau and read of the final estimates (the rightmost columns)
    from here.

	More information (including example plots, similar to those that
	\code{blocking.py} produces) is available at JCP 91, 461.

\section{Anatomy of an FCIQMC calculation}
\section{Useful tools}
\begin{itemize}
	\item Tools for post-processing (e.g. blocking)
	\item Tools for pre-processing (e.g. for Lz pure FCIDUMPS)
	\item Model calculations?
	\item Tools for processing POPSFILES?
\end{itemize}


\chapter{Working in NECI}
In many ways, NECI is a fairly hostile environment to code, especially for
inexperienced software developers, or those who are not familiar with the
ideosynchrasies of different versions of FORTRAN/Fortran.

In the following sections we aim to give some general guidance for working in
the NECI codebase, and 

\textbf{GHB, NSB:} For many of these sections it is arguable whether they
belong in an overall about-the code description, or a walkthrough of the code
itself. Feedback would be good!

\begin{mdframed}[ %
	linewidth = 2pt, %
	linecolor = red, %
	roundcorner = 6pt, %
	leftmargin = 10, %
	rightmargin = 10, %
	backgroundcolor = gray!20
]
As a general guideline, programs should be written to fail as loudly and as
early as possible. This pushes the job of finding errors and debugging up
the tree.

\begin{packed_enum}
	\item Just ``looks wrong'' to the programmer.
	\item Syntax highlighting in the editor makes a mistake stand out.
	\item Error detected by compiler.
	\item Error detected by linker.
	\item Runtime error detected by debug sanity checks through code.
	\item Runtime error caused in code at a different location to the bug.
	\item
		Runtime error only occurs when running in parallel, or in bigger
		calculations.
	\item
		Simulation appears to run correctly, but gives obviously wrong output.
	\item
		Simulation appears to run correctly, but gives subtly wrong output.
\end{packed_enum}
We strongly aim to be at the top of the list.
\end{mdframed}

\section{Code conventions}
The code in NECI has been developed over a number of years by many different
developers, and has very little standardisation of approach or code appearance.
This is not an example to copy!

We are trying to (gradually) normalise sections of code, and isolate those
sections which are old and generally unredemable from the rest of the code
base. As such there are a number of restrictions we place on code in NECI,
and a range of other guidelines.

\begin{description}

	\headitem{Fortran standard}
		Due to the use of procedure pointers, a reasonably up to date compiler
		supporting (at least some of) the Fortran 2003 standard is 
		\emph{required} to compile NECI. The C-interoperability and procedure
		pointer features of Fortran 2003 should be used. Other features of this
		standard should be used sparingly, as Fortran 2003 support in compilers
		is patchy at best.

		Otherwise, code should be written to the Fortran 90/95 standard. In
		particular, several features of FORTRAN 77 should be avoided at all
		costs:
		\begin{packed_itemize}
			\item
				\code{DO} statements using \code{REAL} type loop
				variables.
			\item Assigned \code{GOTO} statements
			\item 
				Cray pointers (declared with the format
				\code{pointer (ptr, pointee)})
			\item
				Implicit variables. \code{implicit none} MUST appear in
				every module, or interface statement.
			\item
				Implicitly typed routines and subroutines. See section on
				modules and interfaces.
			\item
				\code{COMMON} blocks for sharing data between files.
		\end{packed_itemize}
		All of these features do, or have appeared, in NECI at some point. It is also highly advised that \code{intent} arguments should be used for all argument declarations. This both improves performance, and the ability of others to quickly identify what the routine is attempting to do.

	\headitem{CAPITAL letters}
		Fortran is a case insensitive programming language.

		For historical reasons a large proportion of FORTRAN 77 code was
		written entirely in CAPITAL LETTERS (with the exception of displayable
		strings). This is extremely bad practice.
		
		Humans generally read by recognising word shape. This is obliterated
		in fully capitalised text, making code much harder to read, and typos
		especially difficult to identify.

	\headitem{Indentation}
		Indentation of sections of code should use spaces (and not tabs). The
		Fortran 95 standard explicitly rejects the use of tabs, and tabs in
		source code will elicit warnings from the compiler.

		All indentations should be multiples of 4 characters.

		Source code in \textasteriskcentered.F files (old-style FORTRAN 77)
		has specific layout restrictions. In particular an initial indent of
		7 spaces. This style should not be mimicked elsewhere.

	\headitem{Code line length}
		The Fortran 90/95 standard restricts line lengths to a (hard) maximum
		of 132 characters. Code with lines longer than this \emph{may} work
		on \emph{some} compilers, but this limit should be avoided.

		This limit applies after preprocessing has been applied. A number of
		our macros in \code{macros.h} can create lines of considerably
		longer length if not used carefully. These may require using temporary
		variables with shorter names to control the line length.

		The fixed-format FORTRAN 77 code is restricted to 72 characters per
		line.

		On a 19\" monitor at standard resolution, two columns of code
		vertically split and side by side use 79 characters each. This is a
		convenient soft-limit to use - although it is not trivially achievable
		in all code, and overall readability should be prioritised.

	\headitem{Variable name conventions}
		There are a number of competing conventions for variable and function
		names within NECI. Insisting on following any specific one would be
		highly hypocritical for anyone in the Alavi group. That said, there
		are a number of existing conventions that it is \emph{useful} to be
		aware of.
		\begin{packed_itemize}
			\item
				\code{CamelCase} or \code{snake_case} should be used
				to provide descriptive variable names. The wider the scope of
				a variable, the longer and more descriptive the name should be.
				Trivial local variables (loop indices, etc.) can and should be
				trivially named. Variables should not, ever, be used entirely
				in capital letters.
			\item
				\code{tVariableName} is a logical control variable.
				Normally globally declared in a module for switching on (or
				off) an overall feature, or signalling overall calculation
				state.
			\item
				\code{nVariableName} is an integer containing a count of
				a number of a given entity.

			\item
				\code{Varible}, \code{AllVariable} are paired sets of
				variables tracking an extensive property of a simulation. That
				is properties which can be accumulated on an individual node,
				but that the system-relevant property needs to be collected
				from all nodes and amalgamated. This is done once per iteration
				or once per update cycle as appropriate.
		\end{packed_itemize}

		Fortran 95 restricts variable names to 31 characters. Although
		Fortran 2003 extends this to 63, making use of this extension can cause
		problems with some compilers, and this should be avoided.

	\headitem{Subroutine decoration (especially intent statements)}
		Subroutine and function declarations should be decorated to the
		greatest extent feasible. This should restrict the variables to only
		their expected role in a function.

		In particular, all function arguments should be decorated with either
		\code{intent(in)}, \code{intent(out)}, or
		\code{intent(inout)} as appropriate. The additional decorations
		\code{optional} and \code{target} can be used with care.

		The supplied arguments should be as restrictive as possible, to
		maximise the likelihood of the compiler catching programming errors.

		The single exception to this rule is for routines that are to be
		stored in procedure pointers. These routines must exactly match the
		definition of the relevant \code{abstract interface}, which may
		be more general than is required for the specific case.
		
	\headitem{Data types}
		With the exception of small integers being directly assigned to known
		integer variables, or used in loop counters, all constants should have
		their types explicitly specified. The available types are described
		in section \ref{sect:cons-types}.

		Most specifically, the \code{*D*} specifier and the floating
		point type \code{double precision} should never be used. Examples
		such as \code{1.D0} should be replaced with \code{1.0_dp},
		and the data types \code{real(dp)} and \code{complex(dp)}
		should be used.

		As compilers have moved between 16-bit, 32-bit and 64-bit, there is
		ambiguity about whether \code{double precision} should mean a
		32-bit, 64-bit or 128-bit floating point value, depending on the age
		of the compiler and which compiler is used. This can cause chaos and
		difficult to track runtime bugs that appear only on certain machines.

		For Hamiltonian matrix elements (that may be real or complex depending
		on build configuration) the custom (preprocessor defined) data type
		\code{HElement_t} should be used, which resolves to either
		\code{real(dp)} or \code{complex(dp)}.
		
	
	\headitem{Array declarations}
		Fortran arrays can be declared in multiple ways. In particular, the
		dimensionality of an array can be declared on the variable itself,
		or as part of the type declaration;
		\begin{lstlisting}[gobble=8]
			integer, dimension(10, 20) :: arr1
			integer :: arr2(10, 20)
		\end{lstlisting}
		In general the latter declaration is preferred for two reasons:
		\begin{enumerate}
			\item
				It is clear that the array property is attached to the
				variable, and not to the type. When scanning data declarations
				it is not possible to mistake a scalar for an array.
			\item
				Multiple different arrays can be declared in the same data
				declaration with different bounds.
		\end{enumerate}
		The only exception to this is in templated code, where the bounds of
		arrays need to be varied.
		
		Where arrays are passed as arguments to a routine, they can be
		passed in three ways
		\begin{lstlisting}[gobble=8]
			integer, intent(inout) :: arr(*)
			integer, intent(inout) :: arr(10)
			integre, intent(inout) :: arr(:)
		\end{lstlisting}
		The first form should be avoided wherever practical, as it prevents
		any knowledge of the array dimensions being carried into the code.
		This means that whole-array manipulations will no longer work.
		
		The second two types can be used in different circumstances. The first
		essentially overrides the array dimensions passed in. This can be
		useful for re-indexing arrays (e.g. treating a zero-based array as
		one-based).
		
		The last approach allows a receiving routine to inspect the array
		bounds as passed in by the calling routine. This maximises the extent
		to which the compiler and debugging tools can assist in finding
		errors in the code, and should be used wherever possible.

	\headitem{Use statements}
		Globally declared symbols can be shared between modules using
		\code{use} statements. Generally, specific symbols should be
		included rather than all symbols in a module using the notation
		\code{use module_name, only: symbol, ...}.
		
		Modules which are templated (see section \ref{sect:templating}) should
		be included in full (no \code{only} statements). This includes
		\code{sort_mod}, \code{util_mod} and
		\code{Parallel_neci}.
		
		Modules containing only data that have been separated for the purposes
		of dependency resolution can be fully included into their related
		modules (e.g. \code{CalcData} and \code{Calc}).
		
		The module \code{constants} should also be fully included
		everywhere to reduce the likelihood of data types conflicting.

		Use statements should
		(where possible) be located in the module header, and not in
		individual subroutines - this avoids some serious issues associated
		with compilers resolving conflicting dependencies. If the same thing
		is included in multiple places in a file, the compilers dependency
		resolution tree can become very large, and use a lot of time and
		memory to resolve unambiguously.

	
	\headitem{{\ttfamily ASSERT} statements}
		\code{ASSERT} is a macro, defined in \code{macros.h}. In
		an optimised build these statements are entirely removed, and in a
		debug build they will cause execution to be aborted with an error
		message if the condition specified is not met.

		In NECI, the error message contains the current file and line number.
		It also includes the current function, which must be manually supplied
		in a constant named \code{this_routine}.

		An example assert statement, in a function that takes an array with
		the same number of elements as there are basis functions, would be:
		\begin{lstlisting}[gobble=8]
			subroutine foo(arr)
				integer, intent(inout) :: arr(:)
				character(*), parameter :: this_routine = 'foo'
				ASSERT(len(arr) == nBasis)
				...
			end subroutine
		\end{lstlisting}

		\begin{warningbox}
			Be careful not to use tests with side effects in \code{ASSERT}
			statements. In the optimised build the tests will not be called, 
			and this can introduce bugs that appear in only one of the
			optimised or debug builds.
		\end{warningbox}

	\headitem{Modules and interfaces}
		\begin{itemize}
			\item 
		\end{itemize}

	\headitem{Example module layout}
		A sample module layout is given below:
		\begin{lstlisting}[gobble=12]
            #include "macros.h" ! This enables use of our precompiler macros.
            module module_name
            	
            	! To the extent possible, include statements should be at the
				! beginning of a module, and not elsewhere.
            	use SystemData, only: nel, tHPHF
            	use module_data
            	use constants
            	implicit none
            
            	! Add an interface to an external (non-modularised) function
            	interface external_fn
            		function splat_it(in_val) result(ret_val) &
											  bind(c, name='symbol_name')
            			! n.b. interface statements shield from modular includes
            			use constants
            			implicit none
            			integer, intent(in) :: in_val
            			real(dp) :: ret_val
            		end function
            	end interface
            
            contains
            
            	[pure|elemental] subroutine sub_name(in_val, out_val)
            
            		! This is a description of what the subroutine does
            
            		integer, intent(in) :: in_val
            		real(dp), intent(out) :: out_val
            
            	end subroutine [sub_name]
            
            
            	[pure|elemental] function fn_name(in_val) result(ret_val)
            
            		! This is a description of what the function does does
            
            		integer, intent(in) :: in_val
            		real(dp) :: ret_val
            
            	end function [fn_name]
            
            end module
		\end{lstlisting}

\end{description}

\section{Important modules and common (global) variable names}
\label{sect:cons-types}

\subsection{\ttfamily CalcData}
\subsection{\ttfamily SystemData}

\subsection{Inexplicable names and anachronisms}
	A number of the variable names in NECI are largely inexplicable, or
	confusing, outside of their origin in historical accident. This section
	tries to clarify some of these.

	\begin{description}
		\headitem{ARR, BRR}
			\code{ARR(:,1)} contains a list of spin orbital (Fock)
			energies in	order of increasing energy.

			\code{ARR(:,2)} contains a list of spin orbital (Fock)
			energies indexed by the spin-orbital indices used in the
			calculation.

			\code{BRR} contains a list of spin orbital indices in
			increasing (Fock) energy order. Where multiple degenerate orbitals
			have the same symmetry, they are clustered so that spin orbitals
			with the same symmetry are adjacent to each other, and within that
			ordered by $m_s$ value.

		\headitem{TotWalkers and TotParts}
			\code{TotWalkers} refers to the number of determinants or
			sites that must be looped over in the main list. This may include
			a number of blank slots if the hashed storage is being used.

			The number of particles (walkers) on a core is stored in
			\code{TotParts}. The total number of particles in the system
			(including all nodes) is stored in \code{AllTotParts}.

		\headitem{InitWalkers}
			The number of particles \emph{per processor} before a simulation
			will enter variable shift mode.

		\headitem{G1}
			The variable name \code{G1} is a historical anachronism. It
			is an array containing symmetry information about given basis
			functions. In particular \code{G1(orb)} contains information
			about the one electron orbital \code{orb}.

			The data is of type \code{BasisFN}:
			\begin{lstlisting}[gobble=12]
				type symmetry
					sequence
					integer(int64) :: S
				end type
				type BasisFn
					type(symmetry) :: sym ! Spatial symmetry 
					integer :: k(3)       ! K-vector
					integer :: Ms         ! Spin of electron. Represented as +/- 1
					integer :: Ml         ! Angular momentum quantum number (Lz)
					integer :: dummy      ! Padding for alignment
				end type
			\end{lstlisting}
			Not all of these values are required for all simulations. The
			\code{sym} element points at a padded 64-bit integer. This
			is done for memory alignment reasons that are no longer important.
	\end{description}

\section{Don't Repeat Yourself (DRY)}
	When information becomes duplication in software, eventually the people
	that knew about the duplication will forget. And then the information will
	be changed --- but at least one of the duplicates won't be. This introduces
	bugs that are extremely difficult to track down.

	The Don't Repeat Yourself (DRY) principle is one that say a developer
	should systematically, and always, avoid duplication of information.

	NECI is a terrible example of this, but it has been improved over time
	with a lot of effort.

	Information is a very broad term. There are many types of duplication that
	can occur. A non exhaustive list of some types of duplication (and what
	can be done about them) follows.
	\begin{description}
		\headitem{Algorithm duplication across data types}
			There are many algorithms that are either the same, or similar,
			across many different data types. The logic involved in these
			should be written once.

			Major examples are the sorting routines in \code{sort_mod},
			general utilities in \code{util_mod}, shared memory in
			\code{shared_alloc} and MPI routines in
			\code{Parallel_neci}. Prior to implementing a generalised
			quicksort there were 37 different sort routines in NECI, using
			different sort methods, and containing different bugs.

			This duplication should be controlled using templating, as
			described in section~\ref{sect:templating}.

		\headitem{Logic duplication across source files}
			If the same chain of decision making is recurring in different
			regions of the code, these should be abstracted into their own
			subroutine which is called from each location. This prevents the
			logic being duplicated, and then diverging.

			Numerous bad examples of this still persist in NECI.

		\headitem{Duplication of data}
			Compile time constants should only be specified in one place. A
			large proportion of these are found in the
			\code{lib/cons_neci.F90} source file. Other examples include
			the layout of the bit representations (\code{BitReps.F90}).
			A significant proportion of these vary depending on the compile
			configuration, and prior to collecting them here the code was
			extremely fragile.

			Ongoing cases which are problematic include the use of the literal
			constant $6$ to specify output to stdout in statements such as
			\code{write(6,*)}, which doesn't interact well with
			\code{molpro}.

		\headitem{Duplication of representations in memory}
			It is important to have a well defined canonical representation of
			data in memory. The same data should not be allowed to become
			duplicated in multiple places.

			Temporary arrays, with working data copied into them, should be
			clearly temporary and discarded as soon as not necessary. If the
			primary data shifts to a new location, the old storage should be
			deallocated (if possible), or damaged so that attempts to use it
			fail loudly (such as putting a value of $-1$ into a variable that
			would normally hold an index).

			Avoid situations where code might work by accident.

			An ongoing situation of this type is the array variable
			\code{nBasisMax}. It shadowed a large number of global control
			variables, and there are still locations in the code where its
			value is used in preference to the global control value as these
			values diverge, and its value is the one that works in some
			obsolete code.
	\end{description}

	There is one, major, exception to this rule. Code that only exists for
	testing purposes (such as the contents of \code{ASSERT} statements,
	or unit tests) may be as explicitly duplicated as
	desired. Their \emph{purpose} is to explicitly flag up when anything
	elsewhere changes - so the risk of them getting out of sync with the code
	base is their purpose.
	
	\subsection{Procedure pointers (function pointers)}
		One issue that becomes immediately obvious when repeated logic has
		been abstracted into specific functions is that conditional logic is
		executed \emph{every} time certain actions are taken.
		
		In many cases this is not very important, as it occurs high up the
		call hierarchy, and the controlled code consumes the vast majority of
		the execution time. However, the closer we get to the inner most tight
		loops, the more expensive repeated conditional logic becomes. This
		is particularly frustrating if the decision making is based on global
		control parameters, and thus always results in the same code path being
		taken in a simulation. If the decision lies against the branch
		prediction metrics, then this is especially bad.
		
		The canonical example of this is accessing the 4-index integrals, which
		is performed very frequently.
		
		In these case, it is a good idea to separate the decision making logic
		from the execution, so that the conditional logic is only executed at
		runtime. This can be done using \emph{procedure pointers} (called
		function pointers, or similarly functors in other programming
		languages.
	
		These require defining the ``shape'' of a function call (i.e.\ its
		arguments, and return values) in an \code{abstract interface}. A
		variable can then be set to point at which of a range of functions
		with this signature should be executed.
		
		In NECI the global controlling procedure pointers are located in
		the module \code{procedure_pointer}, which contains both the abstract
		interface definitions, and the actual pointer variables. These
		variables can then be used as functions throughout the code.
		
		These procedure pointers are largely initialised in the routine
		\code{init_fcimc_fn_pointers}, where decisions are made between types
		of excitation generator, matrix element evaluation, etc. The procedure
		pointers involved in integral evaluation are set in
		\code{init_getumatel_fn_pointers}.
		
		The use of procedure pointers generates a strict Fortran 2003
		dependency for NECI. We used to make use of a hacky abuse of the
		linker and templating system to implement function pointers without
		language support, but this was deprecated once compiler support
		for procedure pointers was reasonably widespread.
		
%	\begin{description}
%		\headitem{Compiler configurations}
%		\#\#\# TODO: Need to discuss compiler configurations.
%		\headitem{Dependency generation}
%			The code must 
%
%		\headitem{Code templating}
%		\headitem{Code preprocessing}
%		\headitem{Compilation to object code}
%		\headitem{Linking (the stupidity of the linker)}
%			\ldots
%
%			The intel compiler suite can perform global code optimisation after
%			compilation. This is relatively slow and computationally expensive,
%			but should be used for production runs as it reduces the runtime.
%			It is not normally useful for development purposes.
%	\end{description}

\section{Don't optimise prematurely}
	Obviously, good algorithm design is important. If an algorithm scales
	badly, then no implementation will be able to salvage it.
	
	However, there are many tricks and optimisation that can be made to
	eek out small and large performance gains in the implementation of a
	particular algorithm. It is important not to optimise too early for a
	number of reasons.
	\begin{itemize}
		\item
			Good optimisation is extremely time intensive. On the whole time
			is better spent getting the code to work, and making the
			algorithm efficient. Once an implementation works, then code can
			be profiled and performance improved.
		\item
			Optimisations are often highly non-obvious, involving storing
			information in unexpected ways and places, leading to code that
			is hard to write, harder to read and extremely bug prone.
		\item
			The compiler is very good. The obvious `tricks' that you see will
			be done by the compiler anyway.
		\item
			One place where performance gains can legitimately be made is in
			avoiding conditional switching. In many cases this would involve
			duplicating code paths, and horrifically breaking the DRY
			principle above. There are occasions that this is worthwhile, but
			this should be actively justified by profiling data rather than
			just a hunch.
	\end{itemize}

\section{Tracking memory usage}
	NECI contains automated tracking of memory usage. This enables output
	statistics to indicate which memory uses are dominating during a
	calculation.
	
	The \code{MemoryManager} module keeps track of all units of memory, and
	assigns a tag value to each of them. It is the responsibility of the
	developer to store this tag, and pass it when the memory is deallocated.
	This tag is an \code{integer}.
	
	Memory should always be allocated using error checking. That is, an
	allocate statement should always be passed an error value as follows
	\begin{lstlisting}
		integer, allocatable :: arr1(:)
		real(dp), allocatable :: arr2(:,:)
		integer :: ierr
		allocate(arr1(10), arr2(20, 30), stat=ierr)
	\end{lstlisting}
	This value will be zero if the allocation was successful, and non-zero
	otherwise. The memory logging routines check this value, and report an
	error if the memory allocation failed.
	
	Memory is logged using the functions \code{LogMemAlloc} and 
	\code{LogMemDealloc}.
	
	

\section{Code templating}
\label{sect:templating}
	NECI has an additional build step in comparison with most Fortran code.
	Prior to running the C preprocessor, the custom script
	\code{tools/f90_template.py} converts all of the files with filenames
	\code{*.F90.template} into files with the corresponding names
	\code{*.F90}.

	This mechanism exists to allow general code to be written, and reused for
	different types, and combinations of types. While this is largely a
	combinatorial pattern-matching and substitution problem, the templater
	contains specific additional features to facilitate dealing with array
	types in Fortran. There are also a number of specific considerations that
	need to be made.

	The templated code in NECI has been written largely by Simon Smart and Alex
	Thom, who should be able to help with any particularly nasty issues
	arising.

\subsection{How it works}
	Fortran permits multiple routines to be referenced by the same name through
	the use of interface blocks such as
	\begin{lstlisting}[gobble=4]
		interface sub_name
			module procedure actual_name_1
			module procedure actual_name_2
		end interface
	\end{lstlisting}
	which allows either of the routines \code{actual_name_1} or
	\code{actual_name_2} to be called using the Fortran symbol
	\code{sub_name}. Note that these procedure constructs can be used directly in the code
	for a hard-coded set of routines which can be called from one interface name if desired (see section \ref{sec:manualrenamingofroutines}).

	It is perfectly acceptable to have multiple interface blocks for a specific
	routine name, so long as all of the referenced routines have different
	calling signatures. That is, they must accept differently typed arguments
	so that it is possible for the compiler to determine \emph{at compile time}
	which of the routines should be called. In principle the actual routine
	names can always be used.

	The Fortran templater creates one module per specified configuration,
	each with a unique module name. It then performs substitutions on a
	specified template model to create routines for all of the specified
	combinations of input types. These routines have their names adjusted to
	make them unique for each configuration, and an interface block is created
	to make them accessible under their original name. Finally all of these
	newly created modules are collected, with \code{use} statements, into
	a macroscopic module which can be used from elsewhere.

\subsection{Overall structure}
	A sample templated module structure is given here for reference. The
	different sections are explained below.

	\begin{lstlisting}[gobble=4,language=ini]

		# This is the configuration block. Note that it has *.ini syntax,
		# and that comments are preceeded by hashes.
		[int]
		type1=integer(int32)

		[float]
		type1=real(dp)

		===================
	\end{lstlisting}
	\begin{lstlisting}[gobble=4]
		#include "macros.h"

		module module_name

			! This is the module which is templated to generate the ensemble
			! of routines with differing types
			use constants
			implicit none

		contains

			elemental function test_fn(arg) result(ret)

				%(type1)s, intent(in) :: arg
				%(type2)s :: ret

			end function

		end module


		supermodule module_name
			!
			! Here we include code that should be included in the module but
			! does not need to be templated.
			!
		end supermodule


	\end{lstlisting}

\subsection{Configuration names and substitution}
	The top section of the file defines the templated configurations. It has
	the structure of an INI file, and is processed by the standard python ini
	file parser.

	Configurations are defined by a name, contained in square brackets, and
	then by a series of key-value pairs. All values are treated as strings for
	the purpose of substitution in the main body of the routine.

	Configurations are \emph{inherited}. That is to say that all key-value
	pairs (with the exception of \code{conditional_enable}) are carried
	forward to the next configuration in the file unless they are overridden.
	This permits quite sparse configuration files, at the expense of being
	a bit more tricky to modify.

	The length of configuration names must be considered. They will be appended
	to module names and the subroutine and function names contained therein.
	The templater does not have a magic means to circumvent the Fortran 95
	limit of 31 characters in any symbol. Therefore it makes sense to use
	highly abbreviated configuration names.

	The templater modifies the names of subroutines and functions as it
	processes the module. As such, it is a little pick about syntax. Normal
	routine decoration specifiers such as \code{pure} and
	\code{elemental} are supported, but functions \emph{must} be declared
	using the \code{result()} specifier to define the return type.

	If the special key \code{conditional_enable} is present, this is used
	to wrap the generated module in \code{#if #endif} elements. See
	\code{lib/quicksort.F90.template} for examples.

	Values from the key-value pairs are directly substituted into the templated
	module below, where they replace the element \code{%(key)s}. (This
	specifier is the standard python named-string specifier). Keys named
	beginning with \code{type} are treated specially, as described in
	the following section.

	The templater cannot circumvent Fortran line length limits. If necessary
	a value to substitute can be extended over multiple lines by ensuring the
	first character on a new line is a space, and then just continuing.
	Make sure you remember the Fortran line continuation characters, as in
	this example from the MPI wrapper code:
	\begin{lstlisting}[gobble=8]
		mpilen=((ubound(v,1)-lbound(v,1)+1)*(ubound(v,2)-lbound(v,2)+1)*&
         (ubound(v,3)-lbound(v,3)+1))
	\end{lstlisting}
	
	There is a special variable, which can be accessed using \code{%(name)s}.
	This contains the name of the current configuration.

\subsection{Variable substitution}
	The templater has extremely powerful mechanisms to manipulate the types of
	variables. Variable manipulation is enabled by using a key in the key-value
	pair section that begins with \code{type}. In particular, the code is able
	to manipulate the number of dimensions that different arrays have.
	
	As an example, take a routine which is passed an array, and a value
	that could be an element of that array (such as is necessary for a binary
	search), such that
	\begin{lstlisting}[gobble=4]
		subroutine example(arr, elem)
			%(type1)s :: arr(:)
			%(type1)s :: elem()
			...
		end subroutine		
	\end{lstlisting}
	If \code{type1} is a scalar value, this does a substitution exactly as
	would be expected:
	
	\begin{center}
	\begin{tabular}{lcl}
	\begin{lstlisting}[language=ini,gobble=8]
		[int]
		type1=integer(int32)
	\end{lstlisting}
	
	& $\Longrightarrow$ &	
	
	\begin{lstlisting}[gobble=8]
		subroutine example_int(arr, elem)
			integer(int32) :: arr(:)
			integer(int32) :: elem
			...
		end subroutine
	\end{lstlisting}
	\end{tabular}
	\end{center}
	
	However, it may be necessary for the value which is being considered in
	the array to itself be an array. An example of this would be the bit
	representations used in NECI --- a list of of these is a two dimensional
	array, and any intermediate values would be arrays themselves.
	
	In this case, an array type should be specified using the \code{dimension}
	keyword, and the code will be automatically adjusted as follows:
	
	\begin{tabular}{lcl}
	\begin{lstlisting}[language=ini,gobble=8]
		[arr_int64]
		type1=integer(int64), dimension(:)
	\end{lstlisting}
	
	& $\Longrightarrow$ &	
	
	\begin{lstlisting}[gobble=8]
		subroutine example_int(arr, elem)
			integer(int64) :: arr(:,:)
			integer(int64) :: elem()
			...
		end subroutine
	\end{lstlisting}
	\end{tabular}
	
	Essentially the number of \code{:} delimiters appearing in the variable
	definition is combined with the number of dimensions specified in the type.
	
	As a special case, temporary variables can be created of an appropriate
	size which are either scalars, or have one dimension. For the definition
	\begin{lstlisting}[gobble=4]
		%(type1)s :: arr(:)
		%(type1)s :: tmp(size(arr(1)))
	\end{lstlisting}
	then adjustment occurs as follows
			
	\begin{tabular}{lcl}
	\begin{lstlisting}[language=ini,gobble=8]
		[int]
		type1=integer(int32)
	\end{lstlisting}
	& $\Longrightarrow$ &
	\begin{lstlisting}[gobble=8]
		integer(int32) :: arr(:)
		integer(int32) :: tmp
	\end{lstlisting} \\[1.5em]
		
	\begin{lstlisting}[language=ini,gobble=8]
		[arr_int64]
		type1=integer(int64), dimension(:)
	\end{lstlisting}
	& $\Longrightarrow$ &
	\begin{lstlisting}[gobble=8]
		integer(int64) :: arr(:,:)
		integer(int64) :: tmp(size(arr(1)))
	\end{lstlisting}
	\end{tabular}
			
	In a similar way, the references made to these variables within the
	routines must be adjusted. This is to ensure that correct sized array
	slices are used at all times. For the original templated code
	\begin{lstlisting}[gobble=4]
		arr1(j) = arr2(i)
	\end{lstlisting}
	the following will result in the the templated output if the variable is
	of an adjustable type and declared at the top of the function:
	
	\begin{tabular}{lcl}
	\begin{lstlisting}[language=ini,gobble=8]
		[int]
		type1=integer(int32)
	\end{lstlisting}
	& $\Longrightarrow$ &
	\begin{lstlisting}[gobble=8]
		arr1(j) = arr2(i)
	\end{lstlisting} \\[1.5em]
		
	\begin{lstlisting}[language=ini,gobble=8]
		[arr_int64]
		type1=integer(int64), dimension(:)
	\end{lstlisting}
	& $\Longrightarrow$ &
	\begin{lstlisting}[gobble=8]
		arr1(:,j) = arr2(:,i)
	\end{lstlisting}
	\end{tabular}

\subsection{The supermodule}
	In many modules, there are routines that do not need to be templated for
	different variable types. As an example, within the MPI wrapper routines,
	the code to initialise MPI is not variable type specific.
	
	Code which is placed in the supermodule is not templated, and is included
	directly in the final generated module.

\subsection{Optional parameters and lines of code}
	It is good practice to write templated routines as generally as possible.
	This likely involves adding more functionality than is needed in all cases,
	and switching this functionality on and off in some way.
	
	For example, the sorting routine can sort multiple arrays in parallel,
	according to the order in the first array (such as sorting a list of
	determinants into energy order, where the energies are stored in a separate
	array). It also needs to have comparison functions defined for scalars as
	well as arrays.
	
	The extent to which interesting features can be developed is limited only
	by the developers imagination in using the template substition. But two
	tricks are generally useful.
	
		\begin{description}
		\headitem{Additional optional arguments}
			Subroutines can easily be given flexible numbers of arguments. This
			is useful for adding additional functionality (and allows multiple
			templated routines to use the same \code{type} values). The
			templated subroutine definition
			\begin{lstlisting}[gobble=12]
				subroutine example(arg%(extra_args)s)
			\end{lstlisting}
			will generate the following code
			
			\begin{minipage}{\textwidth}
			\begin{tabular}{lcl}
			\begin{lstlisting}[language=ini,gobble=16]
				[simple]
				extra_args=
			\end{lstlisting}
			& $\Longrightarrow$ &
			\begin{lstlisting}[gobble=16]
				subroutine example(arg)
			\end{lstlisting} \\[1.5em]
		
			\begin{lstlisting}[language=ini,gobble=16]
				[extended]
				extra_args=, arg2, arg3
			\end{lstlisting}

			& $\Longrightarrow$ &
			\begin{lstlisting}[gobble=16]
				subroutine example(arg, arg2, arg3)
			\end{lstlisting}
			\end{tabular}
			\end{minipage}
			
			The next trick is useful for adding the type definitions of these
			additional arguments, and enabling the code which uses them.
		
		\headitem{Switching off lines of code}
			Lines of code in Fortran are trivially disabled when they are
			commented out. Prefixing lines with a switch-value allows it to be
			disabled. For example
			\begin{lstlisting}[gobble=12]
				%(use_type2)%(type2) :: val()
			\end{lstlisting}
			will allow an additional type to be used in a routine depending on
			the configuration:
			
			\begin{minipage}{\textwidth}
			\begin{tabular}{lcl}
			\begin{lstlisting}[language=ini,gobble=16]
				[unused]
				type2=
				use_type2=!
			\end{lstlisting}
			& $\Longrightarrow$ &
			\begin{lstlisting}[gobble=16]
				! :: val()
			\end{lstlisting} \\[1.5em]
		
			\begin{lstlisting}[language=ini,gobble=16]
				[arr_real]
				type2=real(dp), dimension(:)
				use_type2=
			\end{lstlisting}

			& $\Longrightarrow$ &
			\begin{lstlisting}[gobble=16]
				real(dp) :: val(:)
			\end{lstlisting}
			\end{tabular}
			\end{minipage}
	\end{description}


\subsection{Manual renaming of routines}	\label{sec:manualrenamingofroutines}
	The user can additionally manually create interface blocks for the
	templated routines. This is useful where there is more than one possible
	function to call for each of the variable types.
	
	An example of this is given in the MPI wrapper functions, where there are
	versions of routines that require manually specifying the lengths of
	various parameters, and automatic versions which take the lengths from the
	sizes of the arrays passed in. At the top of the templated module definiton
	lie interfaces blocks such as
	\begin{lstlisting}[gobble=4]
		interface MPIReduce
			module procedure MPIReduce_len_%(name)s
			module procedure MPIReduce_auto_%(name)s
		end interface
	\end{lstlisting}
	which makes use of the special \code{%(name)s} element to reference the
	generated routines after templating.
	
	In this case, the templated routines \code{MPIReduce_len} and
	\code{MPIReduce_auto} will be available to the user as usual, but the
	routines can both be called by	the more generic name of \code{MPIReduce}
	with the appropriate arguments supplied.

\subsection{Examples}
	All of the features of the templating code have been heavily used the
	Shared Memory code, in \code{lib/allocate_shared.F90.template},
	the sorting code in \code{lib/quicksort.F90.template} and the
	MPI wrapper code in \code{lib/Parallel.F90.template}. Other less
	aggressively used case can be found elsewhere.

\section{Testing}
NECI comes with a set of tests in the test\_suite directory. Each of these
tests have a benchmark file. When you run the tests, the results of your
calculation will be compared against those from the benchmark files. If the
values of certain results agree to within a predefined tolerance, the test
will pass.

The test suite is run using a python program called testcode2. This program
will call the desired tests, compare the results against benchmarks, and let
the user know the outcome of each test.

You can clone testcode2 from github with the following command:

\begin{lstlisting}[language=bash]
    $ git clone https://github.com/jsspencer/testcode ~/testcode2}
\end{lstlisting}

There are tests for each of the three executables: neci, mneci and kneci.
These are stored in the three directories with the corresponding names.
These directories are then divided into further directories for the
different types of tests. For example, mneci has subdirectories called
rdm, excited\_state, kpfciqmc and so on, with tests for each of these
corresponding features of NECI.

To run the entire test suite, just do

\begin{lstlisting}[language=bash]
    $ ~/testcode2/bin/testcode.py
\end{lstlisting}

in the test\_suite directory (assuming you cloned testcode2 to your home
directory). To run this, you will have to have all of neci, mneci and
kneci compiled.  However, you can also run a subset of tests. For example,
to run all mneci tests do

\begin{lstlisting}[language=bash]
    $ ~/testcode2/bin/testcode.py -c mneci
\end{lstlisting}

or to run a particular single test do

\begin{lstlisting}[language=bash]
    $ ~/testcode2/bin/testcode.py -c mneci/rdm/HeHe_int
\end{lstlisting}

or to run two particular tests do

\begin{lstlisting}[language=bash]
    $ ~/testcode2/bin/testcode.py -c mneci/rdm/HeHe_int -c mneci/rdm/HeHe_real
\end{lstlisting}

By default, testcode will just tell you whether or not the test passed.
If the test failed, you can get further information by increasing the
verbosity of the output. For example,

\begin{lstlisting}[language=bash]
    $ ~/testcode2/bin/testcode.py -c mneci/rdm/HeHe_int -v
\end{lstlisting}

or

\begin{lstlisting}[language=bash]
    $ ~/testcode2/bin/testcode.py -c mneci/rdm/HeHe_int -vv
\end{lstlisting}

which will tell you why the individual test did not pass. You can also
use the verbosity flags when running the entire set of all tests.

\subsection{Adding a new test}
To add a new test, first create a directory in the appropriate subdirectory
(for a parallel neci job, inside ./neci/parallel). In this directory add the
test's input file (with a name ending in '.inp') and any necessary additional
files, such as integral files or POPSFILEs.

You should then add these files to git, for example:

\begin{lstlisting}[language=bash]
    $ git add neci/parallel/new_test
\end{lstlisting}

If the test is added to a new subdirectory then you may need to add it to
./jobconfig. If you added it to an already-exisiting directory, such as
neci/parallel, then it should be automatically found by testcode using the
globbing in jobconfig.

You must then create a benchmark file by running the test suite. To create a
new benchmark for \emph{only} the new test then run, for example,

\begin{lstlisting}[language=bash]
    $ testcode2/bin/testcode.py make-benchmarks -ic neci/parallel/new_test
\end{lstlisting}

This should run just the new test. You will be told that the test has failed,
and asked if you would like to set the new benchmark. If you believe that the
test has run correctly then do so with 'y'.

'-i' tells testcode to 'insert' the new benchmark ID at the start of the old
list of benchmarks (located in ./userconfig). When testcode is run later, it
will use the benchmark files with these IDs to compare against.

If you don't include '-i' then testcode will remove all previous benchmarks.
This is useful if you want to reset benchmarks for the whole test suite, which
can be done with:

\begin{lstlisting}[language=bash]
    $ testcode2/bin/testcode.py make-benchmarks
\end{lstlisting}

Finally, once this is done you need to add the new benchmark file to git.
If the new benchmark ID is, for example,

mneci-c3462e0.kneci-c3462e0.neci-c3462e0

then you can do:

\begin{lstlisting}[language=bash]
    $ git add neci/parallel/new_test/*dneci-c3462e0.kneci-c3462e0.neci-c3462e0*
\end{lstlisting}

You should then commit the newly added files. You might like to try running
testcode on the new test, and making sure it runs and passes as expected,
confirming the that the test was added correctly:

\begin{lstlisting}[language=bash]
    $ ~/testcode2/bin/testcode.py -c neci/parallel/new_test
\end{lstlisting}

\subsection{Unit tests}
    Unit tests should test discrete, small, elements of functionality. Ideally these should be
    the smallest elements such that functionality is then composed from ``units'' that have all
    been tested. By considering each of the elements explicitly, it is possible to writ tests
    for edge-case behaviour, where it is difficult to ensure that this behaviour will be tested
    in a full integration test case.

    Unit tests are found in the \code{unit_tests} directory. They are arranged in subdirectories,
    each of which corresponds to one of the \emph{files} of NECI source code. There are a number
    of technical steps to integrating new unit tests. 
    \begin{enumerate}
        \item
            A passing test is an executable that returns 0, and any other return value indicates
            failure. The developer has an entirely free choice to determine how they wish to
            write test executables.
        \item
            The library FRUIT is provided to assist in writing unit tests. Within a given test
            (i.e. testing a given function, or unit), there should be a \emph{suite} of tests
            to cover all possbile cases. FRUIT provides helper functionality to keep track of
            which element of a suite is currently running (using the \code{TEST()} macros),
            check values (using \code{call assert_equals}, \code{call assert_true} and so forth),
            keep track of where errors occurred and report them in an easily readible form. The
            module can be imported with \code{use fruit}. See existing tests for examples.
        \item
            If the current directory of tests does not contain a \code{CMakeLists.txt} file, create it.
            Ensure that it contains a \code{foreach()} loop over the available
            \code|\${PROJECT_NAME}_CONFIGURATIONS| so that all the build configurations of
            neci get tested. The directory should be added to the main \code{CMakeLists.txt} in
            the \code{unit_tests} directory using the \code{add_subdirectory()} command.
        \item
            Add the test to the \code{CMakeLists.txt} file in the directory in which it resides
            using the \code{neci_add_test} command. This will require you to specify a name for
            the test, and the appropriate .F90 file.
        \item
            To test the appropriate configuration of neci, add \code|lib{k,m,d,}neci| to the
            LIBS line. To use the FRUIT helpers, add \code{fruit} to this line.
    \end{enumerate}
    The easiest way to get these details correct is to copy existing examples.

    Unit tests can be run using the command
    \begin{lstlisting}[gobble=4]
        ctest [-R <regex>]
    \end{lstlisting}
    which is a built in part of the CMake toolkit. By default this will execute all avaialable
    tests and print a report on successes and failures. The optional \code{-R} flag specifies
    a regular expression, and only tests matching this will be executed. There are a number
    of further options available.

    A test can also be run directly by running its executable manually. This can be more
    straightforward for capturing the output of a failing test whilst debugging. The
    executables are found in the same position in the build directory as the source files
    are in the main repository (i.e.\ \code[breaklines=true]{unit_tests/det_bit_ops/test_countbits.F90}
    gives \code[breaklines=true]{<build_dir>/unit_tests/det_bit_ops/test_countbits}).

\section{Interfacing C (and C++) code}
Developers have always mixed Fortran code with external routines implemented
in other languages, especially C. This has generally been done on an ad-hoc
basis by exploiting the naivety of the linker --- in particular that the linker
will resolve dependencies with any symbol of the specified name. This is
useful, but introduces a number of potential problems:
\begin{description}
	\headitem{Name clashes}
		Different compilers follow different naming conventions. In particular
		Fortran compilers often (but not always) append or prepend one or two
		underscores to symbols in the object files. This is fine when resolving
		against other Fortran generated symbols, but requires coordination with
		the symbol names produced in C.

		A solution with an array of compile flags controlling the naming in the
		Fortran compiler, and underscores liberally scattered through C files
		is fragile and unreliable. It also makes it difficult to call library
		routines written in C.

	\headitem{No checking of parameters}
		The linker is extremely stupid - it only matches by parameter name. If
		this method is used, absolutely no checking is done on the parameters
		passed to the C routine from Fortran. This is a recipe for disaster,
		and will generate only runtime errors.

	\headitem{Calling}
		By default C passes arguments by value, whereas Fortran passes them
		by pointer. This requires writing wrappers for almost any non-trivial
		C library routine to access it from Fortran. Some constructs simply
		cannot be emulated.

	\headitem{Variable types}
		As an extension of the lack of checking of parameters, there is no
		checking of argument types across the Fortran/C interface. This relies
		on the Fortran and C code using the same types - in particular the
		same size of floating point and integer variables. This is extremely
		hard to guarantee, and these can fluctuate according to compiler flags.
		This can result in compiler and computer specific runtime errors that
		are extremely difficult to track.
\end{description}
All of these problems can be solved using structured interfacing, at the cost
of introducing a dependency on the Fortran 2003 standard.

All access to C routines in NECI must be through a declared interface. This
should be declared only once, in a module. An example is given here:
\begin{lstlisting}
interface
	! Note that we can define the name used in fortran code, and the C
	! symbol that is linked to independently.
	subroutine fortran_symbol(arg1, arg2) bind(c, name="c_symbol")
		! The module iso_c_hack is a wrapper for iso_c_binding. This
		! contains some workaround for incomplete Fortran 2003 support
		! across compilers.
		use iso_c_hack
		integer(c_int), intent(inout) :: arg1      ! Passed by pointer
		integer(c_bool), intent(in), value :: arg2 ! Passed by value
	end subroutine
end interface
\end{lstlisting}
A good summary of the rules and procedures for using this interoperability are
given in this Stack Overflow answer:
\url{http://stackoverflow.com/tags/fortran-iso-c-binding/info}

C++ routines can be made suitable for access from Fortran by prepending symbol
declarations in the C++ code with \code[language=C++]{extern "C"}.

\section{Debugging tips}
Code breaks. Sometimes it appears to rot with time. Finding bugs takes the
majority of most programmers time, and practices which make this quicker are
invaluable!

Obviously, the techniques you will use will depend on the nature of the
problem (tracking down small numerical changes in output is generally much
harder than finding what causes a segfault), but there are number of tricks
which can help!

\subsection{Build configurations}
As soon as you have a problem, build a debug rather than an optimised version
of the code. This cause a large array of changes to the compiled code:
\begin{description}
	\headitem{Array bounds checking}
		All Fortran arrays have well defined bounds on all of their dimensions.
		In debug mode the compiler will insert code to check that all memory
		accesses are within these bounds. If not, execution will be terminated
		with a message indicating at what line of what file the error occurred.
		If running in a debugger (see later) execution will be interrupted at
		this point.

	\headitem{Disable optimisations}
		Hopefully this will not make the bug go away! If it does, you are
		almost certainly looking at either an uninitialised variable, or access
		beyond the end of an array.

		The primary purpose of disabling optimisations is to make the mapping
		between the source code and the executable more linear. This results
		in any error messages, and the output of any tools, being easier to
		interpret.

	\headitem{Adds debugging symbols}
		When your code crashes it is really useful to know what routine was
		running, and what the stack trace (list of routines that have been
		called to get to this point in the code) is. Adding debugging symbols
		provides the information to convert the memory addresses into files
		and lines of source code. This makes error messages useful.		

	\headitem{Enables the {\ttfamily ASSERT} macro}
		There are many consistency checks internally in NECI that can be turned
		on in debug mode. Particularly for difficult-to-find bugs, these are
		likely to fail substantially earlier in a run than it is possible to
		view the problems in the normal output.

	\headitem{Defines {\ttfamily \_\_DEBUG}}
		Any blocks contained inside \code{#ifdef __DEBUG} sections are
		only enabled in debug mode. Most of these contain either additional
		output specifically targetted to make debugging easier, or additional
		consistency checks.
\end{description}

\subsection{{\ttfamily ASSERT} statements are your friend}
	Generally the time to add \code{ASSERT} statements to your code is
	when you are writing it. Debugging will make you acutely aware of their
	benefit. If you have a suspicion in which bits of code a bug might lie,
	liberally sprinkling it with \code{ASSERT} statements can help to
	find bugs.

	More usefully, when you find the bug, add \code{ASSERT} statements to
	the code to catch similar errors in the future.

\subsection{Learn to use your tools}
	Most of the programming tools in existence are for the purposes of
	debugging. Learn to use them! Practice using them. The really work.

	\begin{description}
		\headitem{Debuggers}
			The debugger is the most powerful tool you have. Essentially you
			run your code in a harness, with the debugger hooked into
			everything important. On most Linux systems, the most readily
			available debugger is \code{gdb}.

			The debugger can trap any execution errors, and will interrupt
			(break) execution of your program at this point --- while
			preserving all of the memory and execution state. You can examine
			the call stack (the nested list of functions that have been
			called), and the status of all of the memory. You can also change
			the contents of any relevant memory and continue execution to see
			the effects.

			Break points can be added to the code at any line of any function,
			and execution interrupted at that point. If your bug is only
			showing late in execution, you can interrupt on the $n$-th time
			a function is called. You can step through the execution line by
			line in the source code, examining all variables, and watch
			precisely what goes wrong.

			The debugger is the swiss-army sledgehammer of tools.

		\headitem{Valgrind}
			Valgrind is a tool for memory debugging. In essence it replaces
			most of the memory manipulation primitives provided by the
			operating system with instrumented versions. It will track what
			happens to memory, where it is created, where it is destroyed,
			and what code (in)correctly accesses it.

		\headitem{Intel Inspector XE}
			If you have access to Intel tools, the Intel Inspector is an
			extremely powerful debugger, memory analysis tool, performance
			enhancement and problem tracking tool. An top of a good interface
			to normal debugging tools, it can apply a lot of
			analysis to your code's execution, and then filter through it to
			find where things go wrong.
	\end{description}

\subsection{Machete Debugging}
	When bugs are particularly non-obvious, a good first step is to reduce the
	complexity of the problem. Try and remove as much code as possible. A good
	rule of thumb is to remove half of the code, and retest. If the bug has
	gone away, then it required interacting with the other half of the code.

	This technique can very quickly isolate a test case (that still fails) into
	something of manageable size. In doing so, the bug normally becomes
	obvious. You can then revert to the original code, and fix the problem
	there.

	In a complex code, this process can be tricky. Lots of the sections of code
	are coupled to each other. This can require some creative thinking. Choose
	the domain of your problem carefully (it may only be necessary to apply the
	machete to one file), and aggressively decouple sections by commenting out
	function calls.

	Remember --- it doesn't matter if you break functionality doing this (you
	obviously will) so long as you retain the buggy behaviour in the code that
	is left.

	This is really good for finding strange memory interactions --- left with
	the two routines that are trampling on each others toes.

\subsection{{\ttfamily fcimcdebug 5}}
	For bugs that change the trajectory of simulations, turning on the maximum
	debug output level in NECI (by adding \code{fcimcdebug 5} to the
	logging section of the input file in a debug build) can often give a quick
	insight into where the problem is located.

	Many problems will exhibit with obviously pathological behaviour. For
	example trying to spawn \code{NaN} particles, or an extremely large
	number, generally indicates a problem in either Hamiltonian matrix element
	generation or calculating the generation probabilities. Similarly walkers
	being spawned with repeated or zeroed orbitals are a give away.

	Similarly, if the bug is recently introduced and a prior version functioned
	correctly (such as a test code failure)

\subsection{Look at the git logs}
	If you are chasing down a regression (such as a testcode failure), then
	there is certainly a version that used to work, and a version which now
	does not. Frequently there are not many commits between these two versions
	--- have a look at what they are. Often the failure is really obvious!

	If there are a large number of commits between the last known good commit
	and the first known bad commit, then using \code{git bisect} is a very
	efficient way to locate the first bad commit and the last good commit.


% ------------------------------------------------------

\chapter{Guide to specific code in NECI}

\section{Important modules and common (global) variable names}
\label{sect:cons-types}

\subsection{Inexplicable names and anachronisms}
	A number of the variable names in NECI are largely inexplicable, or
	confusing, outside of their origin in historical accident. This section
	tries to clarify some of these.

	\begin{description}
		\headitem{ARR, BRR}
			\code{ARR(:,1)} contains a list of spin orbital (Fock)
			energies in	order of increasing energy.

			\code{ARR(:,1)} contains a list of spin orbital (Fock)
			energies indexed by the spin-orbital indices used in the
			calculation.

			\code{BRR} contains a list of spin orbital indices in
			increasing (Fock) energy order. Where multiple degenerate orbitals
			have the same symmetry, they are clustered so that spin orbitals
			with the same symmetry are adjacent to each other, and within that
			ordered by $m_s$ value.

		\headitem{TotWalkers and TotParts}
			\code{TotWalkers} refers to the number of determinants or
			sites that must be looped over in the main list. This may include
			a number of blank slots if the hashed storage is being used.

			The number of particles (walkers) on a core is stored in
			\code{TotParts}. The total number of particles in the system
			(including all nodes) is stored in \code{AllTotParts}.

		\headitem{InitWalkers}
			The number of particles \emph{per processor} before a simulation
			will enter variable shift mode.

		\headitem{G1}
			The variable name \code{G1} is a historical anachronism. It
			is an array containing symmetry information about given basis
			functions. In particular \code{G1(orb)} contains information
			about the one electron orbital \code{orb}.

			The data is of type \code{BasisFN}:
			\begin{lstlisting}[gobble=12]
				type symmetry
					sequence
					integer(ints64) :: S
				end type
				type BasisFn
					type(symmetry) :: sym ! Spatial symmetry 
					integer :: k(3)       ! K-vector
					integer :: Ms         ! Spin of electron. Represented as +/- 1
					integer :: Ml         ! Magnetic quantum number (Lz)
					integer :: dummy      ! Padding for alignment
				end type
			\end{lstlisting}
			Not all of these values are required for all simulations. The
			\code{sym} element points at a padded 64-bit integer. This
			is done for memory alignment reasons that are no longer important.
	\end{description}

\section{Parallelism}
	FCIQMC is a highly parallelisable algorithm. Implementationally this is
	achieved through the use of independent MPI processes that communicate
	using MPI.
	
	The raw \code{MPI_*} routines, provided by MPI should not be used directly
	inside NECI for a couple of reasons:
	\begin{packed_itemize}
		\item
			Depending on the build configuration and compiler, many variables
			may change their size or alignment, leading to code which only
			works on some compilers, and
		\item
			Naming conventions for linking vary between compilers.
	\end{packed_itemize}
	The \code{Parallel_neci} module abstracts these implementational details
	away, presenting a consistent interface.

\subsection{Usage}
	The relevant MPI functions are accessible through wrapper functions with the
	underscore in the name removed. For example, the MPI routine
	\code{MPI_AllReduce} is available as \code{MPIAllReduce}.
	
	The arguments to the wrapper routines are a subset of those specified in the
	MPI standard, and the explanations of those values may be found there.
	
	Many of the routines have versions which require specifying the array
	dimensions manually (\code{*_len}) or automatically (\code{*_auto}). The
	latter obtain the dimensions of the arrays to communicate by analysis of
	the array bounds and are preferred, as they are less prone to user error.	
	
	If communication is desired between subsets of the available processors (as
	is used in the CCMC code), the MPI communicator may be specified through the
	\code{Node} parameter. No further detail is provided here about how to
	construct these objects.	
	
	If the necessary MPI routine is not present in the \code{Parallel_neci}
	module it will be necessary to add it. This can be awkward as discussed
	below.

\begin{warningbox}
	The \code{inplace} MPI functionality is present (although disabled) in
	NECI. For the time being it should not be used due to serious bugs in the
	interaction between ifort and OpenMPI.
\end{warningbox}

\begin{warningbox}
	Different implementations of MPI are not interchangeable at runtime, even if
	they initially appear to be. For instance, code compiled using OpenMPI on the
	ifort compiler will run on MPICH built with the gnu toolset, but no
	communication will occur, and \code{nProcessors} independent (identical)
	simulations will run.
\end{warningbox}

\begin{warningbox}
	In keeping with the notion of failing early, and failing loudly, outside
	of the initialisation and cleanup code, most of the templated MPI routines
	do not quietly report errors in status variables, but will kill the
	entire simulation with a runtime error.
\end{warningbox}

\subsection{Important variables}
	A number of important control variables are available for use within NECI.
	\begin{description}
		\codeitem{nProcessors}
			The total number of processors initialised in the MPI calculation.
		\codeitem{iProcIndex}
			The (zero based) index of the current processor. This will be
			between 0 and \code{nProcessors-1}.
		\codeitem{root}
			The (zero based) index of the root (head) processor. This should be
			tested in code using: \code{if (iProcIndex == root)}.
		\codeitem{nNodes}
			The number of nodes initialised in the MPI calculation. In most
			cases this will equal \code{nProcessors}. These values will only
			differ if the MPI space is being subdivided into smaller nodes.
		\codeitem{iNodeIndex}
			This (zero based) index gives the position of the processor in the
			current node. For most calculations this will be zero on all
			processors.
		\codeitem{bNodeRoot}
			This specifies if the current processor is the root processor of a
			node. I.e.\ that this processor is responsible for macroscopic
			communication. For most simulations this is \code{.true.} on all
			processors.
	\end{description}

\subsection{Implementation}
	The current implementation of the \code{Parallel_neci} module works
	extremely well. It may, however, be necessary to modify it, either to
	deal with changes in the available compilers, or (more likely) to add
	support for additional MPI functions.
	
	Fortunately, the latter is much more straightforward than the former!
	A modified version of a currently implemented routine is likely to be
	the best approach.
	
	Much of the complication introduces is the requirement that NECI
	interoperate with MOLPRO, which uses C based MPI libraries. In order
	to function correctly in this environment, it is necessary to wrap
	the MPI routines in a C-wrapper, and interface this with our
	templated library.
	
	As such, the MPI wrapper implementation has a number of layers and
	non-obvious facets. This results in the behaviour being relatively
	opaque, and the code scattered with a large number of \code{#ifdef}
	statements.
	\begin{description}
		\headitem{C-wrapper initialisation}
			The constants required for operation of the C MPI
			libraries are not necessarily of a form compatible with
			Fortran. The C wrapper code (in
			\code{lib/parallel_helper.cpp}) contains lookup tables of
			the values defined in the C MPI headers. The module
			\code{ParallelHelper}, which is included in
			\code{Parallel_neci} contains Fortran definitions of these
			values which are the indices to the lookup tables.
			
			The C types required for MPI communicatiors and groups
			\code{MPI_Comm} and \code{MPI_Group} are not Fortran
			compatible. Fortunately the MPI standard provides inbuilt
			converters to integer types (\code{MPI_comm_2cf}, etc.).
			The c wrapper code mimcs the normal Fortran MPI
			code, returing Fortran compatible values, but internally
			converting them to the required types.
		
		
		\headitem{Determination of parameter sizes}
			The code to determine the lengths of the input (\code{v})
			and output (\code{ret}) arrays is passed to the code a
			template parameters \code{mpilen} and \code{mpilen2}. These
			make use of the \code{lbound} and \code{ubound} intrinsics
			to determine the number of elements in each array.
			
		\headitem{Conversion to pointers}
			Fortran, by default, passes values by pointer. In the case
			arrays, this passes a pointer to the array structure, rather
			than the data. This structure includes a pointer to the data
			and information about the data type and the array bounds.
			
			If the code is to be passed to the C wrapper, the actual
			data pointer needs to be extracted.
			
			When the C wrapper layer is being used, a value of type
			\code{c_ptr_t} is declared. If compiler support is
			present this is equal to \code{type(c_ptr)}, but otherwise
			is a raw 32 or 64 bit integer as appropriate.
			
			The memory address of the array (strictly of its first
			element) is obtained using either the standardised
			\code{c_loc}, or the non-standard \code{loc}
			intrinsics.
			
			\begin{warningbox}
			Due to a bug in some versions of gfortran an
			extra level of indirection is used which hides the type
			of the array being passed using a routine called
			\code{g_loc}. This code circumvents normal interfacing.
			\end{warningbox}
			
		\headitem{Calls to MPI}
			In the normal case (when the C-wrapper is not being
			used), the \code{MPI_*} routines are then called directly.
			After the return values are checked for errors, the
			routine returns the output data as normal.
					
		\headitem{C wrapper layer}
			Interfaces to each of the C mpi wrappers are defined in
			the \code{ParallelHelper} module. These accept the
			pointers discussed earlier. The Fortran symbols for these
			wrappers are named so that they coincide with the normal
			\code{MPI_*} routines, so that no change to the calling
			code is required.
		
	\end{description}

\section{Shared memory}
	In principle, when using MPI all of the processes are entirely independent
	except for the explicit communication made through the MPI library. Within
	NECI, as all of the particles are entirely independent between annihilation
	steps this is a good thing.
	
	However, we have a large amount of read-only data. The integrals which are
	read in from the FCIDUMP files can consume a non-trivial proportion of the
	available system memory, and are duplicated on each of the processes. On
	modern multi-processor and multi-core systems this is an outrageous waste
	of system memory --- which is made worse by the fact that if the same
	regions of this memory are requested on multiple different processors the
	computer will not know these are the same, and will not be able to make use
	of the L1, L2 and L3 cache to speed memory access.
	
	The shared memory provisions in NECI substantially abuse the MPI
	specification by mixing the available memory address space between the
	different processes. This is extremely useful when done careful, but there
	are some caveats to be aware of.
	
\subsection{How to use shared memory provisions}
	The shared memory provisions have been designed to be as interchangeable
	with the normal Fortran memory allocation provisions. Essentially memory
	is allocated largely as usual, but all of the processes on the same
	physical node will end up with pointers to the same block of memory.
	
	Memory is allocated with a call to
	\begin{lstlisting}
		subroutine shared_allocate(name, ptr, dims)
	\end{lstlisting}
	This is a templated routine, and so will work for a wide variety of data
	types and array dimensions. The \code{name} parameter specifies a unique
	name for this array. This name is used to identify the particular block
	of shared memory between the processes (there may be an arbitrary number).
	The \code{ptr} parameter is a Fortran \code{pointer} with the relevant
	data type and the correct array shape. \code{dims} is an array of integers
	indicating the size of each dimension of the array to be allocated.
	
	As an example,
	\begin{lstlisting}
		real(dp), pointer :: real_arr(:,:)
		...
		call shared_allocate("example", real_arr, (/6, 13/))
	\end{lstlisting}
	will allocate a 2-dimensional array named ``example'' with the array
	bounds \code{(1:6, 1:13)}, equivalent to \code{allocate(real_arr(6,13))}.
	
	The read only data should now be written. In principle, the data only
	needs to be written from one of the processors per node --- in practice it
	is easiest to get all of the nodes to write the read only data. It is
	important that data is only written at this stage --- any processing that
	needs to be done on this data must not happen in the shared memory, as it
	could be disrupted by the other processes. This scheme only works because
	all of the processes write exactly the same data.	
	
	After data initialisation, the processes must be synchronised with a
	call to \code{MPIBarrier}. This will ensure that the read-only data is in
	the same state in all of the threads.

	\begin{warningbox}
		The POSIX memory model makes few guarantees about \emph{when} memory
		that is shared between processes gets updated (as opposed to between
		threads). This shared memory \emph{\textbf{must not}} be used for
		communicating between the MPI processes. That is what MPI is for.
	\end{warningbox}
	
	\begin{warningbox}
		There are no synchronisation primitives provided for use with these
		shared memory blocks. No assumptions can be made about the order of
		access between the processes. All actions that are carried out
		\emph{\textbf{must}} be inherently thread-safe, or errors will
		eventually (and unexpectedly) occur.
	\end{warningbox}
	
	The data should be deallocated using the routine
	\code{shared_deallocate}.

	
\subsection{Quirks and limitations}
	\begin{description}
		\headitem{Customising data types}
			The shared memory routines are templated so that they can be used
			with a wide variety of plain-old-data types and array sizes. For
			any data types outside of those already templated, new
			configuration options will be needed in
			\code{lib/allocate_shared.F90.template}. The information
			required is the number of bytes per element of the array.
	
			If custom \code{types} are desired, this may be quite tricky. It
			is difficult to guarantee the size, memory alignment and packing
			of the data in a custom type --- the compiler is free to choose
			how it wishes to do this, and as such it varies from compiler to
			compiler. It is necessary to use the \code{sequence} keyword must
			be used to force the compiler to store data contiguously. If
			there are \code{pointer} or \code{allocatable} elements in the
			custom type, it cannot be used with shared memory, as the size of
			these elements is highly variable between compilers.
			
		\headitem{Storing bit representations}
			There is a special allocate routine \code{shared_allocate_iluts}
			which can be used for storing bit representations --- these differ
			in that the lower bound of the first array index must be $0$,
			rather than $1$.
			
		\headitem{Non-uniform memory architectures}
			Modern chip architectures (in particular the intel i3, i5 and i7
			series of processors) move the memory controller onto the
			processor. This dramatically improves memory access speeds.
			
			The cost is that on multi-processor (as opposed to multi-core)
			architectures the memory is segmented into regions that are
			controlled by the different processors. Memory access within the
			region controlled by the current processor is faster than that
			between them.
			
			A performance increase could be obtained by only sharing memory
			between processes which are hosted on the same physical processor.
			This will require using the processor affinity options of MPI,
			which is not routinely done currently with NECI. Further, it will
			require subdividing the processes on each physical node into
			sub-nodes --- support for this exists in the code in NECI, but the
			processor specific information required to correctly subdivide the
			processes is not included, and this facility is currently switched
			off.
			
			For processor topology, see the Intel documentation at
			\url{https://software.intel.com/en-us/articles/
			intel-64-architecture-processor- topology-enumeration}.
		
		\headitem{Disabling shared memory}
			Shared memory is enabled by the pre-processor define
			\code{__SHARED_MEM}. This can be found in the config files for
			platforms that support it. If necessary, this can be removed from
			relevant config files and Makefiles, which will disable inter
			process memory sharing, and fall back on the normal Fortran
			\code{allocate} mechanism.
	\end{description}
	
\subsection{How it works}
	The way that inter-process memory sharing works is system specific, and
	depends on operating system primitives being used directly.
	
	The code templating functionality in NECI is used to make the shared
	memory wrapper work for a wide range of data types, with differing numbers
	of dimensions in the arguments. From this the size of memory required in
	bytes is calculated, and this is passed to a C helper function that
	allocates the memory. The returned raw pointer is converted to a Fortran
	one through the Fortran 2003 intrinsic \code{c_f_pointer}.
	
	The C helper library contains a number of utility functions to help it
	interface with Fortran. In particular wrapper so it can print to the same
	output, and a mapping to store additional data about the allocated memory
	to assist in deallocating without the Fortan code requiring knowledge of
	how the operating system intrinsics work.
	
	Beyond that, there are three main code paths:
	\begin{description}
		\headitem{POSIX}
			A unique file name is created, based on the current working
			directory. The function call \code{shm_open} with the control
			parameter \code{O_CREAT} will open, or create, a POSIX shared
			memory object. As a result, one of the processes on a node will
			create the mapping, and the others will join it --- it is unknown
			in advance which will do the creating.
			
			The returned descriptor acts as a file, and so is set to the
			desired length with \code{fdtruncate}, and then mapped into
			memory as if it were a memory-mapped file using \code{mmap}. The
			file descriptor is then closed as it is no longer needed.
			
			Once all of the processes have mapped the memory, the shared
			memory object is unlinked, ensuring that the memory will be
			deallocated by the operating system when all of the processes
			stop (preventing a memory leak if the processes crash).
			
			The memory can then be manually deallocated using
			\code{munmap}.
			
		\headitem{System V}
			A unique file name is generated, based on the current working
			directory, and the file is created. A System V Inter Process
			Communication Key is created and obtained from this file using
			the routine \code{ftok}.
			
			Using this key, a shared memory object of the correct size is
			created using \code{shmget}, and this region is mapped into memory
			using \code{shmat}.
			
			Once all of the processes have reached this point, the shared
			memory control object is destroyed using \code{shmctl} to ensure
			the operating system will deallocate the memory in case of a
			crash.
			
			The memory can be manually deallocated using \code{shmdt}.
			
		\headitem{Windows}
			The memory mapped file provisions in Windows are used to generate
			a shared memory region. A unique file name is created, based on
			the current working directory, and then a non-filesystem backed
			file is created with \code{CreateFileMappingW}. This can be
			opened by all of the other processors on the same node using
			\code{OpenFileMappingW}, and this "file" is mapped into memory on
			all of the processes using \code{MapViewOfFile}.
			
			The memory can be manually deallocated using
			\code{UnmapViewOfFile} followed by \code{CloseHandle}.
	\end{description}
	
	The subroutine \code{iluts_pointer_jig}, which is used when allocating
	bit representations which start from an index of $0$ rather than $1$,
	is an interesting demonstration of how to manipulate the bounds of arrays
	declared in Fortran in compilers that do not support the array reshaping
	assignments described in Fortran 2003 (most compilers).
	


\section{Integral retrieval}
	Integral retrieval is found in the tightest of the tightest loops within
	NECI. Calculating each Hamiltonian matrix element may require a number of
	different two- and four-index integrals, and at least one Hamiltonian
	matrix element is required for each generated excitation.

	As a result, the normal approach taken to prevent code repetition
	introduces a bottleneck. If there is a \code{get_umat_el} function,
	this will have to contain the logic as to which type of integral is
	being obtained, and where this should be located. This conditional
	logic will be executed for every required integral.

	As such, access to the integrals is through a procedure pointer, with the
	interface
	\begin{lstlisting}[gobble=4]
		abstract interface
			function get_umat_el(i, j, k, l) result(hel)
				use constants
				implicit none
				integer, intent(in) :: i, j, k, l
				HElement_t :: hel
			end function
		end interface
	\end{lstlisting}
	This function pointer can then be called from anywhere in the code (after
	it is initialised) and this will call the correct routine.
	
	\subsection{FCIDUMP files}
		The most commonly usedintegrals routines store their integrals in
		memory after reading them in from a \code{FCIDUMP} file.
		
		These routines support FCIDUMP files produced by various codes, including MOLPRO, PSI3 and VASP 
		(hacked versions of Dalton and QChem also support this - I believe MOLCAS as well now).
		Certain (generally legacy) FCIDUMP files have a strictly fixed
		format, which can result in adjacent columns of indices merging.
		As such, they are read via an explicit format. To use MOLPRO or other
		FCIDUMP files the \code{MOLPROMIMIC} option should be supplied in
		the \code{SYSTEM} block of the input file. Other sources of
		FCIDUMP file should use the \code{FREEFORMAT} option. In general, this should {\em always} be used.
		
		The FCIDUMP files can be briefly summarised by
		\begin{description}
			\headitem{Header}
				The header is specified as a Fortran namelist, with the
				following possible elements:
				\begin{description}
					\codeitem{NORB}
						Specifies the number of spatial orbitals in the system
						if RHF, or the number of spin-orbitals if UHF.
						
					\codeitem{NELEC}
						Specifies the number of electrons the Hartree--Fock
						determinant should have.
						
					\codeitem{MS2}
						Specifies twice the total projected spin of the system
						(such that it is always an integer)
						
					\codeitem{ORBSYM}
						A list of spatial symmetries of the orbitals specified
						in (??? Check name with Giovanni) format in orbital
						order. Note that the structure of the reference
						determinant will be visible here if the
						\code{MOLPROMIMIC} option is used, and the reference
						is to be determined by the order of the orbitals.
						
					\codeitem{ISYM}
						The symmetry of the reference determinant specified in
						the same format.
						
					\codeitem{UHF}
						T if the file contains a UHF basis, otherwise F.
						
					\codeitem{SYML, SYMLZ}
						The total and projected orbital angular momentum for
						systems with an axis of rotation. Currently FCIDUMP
						files which use this option can only be generated
						using QChem, and the resultant file must be
						pre-processed using the TransLz utility.
					
					\codeitem{PROPBITLEN, NPROP}
						These parameters are used to describe the behaviour of
						k-point symmetries. For more details contact
						George Booth.
				\end{description}
			\headitem{4-index integrals}
				Following the header, all of the integral and energy lines may
				follow in arbitrary order.
				
				The 4-index integrals are specified with the format
				\code{Z i j k l}, where \code{Z} is the \code{real} or
				\code{complex} element, and the indices are integers. All
				indices are one-based. Indices are in {\bf chemical notation}!
				
			\headitem{2-index integrals}
				The 2-index integrals are specified in the same way with the
				final two indices equal to zero.
				
			\headitem{Fock energies}
				The Fock energies are specified in the same way, with the final
				three indices equal to zero.
				
				These values are used for determining the initial
				(Hartree--Fock) determinant, which is constructed from the
				lowest energy spin-orbitals. They are strictly optional. If
				the \code{MOLPROMIMIC} option is supplied then the order of
				the orbitals determines the reference determinant.
				
			\headitem{Core energy}
				The core energy is specified in the same way, but with all
				four indices set to zero.
		\end{description}
		
	\subsection{Generation on the fly}
		A number of schemes exist for generating integrals on the fly. In
		particular the Uniform Electron Gas and Hubbard model have integrals
		that can be trivially calculated, and so FCIDUMP files are not used.
		
	\subsection{Nested schemes}
		There exist two function pointers with the same abstract interface,
		\code{get_umat_el} and \code{get_umat_el_secondary}. Once the primary
		method of determining integrals has been selected, then this be
		moved to the secondary pointer, and another wrapper routine used
		instead. This allows additional filtering logic to be applied.
		
		The wrapper routine should then call \code{get_umat_el_secondary}
		internally.
		
	\subsection{Fixed Lz and complex orbitals}
		If either projected angular momentum (Lz) symmetry, or complex
		orbitals are being used, the pattern of restricted zeros in the
		integrals changes. Wrappers around the normal \code{get_umat_el}
		routine are used to supply these zeros.
		
		These are examples of the nested schemes above.
	
	\subsection{Further schemes}
		It is likely that further schemes will be required in the future. In
		particular, approximate, and interpolating schemes are likely to be
		required to reduce the $\mathcal{O}(M^4)$ memory dependence on the
		number of orbitals. These should be implemented as new routines
		to be pointed at using the function pointers.

\section{Hamiltonian matrix element evaluation}
	The two- and four-index integrals are the primary input to a FCIQMC
	calculation. However, they are used via Hamiltonian matrix elements. There
	are a number of considerations for the way that Hamiltonian matrix
	elements are used in NECI.
	
\subsection{Different ways to obtain Matrix elements}
	Certain pieces of information are required in order to calculate
	Hamiltonian matrix elements. The Excitation level (the number of differing
	orbitals between the determinants), the corresponding excitation matrix
	and the parity of the excitation are necessary. Depending on the
	excitation level, the decoded list of occupied orbitals is required.
	
	Depending on where in the execution path the Hamiltonian matrix elements
	are required, different information is readily availabile. As some of this
	is relatively expensive to calculate (in particular the parity) it is
	important that as much information as is available is used.
	
	As part of the spawning and particle generation process, there is a
	function pointer called \code{get_spawn_helement}. This is passed the
	natural integer and bit representations of the determinant, the excitation
	level, excitation matrix, parity and (potentially) pre-calculated matrix
	element. Depending on the initialisation options, the routine which is
	selected will make use of the correct subset of these. It is important
	that the excitation generator and the choice of Hamiltonian matrix element
	generation here are tightly coupled.
	
	Elsewhere in the code, the routine \code{get_helement} is used to obtain
	matrix elements.\footnote{%
		As an exception, some old code makes use of \code{gethelement} or
		\code{gethelement2}. These should not be used in new code. Some of
		the initialisation code also uses \code{gethelement} as the
		prerequisites for \code{get_helement} have not yet been met.
	} This routine comes in a number of flavours. The available options are
	\begin{description}
		\codeitem{nI, nJ}
			This routine will return the matrix element between any two,
			arbitrary, decoded determinants.
		\codeitem{nI, nJ, ic}
			This version is provided the excitation level of the determinant
			in addition.
		\codeitem{nI, nJ, ic, ilutI, ilutJ}
			The bit representations of the two determinants are provided,
			which greatly enhances calculating the parity if needed.
		\codeitem{nI, nJ, ilutI, ilutJ}
			If both the bit representations and the decoded versions are
			present but no further information is known then this form should
			be used.
		\codeitem{nI, nJ, ilutI, ilutJ, ic\_ret}
			This version is the same as the above, but returns the excitation
			level of the pair of determinants in addition to the matrix
			element.			
		\codeitem{nI, nJ, ic, excitMat, tParity}
			When everything about the relationship between the two
			determinants is fully known, this form should be used. It is
			used implicitly after excitation generation when the excitation
			level, matrix and parity are known. The second decoded determinant
			is not used in determinental calculations, but is provided to
			support systems such as CSFs.		
	\end{description}
		
	For calculating diagonal Hamiltonian matrix elements, the routine
	\code{get_diag_helement} should be used\footnote{%
		This general routine is not implemented at the time of writing, but
		will added shortly.
	}
	
\subsubsection{The cost of the parity}
	The overall sign of the returned Hamiltonian matrix element is modulated
	by the parity of the excitation --- that is, the whether the number of
	pairwise swaps of orbitals required to maximimally align the two
	determinants according to the standard order of the first is odd or
	even.\footnote{%
		Note that for double excitations, in principle there are two
		alignments that work. The two new orbitals could be either way around
		and the parity of these versions are inverted relative to each other.
		A convention for the ordering of the new orbitals (in NECI they are
		required to be numerically increasing) is required but arbitrary. The
		overall simulation will give the same results either way.
	}
	
	The parity may be obtained by actively aligning the decoded
	representations, or by examination of the bit representations. The latter
	is much faster than the former, but is still the rate limiting factor
	for calculating Hamiltonian matrix elements.
	
	Where this factor is known (i.e.\ after excitation generation) it should
	be used. It is worth noting that the weighted excitation generation
	scheme is only viable because only the absolute value of the matrix
	elements is modelled, so the parity generation step can be entirely
	ignored.	
		
\subsubsection{Slater--Condon rules implementations}
	The Slator--Condon rules are implemented in the file \code{sltcnd.F90},
	in the routines \code{sltcnd} and then more specifically \code{sltcnd_0},
	\code{sltcnd_1} and \code{sltcnd_2}. There is a certain amount of
	duplication both of code paths and of conditional testing in these\
	routines --- which is a clear violation of the DRY principle.
	
	Access to matrix elements is inside the smallest of the tight loops in
	NECI. The provision of duplicated logical pathways, rather than
	conditional switching, has a measurable performance benefit in this case.
	
\subsubsection{HPHF matrix elements}
	In most cases the HPHF matrix elements are trivially obtained from the
	normal determinental matrix elements through multiplication by a factor
	of $\sqrt{2}$ in all cases where the determinant is not closed shell.
	
	If the excitation level is less than or equal to 2, then the elements are
	a little more complicated. It is possible that both the target determinant
	and its spin pair are connected to the source determinant, and so the
	resultant matrix elements will require two calls to the Slater--Condon
	routines.

\subsubsection{Spin eigenfunctions}
	Spin eigenfunctions may be expressed as linear combinations of all
	determinants with a given spatial structure. The Hamiltonian matrix
	elements may be expressed as a two index sum over all the determinants
	with each of the involved spatial configurations.
	
	Given that the number of configurations increases permutationally with the
	number of unpaired electrons, this scheme scales extremely badly. NECI has
	some aggressive optimisation to slightly improve this scaling (see Simon's
	thesis), but ultimately it is a dead end for big calculations.
	
	See the SPINS branch for Hamiltonian matrix element calculation between
	other types of spin eigenfunctions. These can scale extremely well, but
	introduce other problems.

\section{Excitation generation}
	Excitation generation is the most complicated and intricate part of NECI.
	Not only must random moves be made, but they must be made in a manner which
	is efficient (taking account of symmetry, and in terms of implementation).
	They must also correctly compute the generation probabilities, taking into
	account multiple possible selection routes to the same determinant.
	
	A failure to generate all of the connected determinants, or mistakes
	made in calculating the generation probabilites will lead to silent
	errors that manifest themselves only through (potentially subtly)
	incorrect energies being produced by the simulation.
	
	There are a large number of factors that must be considered when writing
	an excitation generator. This section aims to give an overview of some
	of them, and a brief outline of the two most commonly used excitation
	generators.
	
	The Alavi group collectively has a large amount of experience writing
	excitation generators, and anybody intending to write or modify them
	would be advised to speak to Simon Smart or George booth first.

\subsection{Interface}
	Excitation generation is accessed through a procedure pointer. As such,
	all excitation generators \emph{must} have the same signature. It is
	described by
	\begin{lstlisting}[gobble=4]
		subroutine generate_excitation_t (nI, ilutI, nJ, ilutJ, exFlag, ic, &
                                          ex, tParity, pGen, hel, store)

            use SystemData, only: nel
            use bit_rep_data, only: NIfTot
            use FciMCData, only: excit_gen_store_type
            use constants
            implicit none

            integer, intent(in) :: nI(nel), exFlag
            integer(n_int), intent(in) :: ilutI(0:NIfTot)
            integer, intent(out) :: nJ(nel), ic, ex(2,2)
            integer(n_int), intent(out) :: ilutJ(0:NifTot)
            real(dp), intent(out) :: pGen
            logical, intent(out) :: tParity
            HElement_t, intent(out) :: hel
            type(excit_gen_store_type), intent(inout), target :: store

        end subroutine
	\end{lstlisting}
	
	\code{nI} and \code{ilutI} describe the natural integer and bit
	representations of the source determinant for the excitation. \code{nJ}
	and \code{ilutJ} will store the generated excitation. \code{exFlag}
	specifies the type of excitation --- this is generally unused, but in
	some excitiaton generators permits selecting between single and double
	excitations for the purposes of testing.
	
	\code{store} provides a location for the excitation to store information
	that is specific to the source determinant. This information is available
	for further excitations from the same site.
	
	\code{ex} will contain the excitation matrix;
	\code{ex(1,:)} contains the \emph{spin-orbitals} of the source electrons in
	\code{nI}, and \code{ex(2,:)} contains the \emph{spin-orbitals} that have
	replaced these values.
	
	\code{ic} is used to return the excitation level of the resultant site
	relative to the source site, \code{pGen} returns the generation probability
	for the generated determinant, \code{tParity} returns the parity of the
	excitation (is the number of pairwise swaps required to align \code{nI}
	and \code{nJ} even or odd), and \code{hel} can return the Hamiltonian
	matrix element for this excitation so that it need not be calculated later
	(this is only really of interest for HPHF calculations, where it can be
	easier to calculate this matrix element in the excitation generator for
	technical reasons).
	
	\begin{warningbox}
		Note that the determinant returned in \code{nJ} \emph{must} be sorted
		in the same fashion as the source determinant \code{nI}. This normally
		means in order of ascending spin-orbital number. This can result in
		substantial reshuffling of the internal order of the detrminant.
	\end{warningbox}
	
	\begin{warningbox}
		It is possible that to implement a sensibly efficient excitation
		generator, the simulation may make choices which leave no possible
		excitations remaining. An excitation may be aborted by setting the
		first element of \code{nJ} to zero, so long as the calculated generation
		probabilites for successful excitations are correct.
	\end{warningbox}
	
\subsection{Symmetry handling}
	The excitation generator must always preserve the symmetry of the
	determinant. The way in which this is done depends on the excitation
	generator, but will involve measuring the symmetry of the source orbitals
	and selecting the target orbitals appropriately.
	
	The symmetry of a specific orbital, \code{orb}, may be found in the
	array \code{G1}. The spatial symmetry is found at \code{G1(Orb)%Sym%S},
	the k-vector (if appropriate) at \code{G1(orb)%k}, and the projected
	spin and orbital angular momentum values at \code{G1(orb)%Ms} and
	\code{G1(orb)%Ml}.
	
	There are also a number of useful macros for measuring and manipulating
	spin values. \code{is_beta} and \code{is_alpha} test if orbitals have
	beta and alpha spin respectively, whilst \code{is_one_alpha_beta} test
	that two orbitals have differing spins. The \code{get_spin} macro
	gets the spin in the unusual format for $1$ for alpha and $2$ for beta as
	used in some other NECI routines, and \code{get_spin_pn} obtains the
	more standard values of $\pm 1$. The \code{is_in_pair} macro tests if two
	orbitals belong to the same spatial orbital (including the case where they
	are the same orbital).
	
	The macros \code{get_alpha} and \code{get_beta} obtain the alpha or beta
	orbital corresponding to the same spatial orbital as the current spin
	orbital (which may or may not be the same orbital). \code{ab_pair} obtains
	the spin paired orbital to the current one.
	
	It is normally necessary to combine and manipulate symmetries, as for a
	double excitation $\Gamma_i\otimes\Gamma_j = \Gamma_a\otimes\Gamma_b$,
	but there is no reason for any of $Gamma_i,Gamma_j,Gamma_a,Gamma_b$ to be
	the same. The products of symmetry labels should be taken using
	\code{RandExcitSymLabelProd}, rather than directly using \code{ieor}
	commands (which will normally obtain the same result), to protect against
	the consequences of using different types of symmetry. When combining Ml
	values it is important to bear in mind the maximum permitted value of Ml,
	and abort the excitation if necessary.
	
	The class count arrays, described above, are indexed by a a combined
	symmetry index, that combines spin, k-point, momentum and spatial
	symmetries. The index to this array is provided by the function
	\code{ClassCountInd}. Given a total spin, momentum and spatial symmetry,
	the paired class count index may be obtained using
	\code{get_paired_cc_ind}. The number of occupied, and unoccupied spin
	orbitals are stored in the data store as decribed above. The total number
	of orbitals corresponding with a given class count is found at the
	corresponding location in the array \code{OrbClassCount}.
	
	The array \code{SymLabelList2} contains all orbitals sorted by symmetry
	class, with offsets given in \code{SymLabelCounts2}, such that all of
	the orbitals with the same symmetry as a given orbital, \code{orb}, may be
	found as follows
	\begin{lstlisting}[gobble=4]
		integer :: sym, spn, ml, norb, cc_ind, offset
		
		! Obtain the symmetry labels associated with this orbital
		sym = G1(orb)%Sym%s
		spn = get_spin(orb)
		Ml = G1(orb)%Ml
		
		! Obtain the combined symmetry index
		! cc_ind = ClassCountInd(orb) ! ... alternatively
		cc_ind = ClassCountInd(spn, sym, ml)
		
		! Get position and count of orbitals
		norb = OrbClassCount(cc_ind)
		offset = SymLabelCounts2(1, cc_ind)
		
		! And this slice contains the appropriate orbitals (including orb)
		SymLabelList2(offset:offset+norb-1)
	\end{lstlisting}
	 
	  
\subsection{Manipulating determinants, and bit representations}
	The primary output of the excitation generator is the newly generated
	determinant. It is much more computationally efficient to copy and modify
	the source determinant and bit representation than to start from scratch.
	As such there are two processes to consider.
	\begin{description}
		\headitem{Manipulating bit representations}
			Orbitals can be cleared from the bit representation using the
			macro \code{clr_orb}, and set using the macro \code{set_orb}.
			For an excitation from orbital \code{a} to orbital \code{i} this
			would look like
			\begin{lstlisting}[gobble=12]
				clr_orb(ilut, a)
				set_orb(ilut, i)
			\end{lstlisting}
			
		\codeitem{make\_single and make\_double}
			Manipulating the natural integer representation of determinants is
			substantially more complicated, as there is a \emph{requirement}
			that they remain sorted by spin-orbital number. Although it would
			be straightforward to, essentially, replace the excited orbital
			with a new one, and sort it, this is inefficient as the parity
			of the excitation is going to be required. If the manipulation can
			be carried out in such a way that the parity is naturally returned
			this will substantially improve the efficiency.
			
			The routines \code{make_single} are passed the source determinant
			and respectively one or two source electron positions and target
			orbitals. They perform the substitution and measure how much the
			newly placed orbitals must be moved by to restore sorting
			(accounting for edge cases).
			
			These functions return the sorted determinant, the excitation
			matrix and the parity of the excitation. They should be used by
			all determinental excitation generators.	
	\end{description}

	  
\subsection{Data storage}
	Each site in the main particle list may be occupied by multiple particles.
	There is a reasonable amount of information which the excitation generator
	must generate for each site it excites from --- it makes sense to store
	this information and reuse it for each of the particles on the site. A
	data structure of type \code{excit_gen_store_type} is passed to the
	excitation generator that it can use for this purpose:
	\begin{lstlisting}[gobble=4]
		type excit_gen_store_type
			integer, pointer :: ClassCountOcc(:) => null()
			integer, pointer :: ClassCountUnocc(:) => null()
			integer, pointer :: scratch3(:) => null()
			integer, pointer :: occ_list(:,:) => null()
			integer, pointer :: virt_list(:,:) => null()
			logical :: tFilled
			integer, pointer :: dorder_i (:) => null()
			integer, pointer :: dorder_j (:) => null()
			integer :: nopen
		end type
	\end{lstlisting}
	The arrays that are required for the current excitation generator are
	allocated during calculation initialisation by the routine
	\code{init_excit_gen_store}.	
	
	When the excitation generator is being called on a new determinant, the
	\code{tFilled} variable will be set to \code{.false.} by the main loop
	code. Once the generator has filled in the required information this should
	be set to \code{.true.} to indicate that the data may be reused.
	
	The arrays \code{ClassCountOcc} and \code{ClassCountUnocc} are initialised
	by calling \code{construct_class_counts} at the start of the excitation
	generator. They contain a count of the number of occupied, and the number
	of unoccupied, orbitals associated with each spin-symmetry combination.
	These are used for calculating the probabilities in essentially all
	excitation generators.
	
	The \code{occ_list} and \code{virt_list} variables store only information
	relevant to the excitation generator in \code{symrandexcit3.F90}.
	
	The \code{dorder_*}, \code{nopen} and \code{scratch3} variables are used
	in the CSF excitation generation code.
	
	As there is only one instantiation of this data structure, in the main loop,
	adding additional elements adds only negligible overhead. If future
	excitation generators need a place to store information between calls, it
	should be added here.
	
	The excitation generation routines should be as tightly coupled to the
	Hamiltonian matrix element generation routines as possible. In particular,
	information such as the excitation level and the excitation matrix are
	trivially available in the excitation generator and expensive to calculate
	otherwise. As it stands, the interface for the excitation generator permits
	passing the excitation level, parity and excitation matrix to the
	matrix element generation routines --- this is sufficient for determinental
	systems.
	
	If NECI is to be extended to efficiently consider other basis functions,
	then other data will be required. A structure should be created to pass
	this information around, so that each element which is added does not
	require adjusting the interface of each and every excitation generator.
	This has been done in the SPINS branch, with a type named
	\code{spawned_info_t}, which is unlikely to be merged
	into master for independent reasons, but may provide a template for doing
	this if required in the future.
	  
\subsection{Re-use and rescaling of random numbers}
	In determinental calculations, even with large basis sets, each site is
	connected to at most a few thousand others. A single 64-bit random number
	contains vastly more random information than is required to make a good
	choice between all of these options.
	
	However, most of the potential algorithms for excitation generation
	involve a hierarchy of choices. The natural implementation of these
	choices is to generate a new random number for each new choice that is
	required. This is highly wasteful, especially as random number generation
	is relatively expensive.
	
	More bang-for-your-buck can be extracted from random numbers by
	partitioning them, and rescaling them as you move through the hierarchy
	of choices. A simple example demonstrates this, considering the macroscopic
	choice between single excitations and double excitations:
	\begin{lstlisting}[gobble=4]
		real(dp) :: r
		r = genrand_real2_dSFMT()
		if (r < pDoubles) then
			! Double excitation selected. Now rescale r
			r = r / pDoubles
			...
		else
			! Single excitation selected. Now rescale r
			r = (r - pDoubles) / (1.0_dp - pDoubles)
			...		
		end if
	\end{lstlisting}

	This is not always done in the current implementations of excitation
	generators, but should always be considered for efficiency.
	
	\begin{warningbox}
		This technique should not be used where selections may need to be
		redrawn. Repeated repartitioning and scaling of the random number
		will rapidly deplete the available randomness if it is repeated
		a number of times due to redrawing.
	\end{warningbox}
	
\subsection{Timestep selection and other control parameters}
	The spawning process is normally the limiting factor for the selection
	of the imaginary timestep. The magnitude of the spawn, $n_s$, is given by
	\begin{equation*}
		n_s = \delta\tau \abs{\frac{H_{ij}}{p_\mathrm{gen}(j|i)}}.
	\end{equation*}
	As such, a limit on the value of $\delta\tau$ may be obtained from the
	maximum value of that ratio, combined with a (chosen) maximum size of
	spawn;
	\begin{equation*}
		\delta\tau_{max} = n_{s,max} \times \left(\max\abs{\frac{H_{ij}}{p_\mathrm{gen}(j|i)}} \right)^{-1}.
	\end{equation*}
	The ratio should be accumulated through a calculation, and can be used
	to determine the optimum time step on the fly.
	
	This approach may be generalised to a larger number of parameters.
	Additional control parameters that influence decisions in the excitation
	generator may be introduces (such as the choice between single and double
	excitations, or the extent to which a bias is made towards or against
	double excitations which are spin aligned).
	
	Considering the different choices that are made in the excitation generator,
	this splits the excitations into a number of different categories. For
	optimal timestep values the worst case for each of these categories should
	be optimised to give the maximum spawn size.
	
	The impact of each of the parameters to be optimised must be removed from
	the generation probability values, so that an unaffected value can be
	maximised. For example considering a split between single and double
	excitations;
	\begin{equation*}
		n_{s,max}
		= \delta\tau \frac{\delta\tau}{p_{single}}
			\abs{\frac{H_{ij}p_{single,iter}}{p_\mathrm{gen}(j|i)}}_{single}
		= \delta\tau \frac{\delta\tau}{1.0 - p_{single}}
			\abs{\frac{H_{ij}(1.0 - p_{single,iter})}{p_\mathrm{gen}(j|i)}}
			_{double}.
	\end{equation*}
	In this case the current value of $p_{single,iter}$ (i.e.\ the value on
	the particular iteration the generation occurred) is removed from the
	ration which can then be maximised. Closed form expressions for the
	optimal values of $\delta\tau$ and $p_{single}$ can then be found
	straightforwardly.
	
	This approach can be extended, although the means to isolate the impact of
	parameters on the generation probability, and the structure of the final
	closed form expressions will depend on the form of the excitation
	generator.	

\subsection{Testing excitation generators}
	It is critical that excitation generators are \emph{correct}. That is that
	all of the connected determinants are generated, and that the generated
	probability is correct.
	
	There are a number of metrics that can be used to test this. A testing
	function should be written that takes a given source determinant as a
	parameter, and runs the excitation generator a very large number of times
	on this determinant (for example, 10 million times). This test function
	should contain a number of tests:
	\begin{description}
		\headitem{Are the correct determinants generated}
			A list of all single and double excitations of the correct symmetry
			should be enumerated in a brute force manner (see
			\code{GenExcitations3}). It should then be checked that \emph{all}
			of the determinants in this list are generated by the excitation
			generator, and no determinants outside this list are generated.

		\headitem{Normalisation of generation probabilities}
			An accumulator value should be kept for each possible determinant
			that can be generated. Each time the determinant is generated, the
			value $p_\mathrm{gen}^{-1}$ should be added to that determinants
			accumulator.
			
			On average this will add a value of $1.0$ to the accumulator for
			every excitation generation attempt. Thus, when all of the
			accumulated values are divided by the number of generation attempts
			made by the test function, all of the values should give roughly
			$1.0$.
			
			There may be a reasonable amount of variation, but repeated across
			a few different runs with different random number seeds it should
			be clear that all of these values give roughly $1.0$.
			
			Note that if
			some connections are \emph{extremely} strongly weighted against
			(as can happen with the weighted excitation generator) they will
			sum in a very large term very rarely, and so their averaged value
			can jump around quite dramatically.
					
		\headitem{Overall probability normalisation}
			As an extension to the above, the sum of all of the accumulators
			should stocastically tend towards the number of connections
			multiplied by the number of spawning attempts.
			
			If this total accumulated value is divided by the number of
			connections multiplied by the number of spawning attempts it
			should average very strongly to $1.0$, normally to four or
			five decimal places.
	\end{description}
	
	For an example test function, see \code{test_excit_gen_4ind} in
	\code{symrandexcit4.F90}. An excitation generator that passes these tests
	is not guaranteed to be correct, but it is quite likely.

\subsection{How this interacts with HPHF functions}
	HPHF functions present a complexity for excitation generation. Excitation
	generation occurs on determinants, but the HPHF function includes the spin
	flipped pair. When there are only a small number of unpaired electrons,
	the possibility of the excitation generator having generated the spin
	flipped version also needs to be available, although this is not readily
	accessible in the excitation generator.
	
	As a result, a routine must be provided to calculate the probability of
	generating $\ket{D_j}$ from $\ket{D_i}$ without actually generating an
	excitation.
	
	\begin{warningbox}
		Once the excitation generator has been planned out, it makes sense to
		write this calculation function first (it tends to be simpler than the
		excitation generator). A call to it can then be included at the end
		of the excitation generator inside \code{#ifdef __DEBUG} flags, which
		provides a powerful correctness check, and catches any cases where
		these functions diverge.
	\end{warningbox}

\subsection{The uniform selection excitation generator}
	A full discussion of this excitation generator is found in the paper (...).
	The operation of this excitation generator is also the basis for a number
	of other more specialised excitation generators (such as the CSF excitation
	generator). This excitation generator is implemented in
	\code{symrandexcit2.F90}.
	
	Although the generation probabilities associated with this excitation
	generator are highly non-uniform, it fundamentally makes choices at each
	\emph{stage} of the excitation generator uniformly between the options
	presented at this point.
	
	For single excitations, an electron is chosen uniformly at random. This
	fully specifies a symmetry index, and from the appropriate list a vacant
	orbital is chosen at random. This selection is generally made by picking
	an orbital with the appropriate symmetry at random, and re-drawing if it
	is already occupied (unless there are very few available choices, in which
	case the $n$-th available orbital is chosen from an enumeration of available
	orbitals).
	
	For double excitations, a pair of electrons are chosen at uniform (using a
	triangular mapping). This specifies the total symmetry of the excitation.
	A vacant orbital, $a$, is then chosen at random from the entire array of
	orbitals (in the same way as above, redrawing if an occupied one is chosen,
	or if an orbital is selected that has no available orbital $b$ that could
	produce the correct total symmetry). This in turn specifies the symmetry
	of orbital $b$, which is chosen in the same way as the orbital for a
	single excitation.
	
	The possibility of having picked the orbital $b$ first, and then $a$ must
	be accounted for in calculating the generation probabilities.
	
\subsection{The weighted excitation generator}
	This excitation generation scheme is discussed in a great deal more
	detail in a paper which is currently pre-publication. This excitation
	generator is implemented in \code{symrandexcit4.F90} as
	\code{gen_excit_4ind_weighted}.
	
	Fundamentally, a weighting value is used to bias the choice of orbitals
	towards those which are likely to correspond to large Hamiltonian matrix
	elements. This is done using a Cauchy--Schwarz decomposition of the
	Hamiltonian matrix elements. The specific elements used depend on the
	spin choices made, and are discussed in the referred paper.
	
	For speed, the entire choice of orbitals of a given symmetry are considered,
	and the weightings corresponding to orbitals which are occupied in the
	source determinant are set to have a zero weighting.
	
	For single excitations, an electron is chosen uniformly at random. This
	determines the target symmetry. A cumulative list of the weighted terms
	for each of the available orbitals is generated, and a random number on
	the range of the largest element in this list is generated. A specific
	index into this list is selected by binary searching for the first element
	in the cumulative list which is greater than or equal to this random
	number, and this index specifies which of the orbitals of the given
	symmetry are selected as the target orbital.
	
	For double excitations, a choice is made of whether the two orbitals
	should have the same spin or different spin (the probability of either
	is optimised on the fly at run time). If the two electrons are to have the
	same spin, they are chosen uniformly using a triangular mapping, and
	otherwise uniformly using a rectangular mapping. This entirely determines
	the target combined symmetry index.
	
	A list is constructed, with a length equal to the total number of
	available target symmetry indices. This is populated with a cumulative
	count of the product of the number of available orbitals of a given
	symmetry with the number of available orbitals of the paired combined
	symmetry required to result in the determined total symmetry. To prevent
	double counting, there are considered to be no combinations where the
	first index is larger than the second index. A pair of symmetries are chosen
	using binary searching as described before, weighted towards the symmetries
	with the most available choices.
	
	Two orbitals, $a,b$, with the corresponding symmetries, are then picked in
	the same way as the orbital was selected for single excitations (although
	the weighting terms must now consider the effects of both source orbitals).
	When choosing orbital $b$, if it comes from the same symmetry category as
	orbital $a$ the relevant elements must be adjusted in the cumulative list.
	
	For calculating the probabilities, the only duplication of generating the
	same determinant is for the case when both orbitals have the same spin
	and symmetry. This must be accounted for in the generation probability.
	
%\section{Data structures}
%The following data structures are important for understanding the overall
%runtime data flows in NECI.
\section{Determinant data storage}
	There are two main locations where blocks of information concerning
	particles associated with determinants are stored.
		
	\subsection{Transmitted data}
			This concerns data that will be (or could be) transmitted between
			different processors. Essentially this includes persistent
			information that is generated by the excitation generator.
			Combined, this information forms the \emph{bit representation}
			of a determinant.
			
			All data access should be through the getter and setter functions
			in the \code{bit_reps} module (\code{BitReps.F90}), with the
			exception of the orbital representation which may be manipulated
			directly as it will always be first. There are general
			\code{encode_bit_rep} and \code{extract_bit_rep} routines which
			package and extract all of the relevant information, as well
			as specific accessors.
			
			In most of the code the encoded values for specific determinants
			are referred to as \code{ilut}, standing for ``Integer Look-Up
			Table'' which relates to the orbital representation.
			
			The packaging of this data varies substantially between different
			compiler and runtime configurations. For example, integer runs
			on 64-bit machines co-opt some of the additional bits in the
			storage of the signed particle counts to store the flags. It is
			essential that no attempt is made to access the data in this
			array direcly.
			
			The representation of each determinant is of dimensions
			\code{0:nIfTot}, that is of length \code{nIfTot+1}. Unusually
			for an array in Fortran it is a $0$-based index. The component
			of the overall representation required to uniquely determine
			one site is \code{0:nIfDBO} (DBO stands for DetBitOps),
			which includes the orbital description and any CSF descriptiors.
			
			Each of the components of the representation has a \code{nIf*}
			(Number of Integers For) and \code{nOff*} (offset of) value.
			These should not be used directly unless adding data to this
			representation.		

			\begin{description}
				\headit{Orbital description}
					Determinants are stored by a bit-representation of the
					choice of orbitals to construct their Slater
					Determinants from. This representation is of length
					$2M$, i.e.\ the same as the number of available
					spin-orbitals. An occupied orbital is represented with a
					$1$, and a vacant orbital by a $0$.
					
					This representation can be accessed directly. To
					simplify things, various macros are available. The
					macros \code{IsOcc} and \code{IsNotOcc} test the
					occupancy of particular orbitals, and the macros
					\code{set_orb} and \code{clr_orb} modify it without
					requiring the developer to worry about the data
					representation.
					
					To decode the bit representation of the orbitals into the
					natural orbital representation use the
					\code{decode_bit_det} routine. Equivalently, the
					\code{EncodeBitDet} generates the orbital bit
					representation from the natural integer version.
				
				\headit{CSF descriptors}
					If CSF descriptor labels are required, they are stored
					here.
					
				\headit{Signed particle count}
					The representation of the coefficient, or ``sign'' of
					the determinant is the primary value which is evolved
					during an FCIQMC simulation.
					
					These values are represented by an array of
					\code{real(dp)} coefficients of length
					\code{lenof_sign}. In a standard simulation this length
					is unity, but for complex walkers two values are used
					and the double-run code uses extras.
					
					These values can be used via the routines
					\code{extract_sign} and \code{encode_sign}. The specific
					elements of the sign array can be accessed by
					\code{extract_part_sign} and \code{encode_part_sign}. If
					all particles are to be removed the routines
					\code{nullify_ilut} and \code{nullify_ilut_part} can
					be used.
					
				\headit{Flags}
					Any number of flags may be associated with a particular
					site. A current full list may be found in the file
					\code{bit_rep_data.F90}. The most commonly used flags
					are the \code{flag_is_initiator} and
					\code{flag_parent_initiator} flags --- be aware that
					these are different names for the same thing (depends
					if considering particles in the main or spawned lists).
					
					There is a pseudo-flag, \code{flag_negative_sign}, that
					only exists to help combining the representation of the
					coefficients and flags. It should never be used directly
					outside of the representation manipulation routines.0
					
					These flags may be tested using the \code{test_flag}
					and \code{set_flag} routines referencing the specific
					flag desired from the above list.
					
					Further, then entire set of flags may be extracted or
					stored using the \code{extract_flags} and
					\code{encode_flags} routines. Once they have been
					extracted, they may be examined and manipulated with the
					\code{btest}, \code{ibset} and \code{ibclr} in the same
					way as the set and test functions above. A special
					routine \code{clear_all_flags} exists for resetting
					the flag status.
					
					It is extremely important that the flags element of the
					bit representation is not accessed directly.
			\end{description}
		
		\subsection{Locally stored data}
			This concerns information that will \emph{never} be transmitted.
			All of this information is either used in tracking the status of
			an occupied determinant, or for values that could in principle
			be regenerated when required but are stored for optimisation.
			
			All data access should be through the \code{get_*} and
			\code{set_*} routines found in the module \code{global_det_data}.
			
			This data is stored in the global array
			\code{global_determinant_data}, but this array should not be
			accessed directly! The location of data within this array is
			determined by the initialisation routine in the module
			\code{global_det_data}, and depends on the calculation being
			performed.
			
			The data currently stored in this array are
			\begin{packed_itemize}
				\item
					The diagonal Hamiltonian matrix element for the
					determinant (\code{diagH}),
				\item
					The average signed occupancy of the determinant
					(\code{av_sgn}, only for RDMs).
				\item
					The iteration on which determinants became occupied,
					(\code{iter_occ}, only for RDMs) and
				\item
					The moment in imaginary time on which the (contiguous)
					occupation of this determinant began (\code{tm_occ},
					only with experimental initiators).						
			\end{packed_itemize}			

%\subsection{Iteration specific data}
%\subsection{Communication in the main loop}

\end{document}
